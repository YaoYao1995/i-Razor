nohup: ignoring input
2023-05-14 14:47:16.474137: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-14 14:47:17.415853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Got hdf Avazu data set, getting metadata...
Initialization finished!
****************************************************************************************************
config-DNN_config, fileds-16, parms-12323301, dim-122.
****************************************************************************************************
current config:  [0, 1, 12, 18, 0, 18, 1, 1, 5, 22, 18, 0, 1, 5, 0, 0, 6, 0, 0, 6, 3, 2, 3, 0]
full_emb_for_1_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_2_emb_size_12 initialized from: -0.043664375559957246 0.043664375559957246
full_emb_for_3_emb_size_18 initialized from: -0.04137439097129242 0.04137439097129242
full_emb_for_5_emb_size_18 initialized from: -0.03863337046431279 0.03863337046431279
full_emb_for_6_emb_size_1 initialized from: -0.15399810070180361 0.15399810070180361
full_emb_for_7_emb_size_1 initialized from: -0.454858826147342 0.454858826147342
full_emb_for_8_emb_size_5 initialized from: -0.007690260262421491 0.007690260262421491
full_emb_for_9_emb_size_22 initialized from: -0.003384829723893765 0.003384829723893765
full_emb_for_10_emb_size_18 initialized from: -0.03177406356760468 0.03177406356760468
full_emb_for_12_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_13_emb_size_5 initialized from: -0.04977239691468088 0.04977239691468088
full_emb_for_16_emb_size_6 initialized from: -0.11785113019775792 0.11785113019775792
full_emb_for_19_emb_size_6 initialized from: -0.1867718419094071 0.1867718419094071
full_emb_for_20_emb_size_3 initialized from: -0.3086066999241838 0.3086066999241838
full_emb_for_21_emb_size_2 initialized from: -0.7071067811865476 0.7071067811865476
full_emb_for_22_emb_size_3 initialized from: -0.4714045207910317 0.4714045207910317
0_transform_1_to_22 initialized from: -0.5107539184552492 0.5107539184552492
0_transform_1_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
1_transform_12_to_22 initialized from: -0.42008402520840293 0.42008402520840293
1_transform_12_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
2_transform_18_to_22 initialized from: -0.3872983346207417 0.3872983346207417
2_transform_18_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
3_transform_18_to_22 initialized from: -0.3872983346207417 0.3872983346207417
3_transform_18_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
4_transform_1_to_22 initialized from: -0.5107539184552492 0.5107539184552492
4_transform_1_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
5_transform_1_to_22 initialized from: -0.5107539184552492 0.5107539184552492
5_transform_1_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
6_transform_5_to_22 initialized from: -0.4714045207910317 0.4714045207910317
6_transform_5_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
8_transform_18_to_22 initialized from: -0.3872983346207417 0.3872983346207417
8_transform_18_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
9_transform_1_to_22 initialized from: -0.5107539184552492 0.5107539184552492
9_transform_1_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
10_transform_5_to_22 initialized from: -0.4714045207910317 0.4714045207910317
10_transform_5_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
11_transform_6_to_22 initialized from: -0.4629100498862757 0.4629100498862757
11_transform_6_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
12_transform_6_to_22 initialized from: -0.4629100498862757 0.4629100498862757
12_transform_6_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
13_transform_3_to_22 initialized from: -0.4898979485566356 0.4898979485566356
13_transform_3_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
14_transform_2_to_22 initialized from: -0.5 0.5
14_transform_2_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
15_transform_3_to_22 initialized from: -0.4898979485566356 0.4898979485566356
15_transform_3_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
current config:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
full_emb_for_0_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_1_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_2_emb_size_1 initialized from: -0.043740888263985325 0.043740888263985325
full_emb_for_3_emb_size_1 initialized from: -0.04147509477069983 0.04147509477069983
full_emb_for_4_emb_size_1 initialized from: -0.4898979485566356 0.4898979485566356
full_emb_for_5_emb_size_1 initialized from: -0.038715317938997504 0.038715317938997504
full_emb_for_6_emb_size_1 initialized from: -0.15399810070180361 0.15399810070180361
full_emb_for_7_emb_size_1 initialized from: -0.454858826147342 0.454858826147342
full_emb_for_8_emb_size_1 initialized from: -0.007690411867832244 0.007690411867832244
full_emb_for_9_emb_size_1 initialized from: -0.003384897591352657 0.003384897591352657
full_emb_for_10_emb_size_1 initialized from: -0.031819606281476856 0.031819606281476856
full_emb_for_11_emb_size_1 initialized from: -1.0 1.0
full_emb_for_12_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_13_emb_size_1 initialized from: -0.04981354813867179 0.04981354813867179
full_emb_for_14_emb_size_1 initialized from: -0.816496580927726 0.816496580927726
full_emb_for_15_emb_size_1 initialized from: -0.7745966692414834 0.7745966692414834
full_emb_for_16_emb_size_1 initialized from: -0.11853911695403994 0.11853911695403994
full_emb_for_17_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_18_emb_size_1 initialized from: -0.2970442628930023 0.2970442628930023
full_emb_for_19_emb_size_1 initialized from: -0.18954720708196904 0.18954720708196904
full_emb_for_20_emb_size_1 initialized from: -0.31362502409359 0.31362502409359
full_emb_for_21_emb_size_1 initialized from: -0.7385489458759964 0.7385489458759964
full_emb_for_22_emb_size_1 initialized from: -0.4898979485566356 0.4898979485566356
full_emb_for_23_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
all_bias [<tf.Tensor 'GatherV2_16:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_17:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_18:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_19:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_20:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_21:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_22:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_23:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_24:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_25:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_26:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_27:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_28:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_29:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_30:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_31:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_32:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_33:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_34:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_35:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_36:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_37:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_38:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_39:0' shape=(?, 1) dtype=float32>]
cin_0_wt_16_new_16 initialized from: -0.14852213144650114 0.14852213144650114
cin_0_bias_16_new_16 initialized from: -0.5773502691896257 0.5773502691896257
cin_1_wt_16_new_16 initialized from: -0.14852213144650114 0.14852213144650114
cin_1_bias_16_new_16 initialized from: -0.5773502691896257 0.5773502691896257
cin_2_wt_16_new_16 initialized from: -0.14852213144650114 0.14852213144650114
cin_2_bias_16_new_16 initialized from: -0.5773502691896257 0.5773502691896257
cin_pooling_in_48_out1 initialized from: -0.3499271061118826 0.3499271061118826
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
w_0 initialized from: -0.0854357657716761 0.0854357657716761
(122, 700) (700,)
relu
w_1 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_2 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_3 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_4 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_5 initialized from: -0.0925159507394634 0.0925159507394634
(700, 1) (1,)
none
mlp output Tensor("hidden_5/add:0", shape=(?, 1), dtype=float32) bias_sum Tensor("Sum_3:0", shape=(?, 1), dtype=float32)
lgits Tensor("Sum_4:0", shape=(?,), dtype=float32)
[[34m2023-05-14 14:47:19[0m] Experiment directory created at /home/ubuntu/results/xDeepFM_Retrain_DNN/avazu/003-[700, 700, 700, 700, 700, 1]-bs-128
[[34m2023-05-14 14:47:19[0m] Batchsize: 128
wandb: Currently logged in as: yao-yao. Use `wandb login --relogin` to force relogin
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /tmp/wandb/run-20230514_144720-981amj8r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avazu-BS-128-003-xDeepFM_Retrain_DNN-2023-05-14 14:47:19
wandb: â­ï¸ View project at https://wandb.ai/yao-yao/irazor
wandb: ðŸš€ View run at https://wandb.ai/yao-yao/irazor/runs/981amj8r
2023-05-14 14:47:25.728865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:47:25.730511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:47:25.731309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:47:27.143435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:47:27.144481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:47:27.145277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:47:27.146060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37804 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
[[34m2023-05-14 14:47:27[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
add l2 0.001 Tensor("concat:0", shape=(?, 122), dtype=float32)
add l2 0.001 Tensor("concat_1:0", shape=(?, 24), dtype=float32)
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
[[34m2023-05-14 14:47:27[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2023-05-14 14:47:28.345523: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
[[34m2023-05-14 14:47:28[0m] total batches: 505370	batch per epoch: 50537
[[34m2023-05-14 14:47:28[0m] new iteration
new iteration
on disk...
2023-05-14 14:47:30.204933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
[[34m2023-05-14 14:47:38[0m] elapsed : 0:00:09, ETA : 1:15:39
[[34m2023-05-14 14:47:38[0m] epoch 1 / 10, batch 1000 / 50537, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.433695, l2 = 0.119309, auc = 0.699524
elapsed : 0:00:09, ETA : 1:15:39
epoch 1 / 10, batch 1000 / 50537, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.433695, l2 = 0.119309, auc = 0.699524
[[34m2023-05-14 14:47:47[0m] elapsed : 0:00:18, ETA : 1:15:30
[[34m2023-05-14 14:47:47[0m] epoch 1 / 10, batch 2000 / 50537, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.403565, l2 = 0.053824, auc = 0.730322
elapsed : 0:00:18, ETA : 1:15:30
epoch 1 / 10, batch 2000 / 50537, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.403565, l2 = 0.053824, auc = 0.730322
[[34m2023-05-14 14:47:55[0m] elapsed : 0:00:26, ETA : 1:12:33
[[34m2023-05-14 14:47:55[0m] epoch 1 / 10, batch 3000 / 50537, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.403675, l2 = 0.032032, auc = 0.736858
elapsed : 0:00:26, ETA : 1:12:33
epoch 1 / 10, batch 3000 / 50537, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.403675, l2 = 0.032032, auc = 0.736858
[[34m2023-05-14 14:48:04[0m] elapsed : 0:00:35, ETA : 1:13:06
[[34m2023-05-14 14:48:04[0m] epoch 1 / 10, batch 4000 / 50537, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.401761, l2 = 0.021420, auc = 0.738799
elapsed : 0:00:35, ETA : 1:13:06
epoch 1 / 10, batch 4000 / 50537, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.401761, l2 = 0.021420, auc = 0.738799
[[34m2023-05-14 14:48:12[0m] elapsed : 0:00:44, ETA : 1:13:23
[[34m2023-05-14 14:48:12[0m] epoch 1 / 10, batch 5000 / 50537, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.400624, l2 = 0.015476, auc = 0.741531
elapsed : 0:00:44, ETA : 1:13:23
epoch 1 / 10, batch 5000 / 50537, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.400624, l2 = 0.015476, auc = 0.741531
[[34m2023-05-14 14:48:21[0m] elapsed : 0:00:52, ETA : 1:12:07
[[34m2023-05-14 14:48:21[0m] epoch 1 / 10, batch 6000 / 50537, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.398310, l2 = 0.011949, auc = 0.744760
elapsed : 0:00:52, ETA : 1:12:07
epoch 1 / 10, batch 6000 / 50537, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.398310, l2 = 0.011949, auc = 0.744760
[[34m2023-05-14 14:48:29[0m] elapsed : 0:01:01, ETA : 1:12:22
[[34m2023-05-14 14:48:29[0m] epoch 1 / 10, batch 7000 / 50537, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.396540, l2 = 0.009665, auc = 0.746973
elapsed : 0:01:01, ETA : 1:12:22
epoch 1 / 10, batch 7000 / 50537, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.396540, l2 = 0.009665, auc = 0.746973
[[34m2023-05-14 14:48:38[0m] elapsed : 0:01:09, ETA : 1:11:29
[[34m2023-05-14 14:48:38[0m] epoch 1 / 10, batch 8000 / 50537, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.397560, l2 = 0.008197, auc = 0.747086
elapsed : 0:01:09, ETA : 1:11:29
epoch 1 / 10, batch 8000 / 50537, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.397560, l2 = 0.008197, auc = 0.747086
[[34m2023-05-14 14:48:46[0m] elapsed : 0:01:18, ETA : 1:11:41
[[34m2023-05-14 14:48:46[0m] epoch 1 / 10, batch 9000 / 50537, global_step = 9000, learning_rate = 1.000000e-02, loss = 0.396895, l2 = 0.007129, auc = 0.745730
elapsed : 0:01:18, ETA : 1:11:41
epoch 1 / 10, batch 9000 / 50537, global_step = 9000, learning_rate = 1.000000e-02, loss = 0.396895, l2 = 0.007129, auc = 0.745730
[[34m2023-05-14 14:48:55[0m] elapsed : 0:01:26, ETA : 1:11:00
[[34m2023-05-14 14:48:55[0m] epoch 1 / 10, batch 10000 / 50537, global_step = 10000, learning_rate = 1.000000e-02, loss = 0.396399, l2 = 0.006408, auc = 0.748047
elapsed : 0:01:26, ETA : 1:11:00
epoch 1 / 10, batch 10000 / 50537, global_step = 10000, learning_rate = 1.000000e-02, loss = 0.396399, l2 = 0.006408, auc = 0.748047
[[34m2023-05-14 14:49:04[0m] elapsed : 0:01:35, ETA : 1:11:09
[[34m2023-05-14 14:49:04[0m] epoch 1 / 10, batch 11000 / 50537, global_step = 11000, learning_rate = 1.000000e-02, loss = 0.395440, l2 = 0.005859, auc = 0.748977
elapsed : 0:01:35, ETA : 1:11:09
epoch 1 / 10, batch 11000 / 50537, global_step = 11000, learning_rate = 1.000000e-02, loss = 0.395440, l2 = 0.005859, auc = 0.748977
[[34m2023-05-14 14:49:12[0m] elapsed : 0:01:43, ETA : 1:10:34
[[34m2023-05-14 14:49:12[0m] epoch 1 / 10, batch 12000 / 50537, global_step = 12000, learning_rate = 1.000000e-02, loss = 0.397172, l2 = 0.005447, auc = 0.750047
elapsed : 0:01:43, ETA : 1:10:34
epoch 1 / 10, batch 12000 / 50537, global_step = 12000, learning_rate = 1.000000e-02, loss = 0.397172, l2 = 0.005447, auc = 0.750047
[[34m2023-05-14 14:49:21[0m] elapsed : 0:01:52, ETA : 1:10:41
[[34m2023-05-14 14:49:21[0m] epoch 1 / 10, batch 13000 / 50537, global_step = 13000, learning_rate = 1.000000e-02, loss = 0.396971, l2 = 0.005084, auc = 0.746935
elapsed : 0:01:52, ETA : 1:10:41
epoch 1 / 10, batch 13000 / 50537, global_step = 13000, learning_rate = 1.000000e-02, loss = 0.396971, l2 = 0.005084, auc = 0.746935
[[34m2023-05-14 14:49:37[0m] elapsed : 0:02:08, ETA : 1:14:52
[[34m2023-05-14 14:49:37[0m] epoch 1 / 10, batch 14000 / 50537, global_step = 14000, learning_rate = 1.000000e-02, loss = 0.394195, l2 = 0.004819, auc = 0.752289
elapsed : 0:02:08, ETA : 1:14:52
epoch 1 / 10, batch 14000 / 50537, global_step = 14000, learning_rate = 1.000000e-02, loss = 0.394195, l2 = 0.004819, auc = 0.752289
[[34m2023-05-14 14:49:54[0m] elapsed : 0:02:26, ETA : 1:19:32
[[34m2023-05-14 14:49:54[0m] epoch 1 / 10, batch 15000 / 50537, global_step = 15000, learning_rate = 1.000000e-02, loss = 0.394441, l2 = 0.004620, auc = 0.753536
elapsed : 0:02:26, ETA : 1:19:32
epoch 1 / 10, batch 15000 / 50537, global_step = 15000, learning_rate = 1.000000e-02, loss = 0.394441, l2 = 0.004620, auc = 0.753536
[[34m2023-05-14 14:50:15[0m] elapsed : 0:02:46, ETA : 1:24:37
[[34m2023-05-14 14:50:15[0m] epoch 1 / 10, batch 16000 / 50537, global_step = 16000, learning_rate = 1.000000e-02, loss = 0.395583, l2 = 0.004429, auc = 0.751583
elapsed : 0:02:46, ETA : 1:24:37
epoch 1 / 10, batch 16000 / 50537, global_step = 16000, learning_rate = 1.000000e-02, loss = 0.395583, l2 = 0.004429, auc = 0.751583
[[34m2023-05-14 14:50:43[0m] elapsed : 0:03:15, ETA : 1:33:21
[[34m2023-05-14 14:50:43[0m] epoch 1 / 10, batch 17000 / 50537, global_step = 17000, learning_rate = 1.000000e-02, loss = 0.394278, l2 = 0.004284, auc = 0.754054
elapsed : 0:03:15, ETA : 1:33:21
epoch 1 / 10, batch 17000 / 50537, global_step = 17000, learning_rate = 1.000000e-02, loss = 0.394278, l2 = 0.004284, auc = 0.754054
[[34m2023-05-14 14:51:12[0m] elapsed : 0:03:44, ETA : 1:41:05
[[34m2023-05-14 14:51:12[0m] epoch 1 / 10, batch 18000 / 50537, global_step = 18000, learning_rate = 1.000000e-02, loss = 0.392292, l2 = 0.004163, auc = 0.754919
elapsed : 0:03:44, ETA : 1:41:05
epoch 1 / 10, batch 18000 / 50537, global_step = 18000, learning_rate = 1.000000e-02, loss = 0.392292, l2 = 0.004163, auc = 0.754919
[[34m2023-05-14 14:51:41[0m] elapsed : 0:04:12, ETA : 1:47:30
[[34m2023-05-14 14:51:41[0m] epoch 1 / 10, batch 19000 / 50537, global_step = 19000, learning_rate = 1.000000e-02, loss = 0.391019, l2 = 0.004075, auc = 0.755204
elapsed : 0:04:12, ETA : 1:47:30
epoch 1 / 10, batch 19000 / 50537, global_step = 19000, learning_rate = 1.000000e-02, loss = 0.391019, l2 = 0.004075, auc = 0.755204
[[34m2023-05-14 14:52:10[0m] elapsed : 0:04:41, ETA : 1:53:39
[[34m2023-05-14 14:52:10[0m] epoch 1 / 10, batch 20000 / 50537, global_step = 20000, learning_rate = 1.000000e-02, loss = 0.392328, l2 = 0.003992, auc = 0.755794
elapsed : 0:04:41, ETA : 1:53:39
epoch 1 / 10, batch 20000 / 50537, global_step = 20000, learning_rate = 1.000000e-02, loss = 0.392328, l2 = 0.003992, auc = 0.755794
[[34m2023-05-14 14:52:38[0m] elapsed : 0:05:10, ETA : 1:59:10
[[34m2023-05-14 14:52:38[0m] epoch 1 / 10, batch 21000 / 50537, global_step = 21000, learning_rate = 1.000000e-02, loss = 0.394341, l2 = 0.003881, auc = 0.753396
elapsed : 0:05:10, ETA : 1:59:10
epoch 1 / 10, batch 21000 / 50537, global_step = 21000, learning_rate = 1.000000e-02, loss = 0.394341, l2 = 0.003881, auc = 0.753396
[[34m2023-05-14 14:53:07[0m] elapsed : 0:05:38, ETA : 2:03:46
[[34m2023-05-14 14:53:07[0m] epoch 1 / 10, batch 22000 / 50537, global_step = 22000, learning_rate = 1.000000e-02, loss = 0.389384, l2 = 0.003816, auc = 0.759258
elapsed : 0:05:38, ETA : 2:03:46
epoch 1 / 10, batch 22000 / 50537, global_step = 22000, learning_rate = 1.000000e-02, loss = 0.389384, l2 = 0.003816, auc = 0.759258
[[34m2023-05-14 14:53:36[0m] elapsed : 0:06:07, ETA : 2:08:16
[[34m2023-05-14 14:53:36[0m] epoch 1 / 10, batch 23000 / 50537, global_step = 23000, learning_rate = 1.000000e-02, loss = 0.390122, l2 = 0.003757, auc = 0.760605
elapsed : 0:06:07, ETA : 2:08:16
epoch 1 / 10, batch 23000 / 50537, global_step = 23000, learning_rate = 1.000000e-02, loss = 0.390122, l2 = 0.003757, auc = 0.760605
[[34m2023-05-14 14:54:04[0m] elapsed : 0:06:35, ETA : 2:12:02
[[34m2023-05-14 14:54:04[0m] epoch 1 / 10, batch 24000 / 50537, global_step = 24000, learning_rate = 1.000000e-02, loss = 0.390604, l2 = 0.003692, auc = 0.758070
elapsed : 0:06:35, ETA : 2:12:02
epoch 1 / 10, batch 24000 / 50537, global_step = 24000, learning_rate = 1.000000e-02, loss = 0.390604, l2 = 0.003692, auc = 0.758070
[[34m2023-05-14 14:54:33[0m] elapsed : 0:07:04, ETA : 2:15:47
[[34m2023-05-14 14:54:33[0m] epoch 1 / 10, batch 25000 / 50537, global_step = 25000, learning_rate = 1.000000e-02, loss = 0.393115, l2 = 0.003639, auc = 0.758228
elapsed : 0:07:04, ETA : 2:15:47
epoch 1 / 10, batch 25000 / 50537, global_step = 25000, learning_rate = 1.000000e-02, loss = 0.393115, l2 = 0.003639, auc = 0.758228
[[34m2023-05-14 14:55:02[0m] elapsed : 0:07:33, ETA : 2:19:12
[[34m2023-05-14 14:55:02[0m] epoch 1 / 10, batch 26000 / 50537, global_step = 26000, learning_rate = 1.000000e-02, loss = 0.392026, l2 = 0.003583, auc = 0.758564
elapsed : 0:07:33, ETA : 2:19:12
epoch 1 / 10, batch 26000 / 50537, global_step = 26000, learning_rate = 1.000000e-02, loss = 0.392026, l2 = 0.003583, auc = 0.758564
[[34m2023-05-14 14:55:30[0m] elapsed : 0:08:01, ETA : 2:22:02
[[34m2023-05-14 14:55:30[0m] epoch 1 / 10, batch 27000 / 50537, global_step = 27000, learning_rate = 1.000000e-02, loss = 0.390470, l2 = 0.003544, auc = 0.760386
elapsed : 0:08:01, ETA : 2:22:02
epoch 1 / 10, batch 27000 / 50537, global_step = 27000, learning_rate = 1.000000e-02, loss = 0.390470, l2 = 0.003544, auc = 0.760386
[[34m2023-05-14 14:55:59[0m] elapsed : 0:08:30, ETA : 2:24:54
[[34m2023-05-14 14:55:59[0m] epoch 1 / 10, batch 28000 / 50537, global_step = 28000, learning_rate = 1.000000e-02, loss = 0.388846, l2 = 0.003515, auc = 0.764322
elapsed : 0:08:30, ETA : 2:24:54
epoch 1 / 10, batch 28000 / 50537, global_step = 28000, learning_rate = 1.000000e-02, loss = 0.388846, l2 = 0.003515, auc = 0.764322
[[34m2023-05-14 14:56:28[0m] elapsed : 0:08:59, ETA : 2:27:33
[[34m2023-05-14 14:56:28[0m] epoch 1 / 10, batch 29000 / 50537, global_step = 29000, learning_rate = 1.000000e-02, loss = 0.389177, l2 = 0.003476, auc = 0.762833
elapsed : 0:08:59, ETA : 2:27:33
epoch 1 / 10, batch 29000 / 50537, global_step = 29000, learning_rate = 1.000000e-02, loss = 0.389177, l2 = 0.003476, auc = 0.762833
[[34m2023-05-14 14:56:56[0m] elapsed : 0:09:28, ETA : 2:30:00
[[34m2023-05-14 14:56:56[0m] epoch 1 / 10, batch 30000 / 50537, global_step = 30000, learning_rate = 1.000000e-02, loss = 0.390303, l2 = 0.003433, auc = 0.761282
elapsed : 0:09:28, ETA : 2:30:00
epoch 1 / 10, batch 30000 / 50537, global_step = 30000, learning_rate = 1.000000e-02, loss = 0.390303, l2 = 0.003433, auc = 0.761282
[[34m2023-05-14 14:57:25[0m] elapsed : 0:09:56, ETA : 2:32:00
[[34m2023-05-14 14:57:25[0m] epoch 1 / 10, batch 31000 / 50537, global_step = 31000, learning_rate = 1.000000e-02, loss = 0.386525, l2 = 0.003400, auc = 0.761631
elapsed : 0:09:56, ETA : 2:32:00
epoch 1 / 10, batch 31000 / 50537, global_step = 31000, learning_rate = 1.000000e-02, loss = 0.386525, l2 = 0.003400, auc = 0.761631
[[34m2023-05-14 14:57:54[0m] elapsed : 0:10:25, ETA : 2:34:05
[[34m2023-05-14 14:57:54[0m] epoch 1 / 10, batch 32000 / 50537, global_step = 32000, learning_rate = 1.000000e-02, loss = 0.391888, l2 = 0.003356, auc = 0.761561
elapsed : 0:10:25, ETA : 2:34:05
epoch 1 / 10, batch 32000 / 50537, global_step = 32000, learning_rate = 1.000000e-02, loss = 0.391888, l2 = 0.003356, auc = 0.761561
[[34m2023-05-14 14:58:23[0m] elapsed : 0:10:54, ETA : 2:36:01
[[34m2023-05-14 14:58:23[0m] epoch 1 / 10, batch 33000 / 50537, global_step = 33000, learning_rate = 1.000000e-02, loss = 0.387955, l2 = 0.003350, auc = 0.762166
elapsed : 0:10:54, ETA : 2:36:01
epoch 1 / 10, batch 33000 / 50537, global_step = 33000, learning_rate = 1.000000e-02, loss = 0.387955, l2 = 0.003350, auc = 0.762166
[[34m2023-05-14 14:58:51[0m] elapsed : 0:11:22, ETA : 2:37:35
[[34m2023-05-14 14:58:51[0m] epoch 1 / 10, batch 34000 / 50537, global_step = 34000, learning_rate = 1.000000e-02, loss = 0.388767, l2 = 0.003331, auc = 0.762430
elapsed : 0:11:22, ETA : 2:37:35
epoch 1 / 10, batch 34000 / 50537, global_step = 34000, learning_rate = 1.000000e-02, loss = 0.388767, l2 = 0.003331, auc = 0.762430
[[34m2023-05-14 14:59:20[0m] elapsed : 0:11:51, ETA : 2:39:15
[[34m2023-05-14 14:59:20[0m] epoch 1 / 10, batch 35000 / 50537, global_step = 35000, learning_rate = 1.000000e-02, loss = 0.389969, l2 = 0.003289, auc = 0.762246
elapsed : 0:11:51, ETA : 2:39:15
epoch 1 / 10, batch 35000 / 50537, global_step = 35000, learning_rate = 1.000000e-02, loss = 0.389969, l2 = 0.003289, auc = 0.762246
[[34m2023-05-14 14:59:49[0m] elapsed : 0:12:20, ETA : 2:40:48
[[34m2023-05-14 14:59:49[0m] epoch 1 / 10, batch 36000 / 50537, global_step = 36000, learning_rate = 1.000000e-02, loss = 0.391885, l2 = 0.003262, auc = 0.761000
elapsed : 0:12:20, ETA : 2:40:48
epoch 1 / 10, batch 36000 / 50537, global_step = 36000, learning_rate = 1.000000e-02, loss = 0.391885, l2 = 0.003262, auc = 0.761000
[[34m2023-05-14 15:00:18[0m] elapsed : 0:12:49, ETA : 2:42:14
[[34m2023-05-14 15:00:18[0m] epoch 1 / 10, batch 37000 / 50537, global_step = 37000, learning_rate = 1.000000e-02, loss = 0.390309, l2 = 0.003238, auc = 0.761926
elapsed : 0:12:49, ETA : 2:42:14
epoch 1 / 10, batch 37000 / 50537, global_step = 37000, learning_rate = 1.000000e-02, loss = 0.390309, l2 = 0.003238, auc = 0.761926
[[34m2023-05-14 15:00:47[0m] elapsed : 0:13:18, ETA : 2:43:34
[[34m2023-05-14 15:00:47[0m] epoch 1 / 10, batch 38000 / 50537, global_step = 38000, learning_rate = 1.000000e-02, loss = 0.387563, l2 = 0.003199, auc = 0.764613
elapsed : 0:13:18, ETA : 2:43:34
epoch 1 / 10, batch 38000 / 50537, global_step = 38000, learning_rate = 1.000000e-02, loss = 0.387563, l2 = 0.003199, auc = 0.764613
[[34m2023-05-14 15:01:15[0m] elapsed : 0:13:46, ETA : 2:44:37
[[34m2023-05-14 15:01:15[0m] epoch 1 / 10, batch 39000 / 50537, global_step = 39000, learning_rate = 1.000000e-02, loss = 0.388331, l2 = 0.003187, auc = 0.765612
elapsed : 0:13:46, ETA : 2:44:37
epoch 1 / 10, batch 39000 / 50537, global_step = 39000, learning_rate = 1.000000e-02, loss = 0.388331, l2 = 0.003187, auc = 0.765612
[[34m2023-05-14 15:01:44[0m] elapsed : 0:14:15, ETA : 2:45:47
[[34m2023-05-14 15:01:44[0m] epoch 1 / 10, batch 40000 / 50537, global_step = 40000, learning_rate = 1.000000e-02, loss = 0.390249, l2 = 0.003163, auc = 0.763299
elapsed : 0:14:15, ETA : 2:45:47
epoch 1 / 10, batch 40000 / 50537, global_step = 40000, learning_rate = 1.000000e-02, loss = 0.390249, l2 = 0.003163, auc = 0.763299
[[34m2023-05-14 15:02:13[0m] elapsed : 0:14:44, ETA : 2:46:52
[[34m2023-05-14 15:02:13[0m] epoch 1 / 10, batch 41000 / 50537, global_step = 41000, learning_rate = 1.000000e-02, loss = 0.390376, l2 = 0.003146, auc = 0.764239
elapsed : 0:14:44, ETA : 2:46:52
epoch 1 / 10, batch 41000 / 50537, global_step = 41000, learning_rate = 1.000000e-02, loss = 0.390376, l2 = 0.003146, auc = 0.764239
[[34m2023-05-14 15:02:41[0m] elapsed : 0:15:12, ETA : 2:47:41
[[34m2023-05-14 15:02:41[0m] epoch 1 / 10, batch 42000 / 50537, global_step = 42000, learning_rate = 1.000000e-02, loss = 0.386303, l2 = 0.003117, auc = 0.763925
elapsed : 0:15:12, ETA : 2:47:41
epoch 1 / 10, batch 42000 / 50537, global_step = 42000, learning_rate = 1.000000e-02, loss = 0.386303, l2 = 0.003117, auc = 0.763925
[[34m2023-05-14 15:03:10[0m] elapsed : 0:15:41, ETA : 2:48:38
[[34m2023-05-14 15:03:10[0m] epoch 1 / 10, batch 43000 / 50537, global_step = 43000, learning_rate = 1.000000e-02, loss = 0.390870, l2 = 0.003095, auc = 0.761998
elapsed : 0:15:41, ETA : 2:48:38
epoch 1 / 10, batch 43000 / 50537, global_step = 43000, learning_rate = 1.000000e-02, loss = 0.390870, l2 = 0.003095, auc = 0.761998
[[34m2023-05-14 15:03:39[0m] elapsed : 0:16:10, ETA : 2:49:31
[[34m2023-05-14 15:03:39[0m] epoch 1 / 10, batch 44000 / 50537, global_step = 44000, learning_rate = 1.000000e-02, loss = 0.387921, l2 = 0.003091, auc = 0.767113
elapsed : 0:16:10, ETA : 2:49:31
epoch 1 / 10, batch 44000 / 50537, global_step = 44000, learning_rate = 1.000000e-02, loss = 0.387921, l2 = 0.003091, auc = 0.767113
[[34m2023-05-14 15:04:07[0m] elapsed : 0:16:39, ETA : 2:50:20
[[34m2023-05-14 15:04:07[0m] epoch 1 / 10, batch 45000 / 50537, global_step = 45000, learning_rate = 1.000000e-02, loss = 0.384778, l2 = 0.003068, auc = 0.767619
elapsed : 0:16:39, ETA : 2:50:20
epoch 1 / 10, batch 45000 / 50537, global_step = 45000, learning_rate = 1.000000e-02, loss = 0.384778, l2 = 0.003068, auc = 0.767619
[[34m2023-05-14 15:04:36[0m] elapsed : 0:17:07, ETA : 2:50:55
[[34m2023-05-14 15:04:36[0m] epoch 1 / 10, batch 46000 / 50537, global_step = 46000, learning_rate = 1.000000e-02, loss = 0.385439, l2 = 0.003066, auc = 0.764002
elapsed : 0:17:07, ETA : 2:50:55
epoch 1 / 10, batch 46000 / 50537, global_step = 46000, learning_rate = 1.000000e-02, loss = 0.385439, l2 = 0.003066, auc = 0.764002
[[34m2023-05-14 15:05:05[0m] elapsed : 0:17:36, ETA : 2:51:38
[[34m2023-05-14 15:05:05[0m] epoch 1 / 10, batch 47000 / 50537, global_step = 47000, learning_rate = 1.000000e-02, loss = 0.387796, l2 = 0.003026, auc = 0.765407
elapsed : 0:17:36, ETA : 2:51:38
epoch 1 / 10, batch 47000 / 50537, global_step = 47000, learning_rate = 1.000000e-02, loss = 0.387796, l2 = 0.003026, auc = 0.765407
[[34m2023-05-14 15:05:33[0m] elapsed : 0:18:05, ETA : 2:52:18
[[34m2023-05-14 15:05:33[0m] epoch 1 / 10, batch 48000 / 50537, global_step = 48000, learning_rate = 1.000000e-02, loss = 0.387488, l2 = 0.003026, auc = 0.768109
elapsed : 0:18:05, ETA : 2:52:18
epoch 1 / 10, batch 48000 / 50537, global_step = 48000, learning_rate = 1.000000e-02, loss = 0.387488, l2 = 0.003026, auc = 0.768109
[[34m2023-05-14 15:06:02[0m] elapsed : 0:18:34, ETA : 2:52:55
[[34m2023-05-14 15:06:02[0m] epoch 1 / 10, batch 49000 / 50537, global_step = 49000, learning_rate = 1.000000e-02, loss = 0.384119, l2 = 0.003012, auc = 0.766391
elapsed : 0:18:34, ETA : 2:52:55
epoch 1 / 10, batch 49000 / 50537, global_step = 49000, learning_rate = 1.000000e-02, loss = 0.384119, l2 = 0.003012, auc = 0.766391
[[34m2023-05-14 15:06:31[0m] elapsed : 0:19:02, ETA : 2:53:20
[[34m2023-05-14 15:06:31[0m] epoch 1 / 10, batch 50000 / 50537, global_step = 50000, learning_rate = 1.000000e-02, loss = 0.388876, l2 = 0.002985, auc = 0.764240
elapsed : 0:19:02, ETA : 2:53:20
epoch 1 / 10, batch 50000 / 50537, global_step = 50000, learning_rate = 1.000000e-02, loss = 0.388876, l2 = 0.002985, auc = 0.764240
[[34m2023-05-14 15:06:46[0m] running test...
on disk...
[[34m2023-05-14 15:06:51[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-14 15:06:56[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-14 15:07:01[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-14 15:07:06[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-14 15:07:11[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-14 15:07:15[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-14 15:07:20[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-14 15:07:25[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-14 15:07:30[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-14 15:07:35[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-14 15:07:40[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-14 15:07:44[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-14 15:07:49[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-14 15:07:54[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-14 15:07:59[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-14 15:08:04[0m] evaluated batches: 16000, 0:00:05
[[34m2023-05-14 15:08:09[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-14 15:08:14[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-14 15:08:18[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-14 15:08:23[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-14 15:08:28[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-14 15:08:33[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-14 15:08:38[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-14 15:08:43[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-14 15:08:48[0m] evaluated batches: 25000, 0:00:04
[[34m2023-05-14 15:08:52[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-14 15:08:57[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-14 15:09:02[0m] evaluated batches: 28000, 0:00:04
[[34m2023-05-14 15:09:07[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-14 15:09:12[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-14 15:09:17[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-14 15:09:22[0m] evaluated batches: 32000, 0:00:05
[[34m2023-05-14 15:09:26[0m] evaluated batches: 33000, 0:00:04
[[34m2023-05-14 15:09:31[0m] evaluated batches: 34000, 0:00:04
[[34m2023-05-14 15:09:36[0m] evaluated batches: 35000, 0:00:04
[[34m2023-05-14 15:09:41[0m] evaluated batches: 36000, 0:00:04
[[34m2023-05-14 15:09:46[0m] evaluated batches: 37000, 0:00:04
[[34m2023-05-14 15:09:51[0m] evaluated batches: 38000, 0:00:04
[[34m2023-05-14 15:09:55[0m] evaluated batches: 39000, 0:00:04
[[34m2023-05-14 15:10:00[0m] evaluated batches: 40000, 0:00:04
[[34m2023-05-14 15:10:05[0m] evaluated batches: 41000, 0:00:04
[[34m2023-05-14 15:10:10[0m] evaluated batches: 42000, 0:00:04
[[34m2023-05-14 15:10:15[0m] evaluated batches: 43000, 0:00:04
[[34m2023-05-14 15:10:20[0m] evaluated batches: 44000, 0:00:04
[[34m2023-05-14 15:10:25[0m] evaluated batches: 45000, 0:00:04
[[34m2023-05-14 15:10:29[0m] evaluated batches: 46000, 0:00:04
[[34m2023-05-14 15:10:34[0m] evaluated batches: 47000, 0:00:05
[[34m2023-05-14 15:10:39[0m] evaluated batches: 48000, 0:00:04
[[34m2023-05-14 15:10:44[0m] evaluated batches: 49000, 0:00:04
[[34m2023-05-14 15:10:49[0m] evaluated batches: 50000, 0:00:04
[[34m2023-05-14 15:10:54[0m] evaluated batches: 51000, 0:00:04
[[34m2023-05-14 15:10:59[0m] evaluated batches: 52000, 0:00:04
[[34m2023-05-14 15:11:03[0m] evaluated batches: 53000, 0:00:04
[[34m2023-05-14 15:11:08[0m] evaluated batches: 54000, 0:00:04
[[34m2023-05-14 15:11:13[0m] evaluated batches: 55000, 0:00:04
[[34m2023-05-14 15:11:18[0m] evaluated batches: 56000, 0:00:04
[[34m2023-05-14 15:11:23[0m] evaluated batches: 57000, 0:00:04
[[34m2023-05-14 15:11:28[0m] evaluated batches: 58000, 0:00:04
[[34m2023-05-14 15:11:33[0m] evaluated batches: 59000, 0:00:04
[[34m2023-05-14 15:11:37[0m] evaluated batches: 60000, 0:00:04
[[34m2023-05-14 15:11:42[0m] evaluated batches: 61000, 0:00:04
[[34m2023-05-14 15:11:47[0m] evaluated batches: 62000, 0:00:04
[[34m2023-05-14 15:11:52[0m] evaluated batches: 63000, 0:00:04
[[34m2023-05-14 15:11:57[0m] test loss = 0.388933, test auc = 0.766671
[[34m2023-05-14 15:11:57[0m] evaluated time: 0:05:10
[[34m2023-05-14 15:11:57[0m] analyse_structure
[[34m2023-05-14 15:12:26[0m] elapsed : 0:24:57, ETA : 3:39:42
[[34m2023-05-14 15:12:26[0m] epoch 2 / 10, batch 1000 / 50537, global_step = 51537, learning_rate = 1.000000e-02, loss = 0.595390, l2 = 0.004560, auc = 0.764400
elapsed : 0:24:57, ETA : 3:39:42
epoch 2 / 10, batch 1000 / 50537, global_step = 51537, learning_rate = 1.000000e-02, loss = 0.595390, l2 = 0.004560, auc = 0.764400
[[34m2023-05-14 15:12:54[0m] elapsed : 0:25:26, ETA : 3:39:13
[[34m2023-05-14 15:12:54[0m] epoch 2 / 10, batch 2000 / 50537, global_step = 52537, learning_rate = 1.000000e-02, loss = 0.386404, l2 = 0.002962, auc = 0.766630
elapsed : 0:25:26, ETA : 3:39:13
epoch 2 / 10, batch 2000 / 50537, global_step = 52537, learning_rate = 1.000000e-02, loss = 0.386404, l2 = 0.002962, auc = 0.766630
[[34m2023-05-14 15:13:23[0m] elapsed : 0:25:54, ETA : 3:38:35
[[34m2023-05-14 15:13:23[0m] epoch 2 / 10, batch 3000 / 50537, global_step = 53537, learning_rate = 1.000000e-02, loss = 0.388876, l2 = 0.002942, auc = 0.767377
elapsed : 0:25:54, ETA : 3:38:35
epoch 2 / 10, batch 3000 / 50537, global_step = 53537, learning_rate = 1.000000e-02, loss = 0.388876, l2 = 0.002942, auc = 0.767377
[[34m2023-05-14 15:13:52[0m] elapsed : 0:26:24, ETA : 3:38:14
[[34m2023-05-14 15:13:52[0m] epoch 2 / 10, batch 4000 / 50537, global_step = 54537, learning_rate = 1.000000e-02, loss = 0.388581, l2 = 0.002923, auc = 0.766925
elapsed : 0:26:24, ETA : 3:38:14
epoch 2 / 10, batch 4000 / 50537, global_step = 54537, learning_rate = 1.000000e-02, loss = 0.388581, l2 = 0.002923, auc = 0.766925
[[34m2023-05-14 15:14:31[0m] elapsed : 0:27:02, ETA : 3:38:57
[[34m2023-05-14 15:14:31[0m] epoch 2 / 10, batch 5000 / 50537, global_step = 55537, learning_rate = 1.000000e-02, loss = 0.387530, l2 = 0.002893, auc = 0.766714
elapsed : 0:27:02, ETA : 3:38:57
epoch 2 / 10, batch 5000 / 50537, global_step = 55537, learning_rate = 1.000000e-02, loss = 0.387530, l2 = 0.002893, auc = 0.766714
[[34m2023-05-14 15:15:11[0m] elapsed : 0:27:43, ETA : 3:40:02
[[34m2023-05-14 15:15:11[0m] epoch 2 / 10, batch 6000 / 50537, global_step = 56537, learning_rate = 1.000000e-02, loss = 0.386885, l2 = 0.002924, auc = 0.766981
elapsed : 0:27:43, ETA : 3:40:02
epoch 2 / 10, batch 6000 / 50537, global_step = 56537, learning_rate = 1.000000e-02, loss = 0.386885, l2 = 0.002924, auc = 0.766981
[[34m2023-05-14 15:15:52[0m] elapsed : 0:28:23, ETA : 3:40:55
[[34m2023-05-14 15:15:52[0m] epoch 2 / 10, batch 7000 / 50537, global_step = 57537, learning_rate = 1.000000e-02, loss = 0.384691, l2 = 0.002899, auc = 0.765468
elapsed : 0:28:23, ETA : 3:40:55
epoch 2 / 10, batch 7000 / 50537, global_step = 57537, learning_rate = 1.000000e-02, loss = 0.384691, l2 = 0.002899, auc = 0.765468
[[34m2023-05-14 15:16:32[0m] elapsed : 0:29:04, ETA : 3:41:52
[[34m2023-05-14 15:16:32[0m] epoch 2 / 10, batch 8000 / 50537, global_step = 58537, learning_rate = 1.000000e-02, loss = 0.387051, l2 = 0.002890, auc = 0.767539
elapsed : 0:29:04, ETA : 3:41:52
epoch 2 / 10, batch 8000 / 50537, global_step = 58537, learning_rate = 1.000000e-02, loss = 0.387051, l2 = 0.002890, auc = 0.767539
[[34m2023-05-14 15:17:13[0m] elapsed : 0:29:44, ETA : 3:42:39
[[34m2023-05-14 15:17:13[0m] epoch 2 / 10, batch 9000 / 50537, global_step = 59537, learning_rate = 1.000000e-02, loss = 0.389227, l2 = 0.002879, auc = 0.766270
elapsed : 0:29:44, ETA : 3:42:39
epoch 2 / 10, batch 9000 / 50537, global_step = 59537, learning_rate = 1.000000e-02, loss = 0.389227, l2 = 0.002879, auc = 0.766270
[[34m2023-05-14 15:17:53[0m] elapsed : 0:30:25, ETA : 3:43:30
[[34m2023-05-14 15:17:53[0m] epoch 2 / 10, batch 10000 / 50537, global_step = 60537, learning_rate = 1.000000e-02, loss = 0.386881, l2 = 0.002869, auc = 0.766829
elapsed : 0:30:25, ETA : 3:43:30
epoch 2 / 10, batch 10000 / 50537, global_step = 60537, learning_rate = 1.000000e-02, loss = 0.386881, l2 = 0.002869, auc = 0.766829
[[34m2023-05-14 15:18:34[0m] elapsed : 0:31:05, ETA : 3:44:11
[[34m2023-05-14 15:18:34[0m] epoch 2 / 10, batch 11000 / 50537, global_step = 61537, learning_rate = 1.000000e-02, loss = 0.387953, l2 = 0.002853, auc = 0.768548
elapsed : 0:31:05, ETA : 3:44:11
epoch 2 / 10, batch 11000 / 50537, global_step = 61537, learning_rate = 1.000000e-02, loss = 0.387953, l2 = 0.002853, auc = 0.768548
[[34m2023-05-14 15:19:16[0m] elapsed : 0:31:47, ETA : 3:45:03
[[34m2023-05-14 15:19:16[0m] epoch 2 / 10, batch 12000 / 50537, global_step = 62537, learning_rate = 1.000000e-02, loss = 0.384666, l2 = 0.002848, auc = 0.769693
elapsed : 0:31:47, ETA : 3:45:03
epoch 2 / 10, batch 12000 / 50537, global_step = 62537, learning_rate = 1.000000e-02, loss = 0.384666, l2 = 0.002848, auc = 0.769693
[[34m2023-05-14 15:19:57[0m] elapsed : 0:32:29, ETA : 3:45:53
[[34m2023-05-14 15:19:57[0m] epoch 2 / 10, batch 13000 / 50537, global_step = 63537, learning_rate = 1.000000e-02, loss = 0.387068, l2 = 0.002829, auc = 0.767581
elapsed : 0:32:29, ETA : 3:45:53
epoch 2 / 10, batch 13000 / 50537, global_step = 63537, learning_rate = 1.000000e-02, loss = 0.387068, l2 = 0.002829, auc = 0.767581
[[34m2023-05-14 15:20:39[0m] elapsed : 0:33:10, ETA : 3:46:33
[[34m2023-05-14 15:20:39[0m] epoch 2 / 10, batch 14000 / 50537, global_step = 64537, learning_rate = 1.000000e-02, loss = 0.388361, l2 = 0.002825, auc = 0.766022
elapsed : 0:33:10, ETA : 3:46:33
epoch 2 / 10, batch 14000 / 50537, global_step = 64537, learning_rate = 1.000000e-02, loss = 0.388361, l2 = 0.002825, auc = 0.766022
[[34m2023-05-14 15:21:20[0m] elapsed : 0:33:51, ETA : 3:47:10
[[34m2023-05-14 15:21:20[0m] epoch 2 / 10, batch 15000 / 50537, global_step = 65537, learning_rate = 1.000000e-02, loss = 0.387181, l2 = 0.002806, auc = 0.765850
elapsed : 0:33:51, ETA : 3:47:10
epoch 2 / 10, batch 15000 / 50537, global_step = 65537, learning_rate = 1.000000e-02, loss = 0.387181, l2 = 0.002806, auc = 0.765850
[[34m2023-05-14 15:22:01[0m] elapsed : 0:34:33, ETA : 3:47:52
[[34m2023-05-14 15:22:01[0m] epoch 2 / 10, batch 16000 / 50537, global_step = 66537, learning_rate = 1.000000e-02, loss = 0.385517, l2 = 0.002810, auc = 0.766563
elapsed : 0:34:33, ETA : 3:47:52
epoch 2 / 10, batch 16000 / 50537, global_step = 66537, learning_rate = 1.000000e-02, loss = 0.385517, l2 = 0.002810, auc = 0.766563
[[34m2023-05-14 15:22:43[0m] elapsed : 0:35:14, ETA : 3:48:24
[[34m2023-05-14 15:22:43[0m] epoch 2 / 10, batch 17000 / 50537, global_step = 67537, learning_rate = 1.000000e-02, loss = 0.385911, l2 = 0.002800, auc = 0.767243
elapsed : 0:35:14, ETA : 3:48:24
epoch 2 / 10, batch 17000 / 50537, global_step = 67537, learning_rate = 1.000000e-02, loss = 0.385911, l2 = 0.002800, auc = 0.767243
[[34m2023-05-14 15:23:25[0m] elapsed : 0:35:56, ETA : 3:49:01
[[34m2023-05-14 15:23:25[0m] epoch 2 / 10, batch 18000 / 50537, global_step = 68537, learning_rate = 1.000000e-02, loss = 0.384852, l2 = 0.002799, auc = 0.773086
elapsed : 0:35:56, ETA : 3:49:01
epoch 2 / 10, batch 18000 / 50537, global_step = 68537, learning_rate = 1.000000e-02, loss = 0.384852, l2 = 0.002799, auc = 0.773086
[[34m2023-05-14 15:24:06[0m] elapsed : 0:36:37, ETA : 3:49:30
[[34m2023-05-14 15:24:06[0m] epoch 2 / 10, batch 19000 / 50537, global_step = 69537, learning_rate = 1.000000e-02, loss = 0.386629, l2 = 0.002785, auc = 0.768784
elapsed : 0:36:37, ETA : 3:49:30
epoch 2 / 10, batch 19000 / 50537, global_step = 69537, learning_rate = 1.000000e-02, loss = 0.386629, l2 = 0.002785, auc = 0.768784
[[34m2023-05-14 15:24:48[0m] elapsed : 0:37:19, ETA : 3:50:02
[[34m2023-05-14 15:24:48[0m] epoch 2 / 10, batch 20000 / 50537, global_step = 70537, learning_rate = 1.000000e-02, loss = 0.386211, l2 = 0.002777, auc = 0.767580
elapsed : 0:37:19, ETA : 3:50:02
epoch 2 / 10, batch 20000 / 50537, global_step = 70537, learning_rate = 1.000000e-02, loss = 0.386211, l2 = 0.002777, auc = 0.767580
[[34m2023-05-14 15:25:26[0m] elapsed : 0:37:57, ETA : 3:50:08
[[34m2023-05-14 15:25:26[0m] epoch 2 / 10, batch 21000 / 50537, global_step = 71537, learning_rate = 1.000000e-02, loss = 0.383865, l2 = 0.002773, auc = 0.771302
elapsed : 0:37:57, ETA : 3:50:08
epoch 2 / 10, batch 21000 / 50537, global_step = 71537, learning_rate = 1.000000e-02, loss = 0.383865, l2 = 0.002773, auc = 0.771302
[[34m2023-05-14 15:26:06[0m] elapsed : 0:38:37, ETA : 3:50:25
[[34m2023-05-14 15:26:06[0m] epoch 2 / 10, batch 22000 / 50537, global_step = 72537, learning_rate = 1.000000e-02, loss = 0.384815, l2 = 0.002758, auc = 0.770935
elapsed : 0:38:37, ETA : 3:50:25
epoch 2 / 10, batch 22000 / 50537, global_step = 72537, learning_rate = 1.000000e-02, loss = 0.384815, l2 = 0.002758, auc = 0.770935
[[34m2023-05-14 15:26:46[0m] elapsed : 0:39:18, ETA : 3:50:46
[[34m2023-05-14 15:26:46[0m] epoch 2 / 10, batch 23000 / 50537, global_step = 73537, learning_rate = 1.000000e-02, loss = 0.383412, l2 = 0.002738, auc = 0.771203
elapsed : 0:39:18, ETA : 3:50:46
epoch 2 / 10, batch 23000 / 50537, global_step = 73537, learning_rate = 1.000000e-02, loss = 0.383412, l2 = 0.002738, auc = 0.771203
[[34m2023-05-14 15:27:27[0m] elapsed : 0:39:58, ETA : 3:51:00
[[34m2023-05-14 15:27:27[0m] epoch 2 / 10, batch 24000 / 50537, global_step = 74537, learning_rate = 1.000000e-02, loss = 0.385044, l2 = 0.002733, auc = 0.771262
elapsed : 0:39:58, ETA : 3:51:00
epoch 2 / 10, batch 24000 / 50537, global_step = 74537, learning_rate = 1.000000e-02, loss = 0.385044, l2 = 0.002733, auc = 0.771262
[[34m2023-05-14 15:28:07[0m] elapsed : 0:40:39, ETA : 3:51:18
[[34m2023-05-14 15:28:07[0m] epoch 2 / 10, batch 25000 / 50537, global_step = 75537, learning_rate = 1.000000e-02, loss = 0.386801, l2 = 0.002734, auc = 0.769738
elapsed : 0:40:39, ETA : 3:51:18
epoch 2 / 10, batch 25000 / 50537, global_step = 75537, learning_rate = 1.000000e-02, loss = 0.386801, l2 = 0.002734, auc = 0.769738
[[34m2023-05-14 15:28:47[0m] elapsed : 0:41:19, ETA : 3:51:29
[[34m2023-05-14 15:28:47[0m] epoch 2 / 10, batch 26000 / 50537, global_step = 76537, learning_rate = 1.000000e-02, loss = 0.384782, l2 = 0.002737, auc = 0.767624
elapsed : 0:41:19, ETA : 3:51:29
epoch 2 / 10, batch 26000 / 50537, global_step = 76537, learning_rate = 1.000000e-02, loss = 0.384782, l2 = 0.002737, auc = 0.767624
[[34m2023-05-14 15:29:28[0m] elapsed : 0:41:59, ETA : 3:51:39
[[34m2023-05-14 15:29:28[0m] epoch 2 / 10, batch 27000 / 50537, global_step = 77537, learning_rate = 1.000000e-02, loss = 0.387295, l2 = 0.002734, auc = 0.770936
elapsed : 0:41:59, ETA : 3:51:39
epoch 2 / 10, batch 27000 / 50537, global_step = 77537, learning_rate = 1.000000e-02, loss = 0.387295, l2 = 0.002734, auc = 0.770936
[[34m2023-05-14 15:30:09[0m] elapsed : 0:42:40, ETA : 3:51:53
[[34m2023-05-14 15:30:09[0m] epoch 2 / 10, batch 28000 / 50537, global_step = 78537, learning_rate = 1.000000e-02, loss = 0.385233, l2 = 0.002731, auc = 0.768922
elapsed : 0:42:40, ETA : 3:51:53
epoch 2 / 10, batch 28000 / 50537, global_step = 78537, learning_rate = 1.000000e-02, loss = 0.385233, l2 = 0.002731, auc = 0.768922
[[34m2023-05-14 15:30:50[0m] elapsed : 0:43:21, ETA : 3:52:05
[[34m2023-05-14 15:30:50[0m] epoch 2 / 10, batch 29000 / 50537, global_step = 79537, learning_rate = 1.000000e-02, loss = 0.385610, l2 = 0.002710, auc = 0.770899
elapsed : 0:43:21, ETA : 3:52:05
epoch 2 / 10, batch 29000 / 50537, global_step = 79537, learning_rate = 1.000000e-02, loss = 0.385610, l2 = 0.002710, auc = 0.770899
[[34m2023-05-14 15:31:30[0m] elapsed : 0:44:01, ETA : 3:52:11
[[34m2023-05-14 15:31:30[0m] epoch 2 / 10, batch 30000 / 50537, global_step = 80537, learning_rate = 1.000000e-02, loss = 0.384289, l2 = 0.002707, auc = 0.771580
elapsed : 0:44:01, ETA : 3:52:11
epoch 2 / 10, batch 30000 / 50537, global_step = 80537, learning_rate = 1.000000e-02, loss = 0.384289, l2 = 0.002707, auc = 0.771580
[[34m2023-05-14 15:32:11[0m] elapsed : 0:44:42, ETA : 3:52:21
[[34m2023-05-14 15:32:11[0m] epoch 2 / 10, batch 31000 / 50537, global_step = 81537, learning_rate = 1.000000e-02, loss = 0.384947, l2 = 0.002709, auc = 0.770679
elapsed : 0:44:42, ETA : 3:52:21
epoch 2 / 10, batch 31000 / 50537, global_step = 81537, learning_rate = 1.000000e-02, loss = 0.384947, l2 = 0.002709, auc = 0.770679
[[34m2023-05-14 15:33:00[0m] elapsed : 0:45:31, ETA : 3:53:10
[[34m2023-05-14 15:33:00[0m] epoch 2 / 10, batch 32000 / 50537, global_step = 82537, learning_rate = 1.000000e-02, loss = 0.386377, l2 = 0.002697, auc = 0.769267
elapsed : 0:45:31, ETA : 3:53:10
epoch 2 / 10, batch 32000 / 50537, global_step = 82537, learning_rate = 1.000000e-02, loss = 0.386377, l2 = 0.002697, auc = 0.769267
[[34m2023-05-14 15:33:53[0m] elapsed : 0:46:24, ETA : 3:54:18
[[34m2023-05-14 15:33:53[0m] epoch 2 / 10, batch 33000 / 50537, global_step = 83537, learning_rate = 1.000000e-02, loss = 0.385162, l2 = 0.002689, auc = 0.769396
elapsed : 0:46:24, ETA : 3:54:18
epoch 2 / 10, batch 33000 / 50537, global_step = 83537, learning_rate = 1.000000e-02, loss = 0.385162, l2 = 0.002689, auc = 0.769396
[[34m2023-05-14 15:34:45[0m] elapsed : 0:47:17, ETA : 3:55:22
[[34m2023-05-14 15:34:45[0m] epoch 2 / 10, batch 34000 / 50537, global_step = 84537, learning_rate = 1.000000e-02, loss = 0.385304, l2 = 0.002684, auc = 0.769537
elapsed : 0:47:17, ETA : 3:55:22
epoch 2 / 10, batch 34000 / 50537, global_step = 84537, learning_rate = 1.000000e-02, loss = 0.385304, l2 = 0.002684, auc = 0.769537
