nohup: ignoring input
2023-05-13 17:11:32.753143: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-13 17:11:33.710591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Got hdf Criteo-8d data set, getting metadata...
Initialization finished!
allocate all embs
current config:  [15, 12, 4, 17, 4, 4, 4, 17, 2, 4, 12, 24, 13, 8, 24, 25, 0, 0, 4, 24, 16, 16, 17, 25, 21, 17, 8, 23, 4, 8, 0, 16, 4, 21, 8, 23, 23, 17, 23]
full_emb_for_0_emb_size_15 initialized from: -0.03691067352627811 0.03691067352627811
full_emb_for_1_emb_size_12 initialized from: -0.02736561135755131 0.02736561135755131
full_emb_for_2_emb_size_4 initialized from: -0.13423121104280486 0.13423121104280486
full_emb_for_3_emb_size_17 initialized from: -0.028380931014817347 0.028380931014817347
full_emb_for_4_emb_size_4 initialized from: -0.04758309514308865 0.04758309514308865
full_emb_for_5_emb_size_4 initialized from: -0.11785113019775792 0.11785113019775792
full_emb_for_6_emb_size_4 initialized from: -0.15911145683514602 0.15911145683514602
full_emb_for_7_emb_size_17 initialized from: -0.030816677568068284 0.030816677568068284
full_emb_for_8_emb_size_2 initialized from: -0.1421338109037403 0.1421338109037403
full_emb_for_9_emb_size_4 initialized from: -0.6324555320336759 0.6324555320336759
full_emb_for_10_emb_size_12 initialized from: -0.18009006755629928 0.18009006755629928
full_emb_for_11_emb_size_24 initialized from: -0.0058277261698637135 0.0058277261698637135
full_emb_for_12_emb_size_13 initialized from: -0.10016708449412667 0.10016708449412667
full_emb_for_13_emb_size_8 initialized from: -0.0063860510691885145 0.0063860510691885145
full_emb_for_14_emb_size_24 initialized from: -0.01737751292933602 0.01737751292933602
full_emb_for_15_emb_size_25 initialized from: -0.020097373193773395 0.020097373193773395
full_emb_for_18_emb_size_4 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_19_emb_size_24 initialized from: -0.029992502811328637 0.029992502811328637
full_emb_for_20_emb_size_16 initialized from: -0.0682523632789935 0.0682523632789935
full_emb_for_21_emb_size_16 initialized from: -0.3110855084191276 0.3110855084191276
full_emb_for_22_emb_size_17 initialized from: -0.0065209225253894925 0.0065209225253894925
full_emb_for_23_emb_size_25 initialized from: -0.009651892169482854 0.009651892169482854
full_emb_for_24_emb_size_21 initialized from: -0.009704241543027231 0.009704241543027231
full_emb_for_25_emb_size_17 initialized from: -0.4629100498862757 0.4629100498862757
full_emb_for_26_emb_size_8 initialized from: -0.052655894762455135 0.052655894762455135
full_emb_for_27_emb_size_23 initialized from: -0.027683594464555476 0.027683594464555476
full_emb_for_28_emb_size_4 initialized from: -0.3038218101251 0.3038218101251
full_emb_for_29_emb_size_8 initialized from: -0.6793662204867574 0.6793662204867574
full_emb_for_31_emb_size_16 initialized from: -0.43994134506405985 0.43994134506405985
full_emb_for_32_emb_size_4 initialized from: -0.006380285938708977 0.006380285938708977
full_emb_for_33_emb_size_21 initialized from: -0.007181062370267826 0.007181062370267826
full_emb_for_34_emb_size_8 initialized from: -0.006418481711859752 0.006418481711859752
full_emb_for_35_emb_size_23 initialized from: -0.010241025486591067 0.010241025486591067
full_emb_for_36_emb_size_23 initialized from: -0.02535915646704869 0.02535915646704869
full_emb_for_37_emb_size_17 initialized from: -0.27386127875258304 0.27386127875258304
full_emb_for_38_emb_size_23 initialized from: -0.3244428422615251 0.3244428422615251
allocate all bias
current config:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]
full_emb_for_0_emb_size_1 initialized from: -0.03696948196568264 0.03696948196568264
full_emb_for_1_emb_size_1 initialized from: -0.02738441640271498 0.02738441640271498
full_emb_for_2_emb_size_1 initialized from: -0.13483997249264842 0.13483997249264842
full_emb_for_3_emb_size_1 initialized from: -0.02841146046402596 0.02841146046402596
full_emb_for_4_emb_size_1 initialized from: -0.04761005186046748 0.04761005186046748
full_emb_for_5_emb_size_1 initialized from: -0.11826247919781652 0.11826247919781652
full_emb_for_6_emb_size_1 initialized from: -0.16012815380508713 0.16012815380508713
full_emb_for_7_emb_size_1 initialized from: -0.03085577263937756 0.03085577263937756
full_emb_for_8_emb_size_1 initialized from: -0.14237369936287486 0.14237369936287486
full_emb_for_9_emb_size_1 initialized from: -0.7071067811865476 0.7071067811865476
full_emb_for_10_emb_size_1 initialized from: -0.18569533817705186 0.18569533817705186
full_emb_for_11_emb_size_1 initialized from: -0.005828105560326563 0.005828105560326563
full_emb_for_12_emb_size_1 initialized from: -0.10118748860323272 0.10118748860323272
full_emb_for_13_emb_size_1 initialized from: -0.0063862029942614255 0.0063862029942614255
full_emb_for_14_emb_size_1 initialized from: -0.017387579619448895 0.017387579619448895
full_emb_for_15_emb_size_1 initialized from: -0.020113627727557096 0.020113627727557096
full_emb_for_18_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_19_emb_size_1 initialized from: -0.03004434814442642 0.03004434814442642
full_emb_for_20_emb_size_1 initialized from: -0.06865330091576084 0.06865330091576084
full_emb_for_21_emb_size_1 initialized from: -0.3572948005052482 0.3572948005052482
full_emb_for_22_emb_size_1 initialized from: -0.0065212922708019596 0.0065212922708019596
full_emb_for_23_emb_size_1 initialized from: -0.009653690993907272 0.009653690993907272
full_emb_for_24_emb_size_1 initialized from: -0.00970576501968314 0.00970576501968314
full_emb_for_25_emb_size_1 initialized from: -0.7071067811865476 0.7071067811865476
full_emb_for_26_emb_size_1 initialized from: -0.052741266274988985 0.052741266274988985
full_emb_for_27_emb_size_1 initialized from: -0.027722572984866366 0.027722572984866366
full_emb_for_28_emb_size_1 initialized from: -0.3110855084191276 0.3110855084191276
full_emb_for_29_emb_size_1 initialized from: -1.0 1.0
full_emb_for_31_emb_size_1 initialized from: -0.6123724356957945 0.6123724356957945
full_emb_for_32_emb_size_1 initialized from: -0.006380350871947833 0.006380350871947833
full_emb_for_33_emb_size_1 initialized from: -0.007181679634111962 0.007181679634111962
full_emb_for_34_emb_size_1 initialized from: -0.00641863596335187 0.00641863596335187
full_emb_for_35_emb_size_1 initialized from: -0.010242995172850529 0.010242995172850529
full_emb_for_36_emb_size_1 initialized from: -0.025389107701102583 0.025389107701102583
full_emb_for_37_emb_size_1 initialized from: -0.30618621784789724 0.30618621784789724
full_emb_for_38_emb_size_1 initialized from: -0.4140393356054125 0.4140393356054125
w_0 initialized from: -0.07050533657462563 0.07050533657462563
(507, 700) (700,)
relu
w_1 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_2 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_3 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_4 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_5 initialized from: -0.0925159507394634 0.0925159507394634
(700, 1) (1,)
none
[[34m2023-05-13 17:11:37[0m] Experiment directory created at /home/ubuntu/results/retrain_irazor/criteo/003-[700, 700, 700, 700, 700, 1]-bs-500
[[34m2023-05-13 17:11:37[0m] Batchsize: 500
wandb: Currently logged in as: yao-yao. Use `wandb login --relogin` to force relogin
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /tmp/wandb/run-20230513_171138-q6xr2yao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run criteo-BS-500-003-retrain_irazor-2023-05-13 17:11:37
wandb: â­ï¸ View project at https://wandb.ai/yao-yao/irazor
wandb: ðŸš€ View run at https://wandb.ai/yao-yao/irazor/runs/q6xr2yao
2023-05-13 17:11:43.322239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 17:11:43.323853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 17:11:43.324649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 17:11:44.849018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 17:11:44.849943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 17:11:44.850726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 17:11:44.851536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37804 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
[[34m2023-05-13 17:11:44[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
add l2 0.001 Tensor("concat:0", shape=(?, 507), dtype=float32)
add l2 0.001 Tensor("concat_1:0", shape=(?, 36), dtype=float32)
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
[[34m2023-05-13 17:11:45[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2023-05-13 17:11:46.263311: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
[[34m2023-05-13 17:11:46[0m] total batches: 173770	batch per epoch: 17377
[[34m2023-05-13 17:11:46[0m] new iteration
new iteration
on disk...
2023-05-13 17:11:48.490850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
[[34m2023-05-13 17:11:58[0m] elapsed : 0:00:11, ETA : 0:31:40
[[34m2023-05-13 17:11:58[0m] epoch 1 / 10, batch 1000 / 17377, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.608680, l2 = 0.438358, auc = 0.742357
elapsed : 0:00:11, ETA : 0:31:40
epoch 1 / 10, batch 1000 / 17377, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.608680, l2 = 0.438358, auc = 0.742357
[[34m2023-05-13 17:12:18[0m] elapsed : 0:00:31, ETA : 0:44:22
[[34m2023-05-13 17:12:18[0m] epoch 1 / 10, batch 2000 / 17377, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.578540, l2 = 0.071814, auc = 0.764020
elapsed : 0:00:31, ETA : 0:44:22
epoch 1 / 10, batch 2000 / 17377, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.578540, l2 = 0.071814, auc = 0.764020
[[34m2023-05-13 17:12:43[0m] elapsed : 0:00:56, ETA : 0:53:07
[[34m2023-05-13 17:12:43[0m] epoch 1 / 10, batch 3000 / 17377, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.573621, l2 = 0.031114, auc = 0.769192
elapsed : 0:00:56, ETA : 0:53:07
epoch 1 / 10, batch 3000 / 17377, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.573621, l2 = 0.031114, auc = 0.769192
[[34m2023-05-13 17:13:08[0m] elapsed : 0:01:21, ETA : 0:57:17
[[34m2023-05-13 17:13:08[0m] epoch 1 / 10, batch 4000 / 17377, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.571393, l2 = 0.019981, auc = 0.771271
elapsed : 0:01:21, ETA : 0:57:17
epoch 1 / 10, batch 4000 / 17377, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.571393, l2 = 0.019981, auc = 0.771271
[[34m2023-05-13 17:13:33[0m] elapsed : 0:01:46, ETA : 0:59:37
[[34m2023-05-13 17:13:33[0m] epoch 1 / 10, batch 5000 / 17377, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.567606, l2 = 0.016038, auc = 0.775373
elapsed : 0:01:46, ETA : 0:59:37
epoch 1 / 10, batch 5000 / 17377, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.567606, l2 = 0.016038, auc = 0.775373
[[34m2023-05-13 17:13:58[0m] elapsed : 0:02:11, ETA : 1:01:02
[[34m2023-05-13 17:13:58[0m] epoch 1 / 10, batch 6000 / 17377, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.565478, l2 = 0.014473, auc = 0.777294
elapsed : 0:02:11, ETA : 1:01:02
epoch 1 / 10, batch 6000 / 17377, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.565478, l2 = 0.014473, auc = 0.777294
[[34m2023-05-13 17:14:23[0m] elapsed : 0:02:36, ETA : 1:01:56
[[34m2023-05-13 17:14:23[0m] epoch 1 / 10, batch 7000 / 17377, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.565391, l2 = 0.013732, auc = 0.777354
elapsed : 0:02:36, ETA : 1:01:56
epoch 1 / 10, batch 7000 / 17377, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.565391, l2 = 0.013732, auc = 0.777354
[[34m2023-05-13 17:14:48[0m] elapsed : 0:03:02, ETA : 1:02:51
[[34m2023-05-13 17:14:48[0m] epoch 1 / 10, batch 8000 / 17377, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.564273, l2 = 0.013235, auc = 0.778465
elapsed : 0:03:02, ETA : 1:02:51
epoch 1 / 10, batch 8000 / 17377, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.564273, l2 = 0.013235, auc = 0.778465
[[34m2023-05-13 17:15:14[0m] elapsed : 0:03:27, ETA : 1:03:09
[[34m2023-05-13 17:15:14[0m] epoch 1 / 10, batch 9000 / 17377, global_step = 9000, learning_rate = 1.000000e-02, loss = 0.562873, l2 = 0.012974, auc = 0.779873
elapsed : 0:03:27, ETA : 1:03:09
epoch 1 / 10, batch 9000 / 17377, global_step = 9000, learning_rate = 1.000000e-02, loss = 0.562873, l2 = 0.012974, auc = 0.779873
[[34m2023-05-13 17:15:39[0m] elapsed : 0:03:52, ETA : 1:03:19
[[34m2023-05-13 17:15:39[0m] epoch 1 / 10, batch 10000 / 17377, global_step = 10000, learning_rate = 1.000000e-02, loss = 0.560802, l2 = 0.012725, auc = 0.781994
elapsed : 0:03:52, ETA : 1:03:19
epoch 1 / 10, batch 10000 / 17377, global_step = 10000, learning_rate = 1.000000e-02, loss = 0.560802, l2 = 0.012725, auc = 0.781994
[[34m2023-05-13 17:16:04[0m] elapsed : 0:04:17, ETA : 1:03:22
[[34m2023-05-13 17:16:04[0m] epoch 1 / 10, batch 11000 / 17377, global_step = 11000, learning_rate = 1.000000e-02, loss = 0.560112, l2 = 0.012527, auc = 0.782400
elapsed : 0:04:17, ETA : 1:03:22
epoch 1 / 10, batch 11000 / 17377, global_step = 11000, learning_rate = 1.000000e-02, loss = 0.560112, l2 = 0.012527, auc = 0.782400
[[34m2023-05-13 17:16:29[0m] elapsed : 0:04:42, ETA : 1:03:21
[[34m2023-05-13 17:16:29[0m] epoch 1 / 10, batch 12000 / 17377, global_step = 12000, learning_rate = 1.000000e-02, loss = 0.560210, l2 = 0.012354, auc = 0.782360
elapsed : 0:04:42, ETA : 1:03:21
epoch 1 / 10, batch 12000 / 17377, global_step = 12000, learning_rate = 1.000000e-02, loss = 0.560210, l2 = 0.012354, auc = 0.782360
[[34m2023-05-13 17:16:55[0m] elapsed : 0:05:08, ETA : 1:03:29
[[34m2023-05-13 17:16:55[0m] epoch 1 / 10, batch 13000 / 17377, global_step = 13000, learning_rate = 1.000000e-02, loss = 0.559478, l2 = 0.012177, auc = 0.783120
elapsed : 0:05:08, ETA : 1:03:29
epoch 1 / 10, batch 13000 / 17377, global_step = 13000, learning_rate = 1.000000e-02, loss = 0.559478, l2 = 0.012177, auc = 0.783120
[[34m2023-05-13 17:17:17[0m] elapsed : 0:05:31, ETA : 1:02:57
[[34m2023-05-13 17:17:17[0m] epoch 1 / 10, batch 14000 / 17377, global_step = 14000, learning_rate = 1.000000e-02, loss = 0.558174, l2 = 0.012050, auc = 0.784538
elapsed : 0:05:31, ETA : 1:02:57
epoch 1 / 10, batch 14000 / 17377, global_step = 14000, learning_rate = 1.000000e-02, loss = 0.558174, l2 = 0.012050, auc = 0.784538
[[34m2023-05-13 17:17:35[0m] elapsed : 0:05:49, ETA : 1:01:34
[[34m2023-05-13 17:17:35[0m] epoch 1 / 10, batch 15000 / 17377, global_step = 15000, learning_rate = 1.000000e-02, loss = 0.558056, l2 = 0.011913, auc = 0.784549
elapsed : 0:05:49, ETA : 1:01:34
epoch 1 / 10, batch 15000 / 17377, global_step = 15000, learning_rate = 1.000000e-02, loss = 0.558056, l2 = 0.011913, auc = 0.784549
[[34m2023-05-13 17:17:53[0m] elapsed : 0:06:06, ETA : 1:00:08
[[34m2023-05-13 17:17:53[0m] epoch 1 / 10, batch 16000 / 17377, global_step = 16000, learning_rate = 1.000000e-02, loss = 0.559334, l2 = 0.011829, auc = 0.783357
elapsed : 0:06:06, ETA : 1:00:08
epoch 1 / 10, batch 16000 / 17377, global_step = 16000, learning_rate = 1.000000e-02, loss = 0.559334, l2 = 0.011829, auc = 0.783357
[[34m2023-05-13 17:18:12[0m] elapsed : 0:06:25, ETA : 0:59:10
[[34m2023-05-13 17:18:12[0m] epoch 1 / 10, batch 17000 / 17377, global_step = 17000, learning_rate = 1.000000e-02, loss = 0.557843, l2 = 0.011687, auc = 0.784784
elapsed : 0:06:25, ETA : 0:59:10
epoch 1 / 10, batch 17000 / 17377, global_step = 17000, learning_rate = 1.000000e-02, loss = 0.557843, l2 = 0.011687, auc = 0.784784
[[34m2023-05-13 17:18:18[0m] running test...
on disk...
[[34m2023-05-13 17:18:22[0m] evaluated batches: 1000, 0:00:03
[[34m2023-05-13 17:18:25[0m] evaluated batches: 2000, 0:00:02
[[34m2023-05-13 17:18:28[0m] evaluated batches: 3000, 0:00:02
[[34m2023-05-13 17:18:30[0m] evaluated batches: 4000, 0:00:02
[[34m2023-05-13 17:18:34[0m] evaluated batches: 5000, 0:00:03
[[34m2023-05-13 17:18:37[0m] evaluated batches: 6000, 0:00:02
[[34m2023-05-13 17:18:39[0m] evaluated batches: 7000, 0:00:02
[[34m2023-05-13 17:18:42[0m] evaluated batches: 8000, 0:00:02
[[34m2023-05-13 17:18:46[0m] evaluated batches: 9000, 0:00:03
[[34m2023-05-13 17:18:49[0m] evaluated batches: 10000, 0:00:02
[[34m2023-05-13 17:18:51[0m] evaluated batches: 11000, 0:00:02
[[34m2023-05-13 17:18:54[0m] evaluated batches: 12000, 0:00:02
[[34m2023-05-13 17:18:58[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 17:19:01[0m] evaluated batches: 14000, 0:00:02
[[34m2023-05-13 17:19:03[0m] evaluated batches: 15000, 0:00:02
[[34m2023-05-13 17:19:06[0m] evaluated batches: 16000, 0:00:02
[[34m2023-05-13 17:19:10[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 17:19:13[0m] evaluated batches: 18000, 0:00:02
[[34m2023-05-13 17:19:15[0m] evaluated batches: 19000, 0:00:02
[[34m2023-05-13 17:19:18[0m] evaluated batches: 20000, 0:00:02
[[34m2023-05-13 17:19:22[0m] evaluated batches: 21000, 0:00:03
[[34m2023-05-13 17:19:24[0m] evaluated batches: 22000, 0:00:02
[[34m2023-05-13 17:19:27[0m] evaluated batches: 23000, 0:00:02
[[34m2023-05-13 17:19:30[0m] evaluated batches: 24000, 0:00:02
[[34m2023-05-13 17:19:33[0m] evaluated batches: 25000, 0:00:02
[[34m2023-05-13 17:19:42[0m] test loss = 0.558011, test auc = 0.784661
[[34m2023-05-13 17:19:42[0m] evaluated time: 0:01:23
[[34m2023-05-13 17:19:42[0m] analyse_structure
[[34m2023-05-13 17:20:06[0m] elapsed : 0:08:19, ETA : 1:10:19
[[34m2023-05-13 17:20:06[0m] epoch 2 / 10, batch 1000 / 17377, global_step = 18377, learning_rate = 1.000000e-02, loss = 0.766713, l2 = 0.015918, auc = 0.785901
elapsed : 0:08:19, ETA : 1:10:19
epoch 2 / 10, batch 1000 / 17377, global_step = 18377, learning_rate = 1.000000e-02, loss = 0.766713, l2 = 0.015918, auc = 0.785901
[[34m2023-05-13 17:20:31[0m] elapsed : 0:08:44, ETA : 1:09:35
[[34m2023-05-13 17:20:31[0m] epoch 2 / 10, batch 2000 / 17377, global_step = 19377, learning_rate = 1.000000e-02, loss = 0.557765, l2 = 0.011420, auc = 0.784921
elapsed : 0:08:44, ETA : 1:09:35
epoch 2 / 10, batch 2000 / 17377, global_step = 19377, learning_rate = 1.000000e-02, loss = 0.557765, l2 = 0.011420, auc = 0.784921
[[34m2023-05-13 17:20:56[0m] elapsed : 0:09:09, ETA : 1:08:52
[[34m2023-05-13 17:20:56[0m] epoch 2 / 10, batch 3000 / 17377, global_step = 20377, learning_rate = 1.000000e-02, loss = 0.555871, l2 = 0.011336, auc = 0.786688
elapsed : 0:09:09, ETA : 1:08:52
epoch 2 / 10, batch 3000 / 17377, global_step = 20377, learning_rate = 1.000000e-02, loss = 0.555871, l2 = 0.011336, auc = 0.786688
[[34m2023-05-13 17:21:21[0m] elapsed : 0:09:35, ETA : 1:08:19
[[34m2023-05-13 17:21:21[0m] epoch 2 / 10, batch 4000 / 17377, global_step = 21377, learning_rate = 1.000000e-02, loss = 0.555909, l2 = 0.011269, auc = 0.786539
elapsed : 0:09:35, ETA : 1:08:19
epoch 2 / 10, batch 4000 / 17377, global_step = 21377, learning_rate = 1.000000e-02, loss = 0.555909, l2 = 0.011269, auc = 0.786539
[[34m2023-05-13 17:21:46[0m] elapsed : 0:10:00, ETA : 1:07:39
[[34m2023-05-13 17:21:46[0m] epoch 2 / 10, batch 5000 / 17377, global_step = 22377, learning_rate = 1.000000e-02, loss = 0.556268, l2 = 0.011177, auc = 0.786269
elapsed : 0:10:00, ETA : 1:07:39
epoch 2 / 10, batch 5000 / 17377, global_step = 22377, learning_rate = 1.000000e-02, loss = 0.556268, l2 = 0.011177, auc = 0.786269
[[34m2023-05-13 17:22:12[0m] elapsed : 0:10:25, ETA : 1:07:00
[[34m2023-05-13 17:22:12[0m] epoch 2 / 10, batch 6000 / 17377, global_step = 23377, learning_rate = 1.000000e-02, loss = 0.555632, l2 = 0.011073, auc = 0.786757
elapsed : 0:10:25, ETA : 1:07:00
epoch 2 / 10, batch 6000 / 17377, global_step = 23377, learning_rate = 1.000000e-02, loss = 0.555632, l2 = 0.011073, auc = 0.786757
[[34m2023-05-13 17:22:37[0m] elapsed : 0:10:50, ETA : 1:06:23
[[34m2023-05-13 17:22:37[0m] epoch 2 / 10, batch 7000 / 17377, global_step = 24377, learning_rate = 1.000000e-02, loss = 0.554509, l2 = 0.011011, auc = 0.788032
elapsed : 0:10:50, ETA : 1:06:23
epoch 2 / 10, batch 7000 / 17377, global_step = 24377, learning_rate = 1.000000e-02, loss = 0.554509, l2 = 0.011011, auc = 0.788032
[[34m2023-05-13 17:23:02[0m] elapsed : 0:11:15, ETA : 1:05:47
[[34m2023-05-13 17:23:02[0m] epoch 2 / 10, batch 8000 / 17377, global_step = 25377, learning_rate = 1.000000e-02, loss = 0.555114, l2 = 0.010940, auc = 0.787452
elapsed : 0:11:15, ETA : 1:05:47
epoch 2 / 10, batch 8000 / 17377, global_step = 25377, learning_rate = 1.000000e-02, loss = 0.555114, l2 = 0.010940, auc = 0.787452
[[34m2023-05-13 17:23:27[0m] elapsed : 0:11:40, ETA : 1:05:11
[[34m2023-05-13 17:23:27[0m] epoch 2 / 10, batch 9000 / 17377, global_step = 26377, learning_rate = 1.000000e-02, loss = 0.554639, l2 = 0.010877, auc = 0.787953
elapsed : 0:11:40, ETA : 1:05:11
epoch 2 / 10, batch 9000 / 17377, global_step = 26377, learning_rate = 1.000000e-02, loss = 0.554639, l2 = 0.010877, auc = 0.787953
[[34m2023-05-13 17:23:52[0m] elapsed : 0:12:05, ETA : 1:04:36
[[34m2023-05-13 17:23:52[0m] epoch 2 / 10, batch 10000 / 17377, global_step = 27377, learning_rate = 1.000000e-02, loss = 0.553531, l2 = 0.010799, auc = 0.788985
elapsed : 0:12:05, ETA : 1:04:36
epoch 2 / 10, batch 10000 / 17377, global_step = 27377, learning_rate = 1.000000e-02, loss = 0.553531, l2 = 0.010799, auc = 0.788985
[[34m2023-05-13 17:24:18[0m] elapsed : 0:12:31, ETA : 1:04:07
[[34m2023-05-13 17:24:18[0m] epoch 2 / 10, batch 11000 / 17377, global_step = 28377, learning_rate = 1.000000e-02, loss = 0.554716, l2 = 0.010753, auc = 0.787690
elapsed : 0:12:31, ETA : 1:04:07
epoch 2 / 10, batch 11000 / 17377, global_step = 28377, learning_rate = 1.000000e-02, loss = 0.554716, l2 = 0.010753, auc = 0.787690
[[34m2023-05-13 17:24:43[0m] elapsed : 0:12:56, ETA : 1:03:34
[[34m2023-05-13 17:24:43[0m] epoch 2 / 10, batch 12000 / 17377, global_step = 29377, learning_rate = 1.000000e-02, loss = 0.553737, l2 = 0.010691, auc = 0.788835
elapsed : 0:12:56, ETA : 1:03:34
epoch 2 / 10, batch 12000 / 17377, global_step = 29377, learning_rate = 1.000000e-02, loss = 0.553737, l2 = 0.010691, auc = 0.788835
[[34m2023-05-13 17:25:03[0m] elapsed : 0:13:17, ETA : 1:02:42
[[34m2023-05-13 17:25:03[0m] epoch 2 / 10, batch 13000 / 17377, global_step = 30377, learning_rate = 1.000000e-02, loss = 0.555094, l2 = 0.010598, auc = 0.787530
elapsed : 0:13:17, ETA : 1:02:42
epoch 2 / 10, batch 13000 / 17377, global_step = 30377, learning_rate = 1.000000e-02, loss = 0.555094, l2 = 0.010598, auc = 0.787530
[[34m2023-05-13 17:25:21[0m] elapsed : 0:13:35, ETA : 1:01:38
[[34m2023-05-13 17:25:21[0m] epoch 2 / 10, batch 14000 / 17377, global_step = 31377, learning_rate = 1.000000e-02, loss = 0.553817, l2 = 0.010528, auc = 0.788611
elapsed : 0:13:35, ETA : 1:01:38
epoch 2 / 10, batch 14000 / 17377, global_step = 31377, learning_rate = 1.000000e-02, loss = 0.553817, l2 = 0.010528, auc = 0.788611
[[34m2023-05-13 17:25:40[0m] elapsed : 0:13:53, ETA : 1:00:37
[[34m2023-05-13 17:25:40[0m] epoch 2 / 10, batch 15000 / 17377, global_step = 32377, learning_rate = 1.000000e-02, loss = 0.553120, l2 = 0.010475, auc = 0.789517
elapsed : 0:13:53, ETA : 1:00:37
epoch 2 / 10, batch 15000 / 17377, global_step = 32377, learning_rate = 1.000000e-02, loss = 0.553120, l2 = 0.010475, auc = 0.789517
[[34m2023-05-13 17:25:58[0m] elapsed : 0:14:11, ETA : 0:59:39
[[34m2023-05-13 17:25:58[0m] epoch 2 / 10, batch 16000 / 17377, global_step = 33377, learning_rate = 1.000000e-02, loss = 0.552847, l2 = 0.010441, auc = 0.789563
elapsed : 0:14:11, ETA : 0:59:39
epoch 2 / 10, batch 16000 / 17377, global_step = 33377, learning_rate = 1.000000e-02, loss = 0.552847, l2 = 0.010441, auc = 0.789563
[[34m2023-05-13 17:26:15[0m] elapsed : 0:14:29, ETA : 0:58:43
[[34m2023-05-13 17:26:15[0m] epoch 2 / 10, batch 17000 / 17377, global_step = 34377, learning_rate = 1.000000e-02, loss = 0.552966, l2 = 0.010382, auc = 0.789418
elapsed : 0:14:29, ETA : 0:58:43
epoch 2 / 10, batch 17000 / 17377, global_step = 34377, learning_rate = 1.000000e-02, loss = 0.552966, l2 = 0.010382, auc = 0.789418
[[34m2023-05-13 17:26:22[0m] running test...
on disk...
[[34m2023-05-13 17:26:26[0m] evaluated batches: 1000, 0:00:03
[[34m2023-05-13 17:26:29[0m] evaluated batches: 2000, 0:00:02
[[34m2023-05-13 17:26:31[0m] evaluated batches: 3000, 0:00:02
[[34m2023-05-13 17:26:34[0m] evaluated batches: 4000, 0:00:02
[[34m2023-05-13 17:26:38[0m] evaluated batches: 5000, 0:00:03
[[34m2023-05-13 17:26:40[0m] evaluated batches: 6000, 0:00:02
[[34m2023-05-13 17:26:43[0m] evaluated batches: 7000, 0:00:02
[[34m2023-05-13 17:26:46[0m] evaluated batches: 8000, 0:00:02
[[34m2023-05-13 17:26:49[0m] evaluated batches: 9000, 0:00:03
[[34m2023-05-13 17:26:52[0m] evaluated batches: 10000, 0:00:02
[[34m2023-05-13 17:26:55[0m] evaluated batches: 11000, 0:00:02
[[34m2023-05-13 17:26:58[0m] evaluated batches: 12000, 0:00:02
[[34m2023-05-13 17:27:01[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 17:27:04[0m] evaluated batches: 14000, 0:00:02
[[34m2023-05-13 17:27:07[0m] evaluated batches: 15000, 0:00:02
[[34m2023-05-13 17:27:10[0m] evaluated batches: 16000, 0:00:02
[[34m2023-05-13 17:27:13[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 17:27:16[0m] evaluated batches: 18000, 0:00:02
[[34m2023-05-13 17:27:19[0m] evaluated batches: 19000, 0:00:02
[[34m2023-05-13 17:27:22[0m] evaluated batches: 20000, 0:00:02
[[34m2023-05-13 17:27:25[0m] evaluated batches: 21000, 0:00:02
[[34m2023-05-13 17:27:27[0m] evaluated batches: 22000, 0:00:02
[[34m2023-05-13 17:27:30[0m] evaluated batches: 23000, 0:00:03
[[34m2023-05-13 17:27:33[0m] evaluated batches: 24000, 0:00:03
[[34m2023-05-13 17:27:37[0m] evaluated batches: 25000, 0:00:03
[[34m2023-05-13 17:27:46[0m] test loss = 0.553443, test auc = 0.788846
[[34m2023-05-13 17:27:46[0m] evaluated time: 0:01:23
[[34m2023-05-13 17:27:46[0m] analyse_structure
[[34m2023-05-13 17:28:11[0m] elapsed : 0:16:24, ETA : 1:03:18
[[34m2023-05-13 17:28:11[0m] epoch 3 / 10, batch 1000 / 17377, global_step = 35754, learning_rate = 1.000000e-02, loss = 0.761277, l2 = 0.014194, auc = 0.789509
elapsed : 0:16:24, ETA : 1:03:18
epoch 3 / 10, batch 1000 / 17377, global_step = 35754, learning_rate = 1.000000e-02, loss = 0.761277, l2 = 0.014194, auc = 0.789509
[[34m2023-05-13 17:28:36[0m] elapsed : 0:16:50, ETA : 1:02:45
[[34m2023-05-13 17:28:36[0m] epoch 3 / 10, batch 2000 / 17377, global_step = 36754, learning_rate = 1.000000e-02, loss = 0.552867, l2 = 0.010254, auc = 0.789606
elapsed : 0:16:50, ETA : 1:02:45
epoch 3 / 10, batch 2000 / 17377, global_step = 36754, learning_rate = 1.000000e-02, loss = 0.552867, l2 = 0.010254, auc = 0.789606
[[34m2023-05-13 17:29:01[0m] elapsed : 0:17:15, ETA : 1:02:08
[[34m2023-05-13 17:29:01[0m] epoch 3 / 10, batch 3000 / 17377, global_step = 37754, learning_rate = 1.000000e-02, loss = 0.551505, l2 = 0.010211, auc = 0.790919
elapsed : 0:17:15, ETA : 1:02:08
epoch 3 / 10, batch 3000 / 17377, global_step = 37754, learning_rate = 1.000000e-02, loss = 0.551505, l2 = 0.010211, auc = 0.790919
[[34m2023-05-13 17:29:26[0m] elapsed : 0:17:40, ETA : 1:01:32
[[34m2023-05-13 17:29:26[0m] epoch 3 / 10, batch 4000 / 17377, global_step = 38754, learning_rate = 1.000000e-02, loss = 0.552020, l2 = 0.010177, auc = 0.790304
elapsed : 0:17:40, ETA : 1:01:32
epoch 3 / 10, batch 4000 / 17377, global_step = 38754, learning_rate = 1.000000e-02, loss = 0.552020, l2 = 0.010177, auc = 0.790304
[[34m2023-05-13 17:29:51[0m] elapsed : 0:18:04, ETA : 1:00:54
[[34m2023-05-13 17:29:51[0m] epoch 3 / 10, batch 5000 / 17377, global_step = 39754, learning_rate = 1.000000e-02, loss = 0.552388, l2 = 0.010129, auc = 0.789950
elapsed : 0:18:04, ETA : 1:00:54
epoch 3 / 10, batch 5000 / 17377, global_step = 39754, learning_rate = 1.000000e-02, loss = 0.552388, l2 = 0.010129, auc = 0.789950
[[34m2023-05-13 17:30:16[0m] elapsed : 0:18:30, ETA : 1:00:22
[[34m2023-05-13 17:30:16[0m] epoch 3 / 10, batch 6000 / 17377, global_step = 40754, learning_rate = 1.000000e-02, loss = 0.551379, l2 = 0.010093, auc = 0.790894
elapsed : 0:18:30, ETA : 1:00:22
epoch 3 / 10, batch 6000 / 17377, global_step = 40754, learning_rate = 1.000000e-02, loss = 0.551379, l2 = 0.010093, auc = 0.790894
[[34m2023-05-13 17:30:41[0m] elapsed : 0:18:55, ETA : 0:59:48
[[34m2023-05-13 17:30:41[0m] epoch 3 / 10, batch 7000 / 17377, global_step = 41754, learning_rate = 1.000000e-02, loss = 0.550829, l2 = 0.010045, auc = 0.791459
elapsed : 0:18:55, ETA : 0:59:48
epoch 3 / 10, batch 7000 / 17377, global_step = 41754, learning_rate = 1.000000e-02, loss = 0.550829, l2 = 0.010045, auc = 0.791459
[[34m2023-05-13 17:31:06[0m] elapsed : 0:19:20, ETA : 0:59:14
[[34m2023-05-13 17:31:06[0m] epoch 3 / 10, batch 8000 / 17377, global_step = 42754, learning_rate = 1.000000e-02, loss = 0.551713, l2 = 0.009999, auc = 0.790581
elapsed : 0:19:20, ETA : 0:59:14
epoch 3 / 10, batch 8000 / 17377, global_step = 42754, learning_rate = 1.000000e-02, loss = 0.551713, l2 = 0.009999, auc = 0.790581
[[34m2023-05-13 17:31:32[0m] elapsed : 0:19:45, ETA : 0:58:41
[[34m2023-05-13 17:31:32[0m] epoch 3 / 10, batch 9000 / 17377, global_step = 43754, learning_rate = 1.000000e-02, loss = 0.552636, l2 = 0.009957, auc = 0.789846
elapsed : 0:19:45, ETA : 0:58:41
epoch 3 / 10, batch 9000 / 17377, global_step = 43754, learning_rate = 1.000000e-02, loss = 0.552636, l2 = 0.009957, auc = 0.789846
[[34m2023-05-13 17:31:57[0m] elapsed : 0:20:11, ETA : 0:58:11
[[34m2023-05-13 17:31:57[0m] epoch 3 / 10, batch 10000 / 17377, global_step = 44754, learning_rate = 1.000000e-02, loss = 0.550391, l2 = 0.009905, auc = 0.792026
elapsed : 0:20:11, ETA : 0:58:11
epoch 3 / 10, batch 10000 / 17377, global_step = 44754, learning_rate = 1.000000e-02, loss = 0.550391, l2 = 0.009905, auc = 0.792026
[[34m2023-05-13 17:32:22[0m] elapsed : 0:20:36, ETA : 0:57:38
[[34m2023-05-13 17:32:22[0m] epoch 3 / 10, batch 11000 / 17377, global_step = 45754, learning_rate = 1.000000e-02, loss = 0.551707, l2 = 0.009889, auc = 0.790654
elapsed : 0:20:36, ETA : 0:57:38
epoch 3 / 10, batch 11000 / 17377, global_step = 45754, learning_rate = 1.000000e-02, loss = 0.551707, l2 = 0.009889, auc = 0.790654
[[34m2023-05-13 17:32:42[0m] elapsed : 0:20:55, ETA : 0:56:49
[[34m2023-05-13 17:32:42[0m] epoch 3 / 10, batch 12000 / 17377, global_step = 46754, learning_rate = 1.000000e-02, loss = 0.552078, l2 = 0.009851, auc = 0.790384
elapsed : 0:20:55, ETA : 0:56:49
epoch 3 / 10, batch 12000 / 17377, global_step = 46754, learning_rate = 1.000000e-02, loss = 0.552078, l2 = 0.009851, auc = 0.790384
[[34m2023-05-13 17:33:00[0m] elapsed : 0:21:13, ETA : 0:55:59
[[34m2023-05-13 17:33:00[0m] epoch 3 / 10, batch 13000 / 17377, global_step = 47754, learning_rate = 1.000000e-02, loss = 0.549919, l2 = 0.009802, auc = 0.792367
elapsed : 0:21:13, ETA : 0:55:59
epoch 3 / 10, batch 13000 / 17377, global_step = 47754, learning_rate = 1.000000e-02, loss = 0.549919, l2 = 0.009802, auc = 0.792367
[[34m2023-05-13 17:33:18[0m] elapsed : 0:21:31, ETA : 0:55:10
[[34m2023-05-13 17:33:18[0m] epoch 3 / 10, batch 14000 / 17377, global_step = 48754, learning_rate = 1.000000e-02, loss = 0.550939, l2 = 0.009761, auc = 0.791484
elapsed : 0:21:31, ETA : 0:55:10
epoch 3 / 10, batch 14000 / 17377, global_step = 48754, learning_rate = 1.000000e-02, loss = 0.550939, l2 = 0.009761, auc = 0.791484
[[34m2023-05-13 17:33:36[0m] elapsed : 0:21:49, ETA : 0:54:22
[[34m2023-05-13 17:33:36[0m] epoch 3 / 10, batch 15000 / 17377, global_step = 49754, learning_rate = 1.000000e-02, loss = 0.550178, l2 = 0.009747, auc = 0.792350
elapsed : 0:21:49, ETA : 0:54:22
epoch 3 / 10, batch 15000 / 17377, global_step = 49754, learning_rate = 1.000000e-02, loss = 0.550178, l2 = 0.009747, auc = 0.792350
[[34m2023-05-13 17:33:54[0m] elapsed : 0:22:07, ETA : 0:53:36
[[34m2023-05-13 17:33:54[0m] epoch 3 / 10, batch 16000 / 17377, global_step = 50754, learning_rate = 1.000000e-02, loss = 0.549995, l2 = 0.009730, auc = 0.792052
elapsed : 0:22:07, ETA : 0:53:36
epoch 3 / 10, batch 16000 / 17377, global_step = 50754, learning_rate = 1.000000e-02, loss = 0.549995, l2 = 0.009730, auc = 0.792052
[[34m2023-05-13 17:34:12[0m] elapsed : 0:22:25, ETA : 0:52:50
[[34m2023-05-13 17:34:12[0m] epoch 3 / 10, batch 17000 / 17377, global_step = 51754, learning_rate = 1.000000e-02, loss = 0.549835, l2 = 0.009679, auc = 0.792401
elapsed : 0:22:25, ETA : 0:52:50
epoch 3 / 10, batch 17000 / 17377, global_step = 51754, learning_rate = 1.000000e-02, loss = 0.549835, l2 = 0.009679, auc = 0.792401
[[34m2023-05-13 17:34:19[0m] running test...
on disk...
[[34m2023-05-13 17:34:22[0m] evaluated batches: 1000, 0:00:03
[[34m2023-05-13 17:34:25[0m] evaluated batches: 2000, 0:00:02
[[34m2023-05-13 17:34:28[0m] evaluated batches: 3000, 0:00:02
[[34m2023-05-13 17:34:31[0m] evaluated batches: 4000, 0:00:02
[[34m2023-05-13 17:34:34[0m] evaluated batches: 5000, 0:00:03
[[34m2023-05-13 17:34:37[0m] evaluated batches: 6000, 0:00:02
[[34m2023-05-13 17:34:40[0m] evaluated batches: 7000, 0:00:02
[[34m2023-05-13 17:34:43[0m] evaluated batches: 8000, 0:00:02
[[34m2023-05-13 17:34:46[0m] evaluated batches: 9000, 0:00:03
[[34m2023-05-13 17:34:49[0m] evaluated batches: 10000, 0:00:02
[[34m2023-05-13 17:34:52[0m] evaluated batches: 11000, 0:00:02
[[34m2023-05-13 17:34:55[0m] evaluated batches: 12000, 0:00:02
[[34m2023-05-13 17:34:58[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 17:35:01[0m] evaluated batches: 14000, 0:00:02
[[34m2023-05-13 17:35:03[0m] evaluated batches: 15000, 0:00:02
[[34m2023-05-13 17:35:05[0m] evaluated batches: 16000, 0:00:02
[[34m2023-05-13 17:35:09[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 17:35:12[0m] evaluated batches: 18000, 0:00:03
[[34m2023-05-13 17:35:15[0m] evaluated batches: 19000, 0:00:03
[[34m2023-05-13 17:35:19[0m] evaluated batches: 20000, 0:00:03
[[34m2023-05-13 17:35:23[0m] evaluated batches: 21000, 0:00:03
[[34m2023-05-13 17:35:26[0m] evaluated batches: 22000, 0:00:03
[[34m2023-05-13 17:35:29[0m] evaluated batches: 23000, 0:00:03
[[34m2023-05-13 17:35:32[0m] evaluated batches: 24000, 0:00:03
[[34m2023-05-13 17:35:36[0m] evaluated batches: 25000, 0:00:03
[[34m2023-05-13 17:35:45[0m] test loss = 0.551303, test auc = 0.790963
[[34m2023-05-13 17:35:45[0m] evaluated time: 0:01:25
[[34m2023-05-13 17:35:45[0m] analyse_structure
[[34m2023-05-13 17:36:10[0m] elapsed : 0:24:23, ETA : 0:55:21
[[34m2023-05-13 17:36:10[0m] epoch 4 / 10, batch 1000 / 17377, global_step = 53131, learning_rate = 1.000000e-02, loss = 0.757310, l2 = 0.013269, auc = 0.792193
elapsed : 0:24:23, ETA : 0:55:21
epoch 4 / 10, batch 1000 / 17377, global_step = 53131, learning_rate = 1.000000e-02, loss = 0.757310, l2 = 0.013269, auc = 0.792193
[[34m2023-05-13 17:36:35[0m] elapsed : 0:24:48, ETA : 0:54:48
[[34m2023-05-13 17:36:35[0m] epoch 4 / 10, batch 2000 / 17377, global_step = 54131, learning_rate = 1.000000e-02, loss = 0.549510, l2 = 0.009587, auc = 0.792668
elapsed : 0:24:48, ETA : 0:54:48
epoch 4 / 10, batch 2000 / 17377, global_step = 54131, learning_rate = 1.000000e-02, loss = 0.549510, l2 = 0.009587, auc = 0.792668
[[34m2023-05-13 17:37:00[0m] elapsed : 0:25:13, ETA : 0:54:15
[[34m2023-05-13 17:37:00[0m] epoch 4 / 10, batch 3000 / 17377, global_step = 55131, learning_rate = 1.000000e-02, loss = 0.549074, l2 = 0.009551, auc = 0.792914
elapsed : 0:25:13, ETA : 0:54:15
epoch 4 / 10, batch 3000 / 17377, global_step = 55131, learning_rate = 1.000000e-02, loss = 0.549074, l2 = 0.009551, auc = 0.792914
[[34m2023-05-13 17:37:25[0m] elapsed : 0:25:39, ETA : 0:53:45
[[34m2023-05-13 17:37:25[0m] epoch 4 / 10, batch 4000 / 17377, global_step = 56131, learning_rate = 1.000000e-02, loss = 0.549503, l2 = 0.009506, auc = 0.792683
elapsed : 0:25:39, ETA : 0:53:45
epoch 4 / 10, batch 4000 / 17377, global_step = 56131, learning_rate = 1.000000e-02, loss = 0.549503, l2 = 0.009506, auc = 0.792683
[[34m2023-05-13 17:37:51[0m] elapsed : 0:26:04, ETA : 0:53:13
[[34m2023-05-13 17:37:51[0m] epoch 4 / 10, batch 5000 / 17377, global_step = 57131, learning_rate = 1.000000e-02, loss = 0.549671, l2 = 0.009504, auc = 0.792547
elapsed : 0:26:04, ETA : 0:53:13
epoch 4 / 10, batch 5000 / 17377, global_step = 57131, learning_rate = 1.000000e-02, loss = 0.549671, l2 = 0.009504, auc = 0.792547
[[34m2023-05-13 17:38:16[0m] elapsed : 0:26:29, ETA : 0:52:40
[[34m2023-05-13 17:38:16[0m] epoch 4 / 10, batch 6000 / 17377, global_step = 58131, learning_rate = 1.000000e-02, loss = 0.549386, l2 = 0.009480, auc = 0.792849
elapsed : 0:26:29, ETA : 0:52:40
epoch 4 / 10, batch 6000 / 17377, global_step = 58131, learning_rate = 1.000000e-02, loss = 0.549386, l2 = 0.009480, auc = 0.792849
[[34m2023-05-13 17:38:40[0m] elapsed : 0:26:53, ETA : 0:52:07
[[34m2023-05-13 17:38:40[0m] epoch 4 / 10, batch 7000 / 17377, global_step = 59131, learning_rate = 1.000000e-02, loss = 0.550422, l2 = 0.009450, auc = 0.791770
elapsed : 0:26:53, ETA : 0:52:07
epoch 4 / 10, batch 7000 / 17377, global_step = 59131, learning_rate = 1.000000e-02, loss = 0.550422, l2 = 0.009450, auc = 0.791770
[[34m2023-05-13 17:39:06[0m] elapsed : 0:27:19, ETA : 0:51:37
[[34m2023-05-13 17:39:06[0m] epoch 4 / 10, batch 8000 / 17377, global_step = 60131, learning_rate = 1.000000e-02, loss = 0.549042, l2 = 0.009431, auc = 0.793079
elapsed : 0:27:19, ETA : 0:51:37
epoch 4 / 10, batch 8000 / 17377, global_step = 60131, learning_rate = 1.000000e-02, loss = 0.549042, l2 = 0.009431, auc = 0.793079
[[34m2023-05-13 17:39:31[0m] elapsed : 0:27:44, ETA : 0:51:06
[[34m2023-05-13 17:39:31[0m] epoch 4 / 10, batch 9000 / 17377, global_step = 61131, learning_rate = 1.000000e-02, loss = 0.550286, l2 = 0.009386, auc = 0.791856
elapsed : 0:27:44, ETA : 0:51:06
epoch 4 / 10, batch 9000 / 17377, global_step = 61131, learning_rate = 1.000000e-02, loss = 0.550286, l2 = 0.009386, auc = 0.791856
[[34m2023-05-13 17:39:56[0m] elapsed : 0:28:09, ETA : 0:50:34
[[34m2023-05-13 17:39:56[0m] epoch 4 / 10, batch 10000 / 17377, global_step = 62131, learning_rate = 1.000000e-02, loss = 0.549365, l2 = 0.009350, auc = 0.792750
elapsed : 0:28:09, ETA : 0:50:34
epoch 4 / 10, batch 10000 / 17377, global_step = 62131, learning_rate = 1.000000e-02, loss = 0.549365, l2 = 0.009350, auc = 0.792750
[[34m2023-05-13 17:40:15[0m] elapsed : 0:28:28, ETA : 0:49:53
[[34m2023-05-13 17:40:15[0m] epoch 4 / 10, batch 11000 / 17377, global_step = 63131, learning_rate = 1.000000e-02, loss = 0.549631, l2 = 0.009347, auc = 0.792591
elapsed : 0:28:28, ETA : 0:49:53
epoch 4 / 10, batch 11000 / 17377, global_step = 63131, learning_rate = 1.000000e-02, loss = 0.549631, l2 = 0.009347, auc = 0.792591
[[34m2023-05-13 17:40:33[0m] elapsed : 0:28:46, ETA : 0:49:10
[[34m2023-05-13 17:40:33[0m] epoch 4 / 10, batch 12000 / 17377, global_step = 64131, learning_rate = 1.000000e-02, loss = 0.549451, l2 = 0.009330, auc = 0.792665
elapsed : 0:28:46, ETA : 0:49:10
epoch 4 / 10, batch 12000 / 17377, global_step = 64131, learning_rate = 1.000000e-02, loss = 0.549451, l2 = 0.009330, auc = 0.792665
[[34m2023-05-13 17:40:51[0m] elapsed : 0:29:04, ETA : 0:48:29
[[34m2023-05-13 17:40:51[0m] epoch 4 / 10, batch 13000 / 17377, global_step = 65131, learning_rate = 1.000000e-02, loss = 0.548785, l2 = 0.009262, auc = 0.793504
elapsed : 0:29:04, ETA : 0:48:29
epoch 4 / 10, batch 13000 / 17377, global_step = 65131, learning_rate = 1.000000e-02, loss = 0.548785, l2 = 0.009262, auc = 0.793504
[[34m2023-05-13 17:41:09[0m] elapsed : 0:29:22, ETA : 0:47:47
[[34m2023-05-13 17:41:09[0m] epoch 4 / 10, batch 14000 / 17377, global_step = 66131, learning_rate = 1.000000e-02, loss = 0.548875, l2 = 0.009249, auc = 0.793338
elapsed : 0:29:22, ETA : 0:47:47
epoch 4 / 10, batch 14000 / 17377, global_step = 66131, learning_rate = 1.000000e-02, loss = 0.548875, l2 = 0.009249, auc = 0.793338
[[34m2023-05-13 17:41:26[0m] elapsed : 0:29:40, ETA : 0:47:07
[[34m2023-05-13 17:41:26[0m] epoch 4 / 10, batch 15000 / 17377, global_step = 67131, learning_rate = 1.000000e-02, loss = 0.548204, l2 = 0.009237, auc = 0.793915
elapsed : 0:29:40, ETA : 0:47:07
epoch 4 / 10, batch 15000 / 17377, global_step = 67131, learning_rate = 1.000000e-02, loss = 0.548204, l2 = 0.009237, auc = 0.793915
[[34m2023-05-13 17:41:45[0m] elapsed : 0:29:58, ETA : 0:46:27
[[34m2023-05-13 17:41:45[0m] epoch 4 / 10, batch 16000 / 17377, global_step = 68131, learning_rate = 1.000000e-02, loss = 0.548094, l2 = 0.009206, auc = 0.794068
elapsed : 0:29:58, ETA : 0:46:27
epoch 4 / 10, batch 16000 / 17377, global_step = 68131, learning_rate = 1.000000e-02, loss = 0.548094, l2 = 0.009206, auc = 0.794068
[[34m2023-05-13 17:42:03[0m] elapsed : 0:30:16, ETA : 0:45:48
[[34m2023-05-13 17:42:03[0m] epoch 4 / 10, batch 17000 / 17377, global_step = 69131, learning_rate = 1.000000e-02, loss = 0.548552, l2 = 0.009173, auc = 0.793459
elapsed : 0:30:16, ETA : 0:45:48
epoch 4 / 10, batch 17000 / 17377, global_step = 69131, learning_rate = 1.000000e-02, loss = 0.548552, l2 = 0.009173, auc = 0.793459
[[34m2023-05-13 17:42:09[0m] running test...
on disk...
[[34m2023-05-13 17:42:13[0m] evaluated batches: 1000, 0:00:03
[[34m2023-05-13 17:42:15[0m] evaluated batches: 2000, 0:00:02
[[34m2023-05-13 17:42:18[0m] evaluated batches: 3000, 0:00:02
[[34m2023-05-13 17:42:21[0m] evaluated batches: 4000, 0:00:02
[[34m2023-05-13 17:42:25[0m] evaluated batches: 5000, 0:00:03
[[34m2023-05-13 17:42:27[0m] evaluated batches: 6000, 0:00:02
[[34m2023-05-13 17:42:30[0m] evaluated batches: 7000, 0:00:02
[[34m2023-05-13 17:42:33[0m] evaluated batches: 8000, 0:00:02
[[34m2023-05-13 17:42:36[0m] evaluated batches: 9000, 0:00:02
[[34m2023-05-13 17:42:38[0m] evaluated batches: 10000, 0:00:02
[[34m2023-05-13 17:42:41[0m] evaluated batches: 11000, 0:00:03
[[34m2023-05-13 17:42:44[0m] evaluated batches: 12000, 0:00:03
[[34m2023-05-13 17:42:48[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 17:42:51[0m] evaluated batches: 14000, 0:00:03
[[34m2023-05-13 17:42:55[0m] evaluated batches: 15000, 0:00:03
[[34m2023-05-13 17:42:58[0m] evaluated batches: 16000, 0:00:03
[[34m2023-05-13 17:43:02[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 17:43:05[0m] evaluated batches: 18000, 0:00:03
[[34m2023-05-13 17:43:08[0m] evaluated batches: 19000, 0:00:03
[[34m2023-05-13 17:43:11[0m] evaluated batches: 20000, 0:00:03
[[34m2023-05-13 17:43:15[0m] evaluated batches: 21000, 0:00:03
[[34m2023-05-13 17:43:18[0m] evaluated batches: 22000, 0:00:03
[[34m2023-05-13 17:43:22[0m] evaluated batches: 23000, 0:00:03
[[34m2023-05-13 17:43:25[0m] evaluated batches: 24000, 0:00:03
[[34m2023-05-13 17:43:29[0m] evaluated batches: 25000, 0:00:03
[[34m2023-05-13 17:43:37[0m] test loss = 0.549758, test auc = 0.792604
[[34m2023-05-13 17:43:37[0m] evaluated time: 0:01:28
[[34m2023-05-13 17:43:37[0m] analyse_structure
[[34m2023-05-13 17:44:02[0m] elapsed : 0:32:15, ETA : 0:47:13
[[34m2023-05-13 17:44:02[0m] epoch 5 / 10, batch 1000 / 17377, global_step = 70508, learning_rate = 1.000000e-02, loss = 0.754909, l2 = 0.012603, auc = 0.793800
elapsed : 0:32:15, ETA : 0:47:13
epoch 5 / 10, batch 1000 / 17377, global_step = 70508, learning_rate = 1.000000e-02, loss = 0.754909, l2 = 0.012603, auc = 0.793800
[[34m2023-05-13 17:44:27[0m] elapsed : 0:32:40, ETA : 0:46:42
[[34m2023-05-13 17:44:27[0m] epoch 5 / 10, batch 2000 / 17377, global_step = 71508, learning_rate = 1.000000e-02, loss = 0.548524, l2 = 0.009113, auc = 0.793620
elapsed : 0:32:40, ETA : 0:46:42
epoch 5 / 10, batch 2000 / 17377, global_step = 71508, learning_rate = 1.000000e-02, loss = 0.548524, l2 = 0.009113, auc = 0.793620
[[34m2023-05-13 17:44:53[0m] elapsed : 0:33:06, ETA : 0:46:13
[[34m2023-05-13 17:44:53[0m] epoch 5 / 10, batch 3000 / 17377, global_step = 72508, learning_rate = 1.000000e-02, loss = 0.547539, l2 = 0.009086, auc = 0.794394
elapsed : 0:33:06, ETA : 0:46:13
epoch 5 / 10, batch 3000 / 17377, global_step = 72508, learning_rate = 1.000000e-02, loss = 0.547539, l2 = 0.009086, auc = 0.794394
[[34m2023-05-13 17:45:18[0m] elapsed : 0:33:31, ETA : 0:45:42
[[34m2023-05-13 17:45:18[0m] epoch 5 / 10, batch 4000 / 17377, global_step = 73508, learning_rate = 1.000000e-02, loss = 0.547585, l2 = 0.009069, auc = 0.794430
elapsed : 0:33:31, ETA : 0:45:42
epoch 5 / 10, batch 4000 / 17377, global_step = 73508, learning_rate = 1.000000e-02, loss = 0.547585, l2 = 0.009069, auc = 0.794430
[[34m2023-05-13 17:45:43[0m] elapsed : 0:33:56, ETA : 0:45:12
[[34m2023-05-13 17:45:43[0m] epoch 5 / 10, batch 5000 / 17377, global_step = 74508, learning_rate = 1.000000e-02, loss = 0.548201, l2 = 0.009053, auc = 0.793963
elapsed : 0:33:56, ETA : 0:45:12
epoch 5 / 10, batch 5000 / 17377, global_step = 74508, learning_rate = 1.000000e-02, loss = 0.548201, l2 = 0.009053, auc = 0.793963
[[34m2023-05-13 17:46:08[0m] elapsed : 0:34:21, ETA : 0:44:42
[[34m2023-05-13 17:46:08[0m] epoch 5 / 10, batch 6000 / 17377, global_step = 75508, learning_rate = 1.000000e-02, loss = 0.548023, l2 = 0.009033, auc = 0.793990
elapsed : 0:34:21, ETA : 0:44:42
epoch 5 / 10, batch 6000 / 17377, global_step = 75508, learning_rate = 1.000000e-02, loss = 0.548023, l2 = 0.009033, auc = 0.793990
[[34m2023-05-13 17:46:33[0m] elapsed : 0:34:47, ETA : 0:44:13
[[34m2023-05-13 17:46:33[0m] epoch 5 / 10, batch 7000 / 17377, global_step = 76508, learning_rate = 1.000000e-02, loss = 0.546814, l2 = 0.009002, auc = 0.795170
elapsed : 0:34:47, ETA : 0:44:13
epoch 5 / 10, batch 7000 / 17377, global_step = 76508, learning_rate = 1.000000e-02, loss = 0.546814, l2 = 0.009002, auc = 0.795170
[[34m2023-05-13 17:46:59[0m] elapsed : 0:35:12, ETA : 0:43:43
[[34m2023-05-13 17:46:59[0m] epoch 5 / 10, batch 8000 / 17377, global_step = 77508, learning_rate = 1.000000e-02, loss = 0.546989, l2 = 0.008981, auc = 0.795100
elapsed : 0:35:12, ETA : 0:43:43
epoch 5 / 10, batch 8000 / 17377, global_step = 77508, learning_rate = 1.000000e-02, loss = 0.546989, l2 = 0.008981, auc = 0.795100
[[34m2023-05-13 17:47:23[0m] elapsed : 0:35:36, ETA : 0:43:11
[[34m2023-05-13 17:47:23[0m] epoch 5 / 10, batch 9000 / 17377, global_step = 78508, learning_rate = 1.000000e-02, loss = 0.548389, l2 = 0.008968, auc = 0.793780
elapsed : 0:35:36, ETA : 0:43:11
epoch 5 / 10, batch 9000 / 17377, global_step = 78508, learning_rate = 1.000000e-02, loss = 0.548389, l2 = 0.008968, auc = 0.793780
[[34m2023-05-13 17:47:42[0m] elapsed : 0:35:55, ETA : 0:42:34
[[34m2023-05-13 17:47:42[0m] epoch 5 / 10, batch 10000 / 17377, global_step = 79508, learning_rate = 1.000000e-02, loss = 0.546557, l2 = 0.008948, auc = 0.795525
elapsed : 0:35:55, ETA : 0:42:34
epoch 5 / 10, batch 10000 / 17377, global_step = 79508, learning_rate = 1.000000e-02, loss = 0.546557, l2 = 0.008948, auc = 0.795525
[[34m2023-05-13 17:48:00[0m] elapsed : 0:36:14, ETA : 0:41:58
[[34m2023-05-13 17:48:00[0m] epoch 5 / 10, batch 11000 / 17377, global_step = 80508, learning_rate = 1.000000e-02, loss = 0.547712, l2 = 0.008924, auc = 0.794284
elapsed : 0:36:14, ETA : 0:41:58
epoch 5 / 10, batch 11000 / 17377, global_step = 80508, learning_rate = 1.000000e-02, loss = 0.547712, l2 = 0.008924, auc = 0.794284
[[34m2023-05-13 17:48:18[0m] elapsed : 0:36:31, ETA : 0:41:20
[[34m2023-05-13 17:48:18[0m] epoch 5 / 10, batch 12000 / 17377, global_step = 81508, learning_rate = 1.000000e-02, loss = 0.547114, l2 = 0.008916, auc = 0.794789
elapsed : 0:36:31, ETA : 0:41:20
epoch 5 / 10, batch 12000 / 17377, global_step = 81508, learning_rate = 1.000000e-02, loss = 0.547114, l2 = 0.008916, auc = 0.794789
[[34m2023-05-13 17:48:36[0m] elapsed : 0:36:49, ETA : 0:40:43
[[34m2023-05-13 17:48:36[0m] epoch 5 / 10, batch 13000 / 17377, global_step = 82508, learning_rate = 1.000000e-02, loss = 0.548026, l2 = 0.008880, auc = 0.794106
elapsed : 0:36:49, ETA : 0:40:43
epoch 5 / 10, batch 13000 / 17377, global_step = 82508, learning_rate = 1.000000e-02, loss = 0.548026, l2 = 0.008880, auc = 0.794106
[[34m2023-05-13 17:48:54[0m] elapsed : 0:37:07, ETA : 0:40:07
[[34m2023-05-13 17:48:54[0m] epoch 5 / 10, batch 14000 / 17377, global_step = 83508, learning_rate = 1.000000e-02, loss = 0.548004, l2 = 0.008855, auc = 0.794001
elapsed : 0:37:07, ETA : 0:40:07
epoch 5 / 10, batch 14000 / 17377, global_step = 83508, learning_rate = 1.000000e-02, loss = 0.548004, l2 = 0.008855, auc = 0.794001
[[34m2023-05-13 17:49:12[0m] elapsed : 0:37:25, ETA : 0:39:31
[[34m2023-05-13 17:49:12[0m] epoch 5 / 10, batch 15000 / 17377, global_step = 84508, learning_rate = 1.000000e-02, loss = 0.546788, l2 = 0.008854, auc = 0.795151
elapsed : 0:37:25, ETA : 0:39:31
epoch 5 / 10, batch 15000 / 17377, global_step = 84508, learning_rate = 1.000000e-02, loss = 0.546788, l2 = 0.008854, auc = 0.795151
[[34m2023-05-13 17:49:30[0m] elapsed : 0:37:43, ETA : 0:38:55
[[34m2023-05-13 17:49:30[0m] epoch 5 / 10, batch 16000 / 17377, global_step = 85508, learning_rate = 1.000000e-02, loss = 0.547873, l2 = 0.008842, auc = 0.794210
elapsed : 0:37:43, ETA : 0:38:55
epoch 5 / 10, batch 16000 / 17377, global_step = 85508, learning_rate = 1.000000e-02, loss = 0.547873, l2 = 0.008842, auc = 0.794210
[[34m2023-05-13 17:49:47[0m] elapsed : 0:38:01, ETA : 0:38:20
[[34m2023-05-13 17:49:47[0m] epoch 5 / 10, batch 17000 / 17377, global_step = 86508, learning_rate = 1.000000e-02, loss = 0.547226, l2 = 0.008827, auc = 0.794837
elapsed : 0:38:01, ETA : 0:38:20
epoch 5 / 10, batch 17000 / 17377, global_step = 86508, learning_rate = 1.000000e-02, loss = 0.547226, l2 = 0.008827, auc = 0.794837
[[34m2023-05-13 17:49:54[0m] running test...
on disk...
[[34m2023-05-13 17:49:57[0m] evaluated batches: 1000, 0:00:03
[[34m2023-05-13 17:50:00[0m] evaluated batches: 2000, 0:00:02
[[34m2023-05-13 17:50:03[0m] evaluated batches: 3000, 0:00:02
[[34m2023-05-13 17:50:05[0m] evaluated batches: 4000, 0:00:02
[[34m2023-05-13 17:50:08[0m] evaluated batches: 5000, 0:00:02
[[34m2023-05-13 17:50:11[0m] evaluated batches: 6000, 0:00:03
[[34m2023-05-13 17:50:14[0m] evaluated batches: 7000, 0:00:03
[[34m2023-05-13 17:50:18[0m] evaluated batches: 8000, 0:00:03
[[34m2023-05-13 17:50:21[0m] evaluated batches: 9000, 0:00:03
[[34m2023-05-13 17:50:25[0m] evaluated batches: 10000, 0:00:03
[[34m2023-05-13 17:50:28[0m] evaluated batches: 11000, 0:00:03
[[34m2023-05-13 17:50:31[0m] evaluated batches: 12000, 0:00:03
[[34m2023-05-13 17:50:35[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 17:50:38[0m] evaluated batches: 14000, 0:00:03
[[34m2023-05-13 17:50:42[0m] evaluated batches: 15000, 0:00:03
[[34m2023-05-13 17:50:45[0m] evaluated batches: 16000, 0:00:03
[[34m2023-05-13 17:50:49[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 17:50:52[0m] evaluated batches: 18000, 0:00:03
[[34m2023-05-13 17:50:55[0m] evaluated batches: 19000, 0:00:03
[[34m2023-05-13 17:50:59[0m] evaluated batches: 20000, 0:00:03
[[34m2023-05-13 17:51:02[0m] evaluated batches: 21000, 0:00:03
[[34m2023-05-13 17:51:06[0m] evaluated batches: 22000, 0:00:03
[[34m2023-05-13 17:51:09[0m] evaluated batches: 23000, 0:00:03
[[34m2023-05-13 17:51:12[0m] evaluated batches: 24000, 0:00:03
[[34m2023-05-13 17:51:16[0m] evaluated batches: 25000, 0:00:03
[[34m2023-05-13 17:51:25[0m] test loss = 0.548209, test auc = 0.793725
[[34m2023-05-13 17:51:25[0m] evaluated time: 0:01:30
[[34m2023-05-13 17:51:25[0m] analyse_structure
[[34m2023-05-13 17:51:50[0m] elapsed : 0:40:03, ETA : 0:39:08
[[34m2023-05-13 17:51:50[0m] epoch 6 / 10, batch 1000 / 17377, global_step = 87885, learning_rate = 1.000000e-02, loss = 0.752681, l2 = 0.012119, auc = 0.795397
elapsed : 0:40:03, ETA : 0:39:08
epoch 6 / 10, batch 1000 / 17377, global_step = 87885, learning_rate = 1.000000e-02, loss = 0.752681, l2 = 0.012119, auc = 0.795397
[[34m2023-05-13 17:52:15[0m] elapsed : 0:40:28, ETA : 0:38:38
[[34m2023-05-13 17:52:15[0m] epoch 6 / 10, batch 2000 / 17377, global_step = 88885, learning_rate = 1.000000e-02, loss = 0.548205, l2 = 0.008755, auc = 0.793705
elapsed : 0:40:28, ETA : 0:38:38
epoch 6 / 10, batch 2000 / 17377, global_step = 88885, learning_rate = 1.000000e-02, loss = 0.548205, l2 = 0.008755, auc = 0.793705
[[34m2023-05-13 17:52:40[0m] elapsed : 0:40:53, ETA : 0:38:09
[[34m2023-05-13 17:52:40[0m] epoch 6 / 10, batch 3000 / 17377, global_step = 89885, learning_rate = 1.000000e-02, loss = 0.547327, l2 = 0.008723, auc = 0.794687
elapsed : 0:40:53, ETA : 0:38:09
epoch 6 / 10, batch 3000 / 17377, global_step = 89885, learning_rate = 1.000000e-02, loss = 0.547327, l2 = 0.008723, auc = 0.794687
[[34m2023-05-13 17:53:05[0m] elapsed : 0:41:18, ETA : 0:37:39
[[34m2023-05-13 17:53:05[0m] epoch 6 / 10, batch 4000 / 17377, global_step = 90885, learning_rate = 1.000000e-02, loss = 0.547201, l2 = 0.008730, auc = 0.794539
elapsed : 0:41:18, ETA : 0:37:39
epoch 6 / 10, batch 4000 / 17377, global_step = 90885, learning_rate = 1.000000e-02, loss = 0.547201, l2 = 0.008730, auc = 0.794539
[[34m2023-05-13 17:53:30[0m] elapsed : 0:41:43, ETA : 0:37:10
[[34m2023-05-13 17:53:30[0m] epoch 6 / 10, batch 5000 / 17377, global_step = 91885, learning_rate = 1.000000e-02, loss = 0.545742, l2 = 0.008699, auc = 0.796041
elapsed : 0:41:43, ETA : 0:37:10
epoch 6 / 10, batch 5000 / 17377, global_step = 91885, learning_rate = 1.000000e-02, loss = 0.545742, l2 = 0.008699, auc = 0.796041
[[34m2023-05-13 17:53:55[0m] elapsed : 0:42:08, ETA : 0:36:41
[[34m2023-05-13 17:53:55[0m] epoch 6 / 10, batch 6000 / 17377, global_step = 92885, learning_rate = 1.000000e-02, loss = 0.547028, l2 = 0.008684, auc = 0.794929
elapsed : 0:42:08, ETA : 0:36:41
epoch 6 / 10, batch 6000 / 17377, global_step = 92885, learning_rate = 1.000000e-02, loss = 0.547028, l2 = 0.008684, auc = 0.794929
[[34m2023-05-13 17:54:20[0m] elapsed : 0:42:34, ETA : 0:36:13
[[34m2023-05-13 17:54:20[0m] epoch 6 / 10, batch 7000 / 17377, global_step = 93885, learning_rate = 1.000000e-02, loss = 0.547423, l2 = 0.008650, auc = 0.794529
elapsed : 0:42:34, ETA : 0:36:13
epoch 6 / 10, batch 7000 / 17377, global_step = 93885, learning_rate = 1.000000e-02, loss = 0.547423, l2 = 0.008650, auc = 0.794529
[[34m2023-05-13 17:54:45[0m] elapsed : 0:42:59, ETA : 0:35:44
[[34m2023-05-13 17:54:45[0m] epoch 6 / 10, batch 8000 / 17377, global_step = 94885, learning_rate = 1.000000e-02, loss = 0.546271, l2 = 0.008635, auc = 0.795608
elapsed : 0:42:59, ETA : 0:35:44
epoch 6 / 10, batch 8000 / 17377, global_step = 94885, learning_rate = 1.000000e-02, loss = 0.546271, l2 = 0.008635, auc = 0.795608
[[34m2023-05-13 17:55:05[0m] elapsed : 0:43:18, ETA : 0:35:10
[[34m2023-05-13 17:55:05[0m] epoch 6 / 10, batch 9000 / 17377, global_step = 95885, learning_rate = 1.000000e-02, loss = 0.546956, l2 = 0.008629, auc = 0.795013
elapsed : 0:43:18, ETA : 0:35:10
epoch 6 / 10, batch 9000 / 17377, global_step = 95885, learning_rate = 1.000000e-02, loss = 0.546956, l2 = 0.008629, auc = 0.795013
[[34m2023-05-13 17:55:23[0m] elapsed : 0:43:37, ETA : 0:34:36
[[34m2023-05-13 17:55:23[0m] epoch 6 / 10, batch 10000 / 17377, global_step = 96885, learning_rate = 1.000000e-02, loss = 0.546980, l2 = 0.008623, auc = 0.794864
elapsed : 0:43:37, ETA : 0:34:36
epoch 6 / 10, batch 10000 / 17377, global_step = 96885, learning_rate = 1.000000e-02, loss = 0.546980, l2 = 0.008623, auc = 0.794864
[[34m2023-05-13 17:55:41[0m] elapsed : 0:43:54, ETA : 0:34:01
[[34m2023-05-13 17:55:41[0m] epoch 6 / 10, batch 11000 / 17377, global_step = 97885, learning_rate = 1.000000e-02, loss = 0.546079, l2 = 0.008578, auc = 0.795853
elapsed : 0:43:54, ETA : 0:34:01
epoch 6 / 10, batch 11000 / 17377, global_step = 97885, learning_rate = 1.000000e-02, loss = 0.546079, l2 = 0.008578, auc = 0.795853
[[34m2023-05-13 17:55:59[0m] elapsed : 0:44:12, ETA : 0:33:28
[[34m2023-05-13 17:55:59[0m] epoch 6 / 10, batch 12000 / 17377, global_step = 98885, learning_rate = 1.000000e-02, loss = 0.547194, l2 = 0.008557, auc = 0.794743
elapsed : 0:44:12, ETA : 0:33:28
epoch 6 / 10, batch 12000 / 17377, global_step = 98885, learning_rate = 1.000000e-02, loss = 0.547194, l2 = 0.008557, auc = 0.794743
[[34m2023-05-13 17:56:17[0m] elapsed : 0:44:30, ETA : 0:32:55
[[34m2023-05-13 17:56:17[0m] epoch 6 / 10, batch 13000 / 17377, global_step = 99885, learning_rate = 1.000000e-02, loss = 0.545545, l2 = 0.008558, auc = 0.796260
elapsed : 0:44:30, ETA : 0:32:55
epoch 6 / 10, batch 13000 / 17377, global_step = 99885, learning_rate = 1.000000e-02, loss = 0.545545, l2 = 0.008558, auc = 0.796260
[[34m2023-05-13 17:56:35[0m] elapsed : 0:44:49, ETA : 0:32:22
[[34m2023-05-13 17:56:35[0m] epoch 6 / 10, batch 14000 / 17377, global_step = 100885, learning_rate = 1.000000e-02, loss = 0.545649, l2 = 0.008542, auc = 0.796242
elapsed : 0:44:49, ETA : 0:32:22
epoch 6 / 10, batch 14000 / 17377, global_step = 100885, learning_rate = 1.000000e-02, loss = 0.545649, l2 = 0.008542, auc = 0.796242
[[34m2023-05-13 17:56:53[0m] elapsed : 0:45:06, ETA : 0:31:49
[[34m2023-05-13 17:56:53[0m] epoch 6 / 10, batch 15000 / 17377, global_step = 101885, learning_rate = 1.000000e-02, loss = 0.546709, l2 = 0.008547, auc = 0.795201
elapsed : 0:45:06, ETA : 0:31:49
epoch 6 / 10, batch 15000 / 17377, global_step = 101885, learning_rate = 1.000000e-02, loss = 0.546709, l2 = 0.008547, auc = 0.795201
[[34m2023-05-13 17:57:11[0m] elapsed : 0:45:24, ETA : 0:31:16
[[34m2023-05-13 17:57:11[0m] epoch 6 / 10, batch 16000 / 17377, global_step = 102885, learning_rate = 1.000000e-02, loss = 0.545769, l2 = 0.008519, auc = 0.796061
elapsed : 0:45:24, ETA : 0:31:16
epoch 6 / 10, batch 16000 / 17377, global_step = 102885, learning_rate = 1.000000e-02, loss = 0.545769, l2 = 0.008519, auc = 0.796061
[[34m2023-05-13 17:57:29[0m] elapsed : 0:45:42, ETA : 0:30:44
[[34m2023-05-13 17:57:29[0m] epoch 6 / 10, batch 17000 / 17377, global_step = 103885, learning_rate = 1.000000e-02, loss = 0.546316, l2 = 0.008503, auc = 0.795541
elapsed : 0:45:42, ETA : 0:30:44
epoch 6 / 10, batch 17000 / 17377, global_step = 103885, learning_rate = 1.000000e-02, loss = 0.546316, l2 = 0.008503, auc = 0.795541
[[34m2023-05-13 17:57:33[0m] running test...
on disk...
[[34m2023-05-13 17:57:37[0m] evaluated batches: 1000, 0:00:03
[[34m2023-05-13 17:57:40[0m] evaluated batches: 2000, 0:00:03
[[34m2023-05-13 17:57:43[0m] evaluated batches: 3000, 0:00:03
[[34m2023-05-13 17:57:46[0m] evaluated batches: 4000, 0:00:03
[[34m2023-05-13 17:57:50[0m] evaluated batches: 5000, 0:00:03
[[34m2023-05-13 17:57:53[0m] evaluated batches: 6000, 0:00:03
[[34m2023-05-13 17:57:57[0m] evaluated batches: 7000, 0:00:03
[[34m2023-05-13 17:58:00[0m] evaluated batches: 8000, 0:00:03
[[34m2023-05-13 17:58:04[0m] evaluated batches: 9000, 0:00:03
[[34m2023-05-13 17:58:07[0m] evaluated batches: 10000, 0:00:03
[[34m2023-05-13 17:58:11[0m] evaluated batches: 11000, 0:00:03
[[34m2023-05-13 17:58:14[0m] evaluated batches: 12000, 0:00:03
[[34m2023-05-13 17:58:17[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 17:58:21[0m] evaluated batches: 14000, 0:00:03
[[34m2023-05-13 17:58:24[0m] evaluated batches: 15000, 0:00:03
[[34m2023-05-13 17:58:27[0m] evaluated batches: 16000, 0:00:03
[[34m2023-05-13 17:58:31[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 17:58:34[0m] evaluated batches: 18000, 0:00:03
[[34m2023-05-13 17:58:38[0m] evaluated batches: 19000, 0:00:03
[[34m2023-05-13 17:58:41[0m] evaluated batches: 20000, 0:00:03
[[34m2023-05-13 17:58:45[0m] evaluated batches: 21000, 0:00:03
[[34m2023-05-13 17:58:48[0m] evaluated batches: 22000, 0:00:03
[[34m2023-05-13 17:58:51[0m] evaluated batches: 23000, 0:00:03
[[34m2023-05-13 17:58:55[0m] evaluated batches: 24000, 0:00:03
[[34m2023-05-13 17:58:58[0m] evaluated batches: 25000, 0:00:03
[[34m2023-05-13 17:59:07[0m] test loss = 0.547280, test auc = 0.794534
[[34m2023-05-13 17:59:07[0m] evaluated time: 0:01:34
[[34m2023-05-13 17:59:07[0m] analyse_structure
[[34m2023-05-13 17:59:32[0m] elapsed : 0:47:46, ETA : 0:31:05
[[34m2023-05-13 17:59:32[0m] epoch 7 / 10, batch 1000 / 17377, global_step = 105262, learning_rate = 1.000000e-02, loss = 0.751369, l2 = 0.011666, auc = 0.796082
elapsed : 0:47:46, ETA : 0:31:05
epoch 7 / 10, batch 1000 / 17377, global_step = 105262, learning_rate = 1.000000e-02, loss = 0.751369, l2 = 0.011666, auc = 0.796082
[[34m2023-05-13 17:59:57[0m] elapsed : 0:48:11, ETA : 0:30:36
[[34m2023-05-13 17:59:57[0m] epoch 7 / 10, batch 2000 / 17377, global_step = 106262, learning_rate = 1.000000e-02, loss = 0.546033, l2 = 0.008457, auc = 0.795752
elapsed : 0:48:11, ETA : 0:30:36
epoch 7 / 10, batch 2000 / 17377, global_step = 106262, learning_rate = 1.000000e-02, loss = 0.546033, l2 = 0.008457, auc = 0.795752
[[34m2023-05-13 18:00:22[0m] elapsed : 0:48:36, ETA : 0:30:08
[[34m2023-05-13 18:00:22[0m] epoch 7 / 10, batch 3000 / 17377, global_step = 107262, learning_rate = 1.000000e-02, loss = 0.546872, l2 = 0.008446, auc = 0.794856
elapsed : 0:48:36, ETA : 0:30:08
epoch 7 / 10, batch 3000 / 17377, global_step = 107262, learning_rate = 1.000000e-02, loss = 0.546872, l2 = 0.008446, auc = 0.794856
[[34m2023-05-13 18:00:48[0m] elapsed : 0:49:01, ETA : 0:29:39
[[34m2023-05-13 18:00:48[0m] epoch 7 / 10, batch 4000 / 17377, global_step = 108262, learning_rate = 1.000000e-02, loss = 0.545660, l2 = 0.008422, auc = 0.796160
elapsed : 0:49:01, ETA : 0:29:39
epoch 7 / 10, batch 4000 / 17377, global_step = 108262, learning_rate = 1.000000e-02, loss = 0.545660, l2 = 0.008422, auc = 0.796160
[[34m2023-05-13 18:01:12[0m] elapsed : 0:49:26, ETA : 0:29:11
[[34m2023-05-13 18:01:12[0m] epoch 7 / 10, batch 5000 / 17377, global_step = 109262, learning_rate = 1.000000e-02, loss = 0.544049, l2 = 0.008396, auc = 0.797718
elapsed : 0:49:26, ETA : 0:29:11
epoch 7 / 10, batch 5000 / 17377, global_step = 109262, learning_rate = 1.000000e-02, loss = 0.544049, l2 = 0.008396, auc = 0.797718
[[34m2023-05-13 18:01:37[0m] elapsed : 0:49:51, ETA : 0:28:42
[[34m2023-05-13 18:01:37[0m] epoch 7 / 10, batch 6000 / 17377, global_step = 110262, learning_rate = 1.000000e-02, loss = 0.545583, l2 = 0.008385, auc = 0.796375
elapsed : 0:49:51, ETA : 0:28:42
epoch 7 / 10, batch 6000 / 17377, global_step = 110262, learning_rate = 1.000000e-02, loss = 0.545583, l2 = 0.008385, auc = 0.796375
[[34m2023-05-13 18:02:03[0m] elapsed : 0:50:16, ETA : 0:28:14
[[34m2023-05-13 18:02:03[0m] epoch 7 / 10, batch 7000 / 17377, global_step = 111262, learning_rate = 1.000000e-02, loss = 0.546131, l2 = 0.008384, auc = 0.795762
elapsed : 0:50:16, ETA : 0:28:14
epoch 7 / 10, batch 7000 / 17377, global_step = 111262, learning_rate = 1.000000e-02, loss = 0.546131, l2 = 0.008384, auc = 0.795762
[[34m2023-05-13 18:02:23[0m] elapsed : 0:50:37, ETA : 0:27:43
[[34m2023-05-13 18:02:23[0m] epoch 7 / 10, batch 8000 / 17377, global_step = 112262, learning_rate = 1.000000e-02, loss = 0.546117, l2 = 0.008354, auc = 0.795864
elapsed : 0:50:37, ETA : 0:27:43
epoch 7 / 10, batch 8000 / 17377, global_step = 112262, learning_rate = 1.000000e-02, loss = 0.546117, l2 = 0.008354, auc = 0.795864
[[34m2023-05-13 18:02:41[0m] elapsed : 0:50:55, ETA : 0:27:12
[[34m2023-05-13 18:02:41[0m] epoch 7 / 10, batch 9000 / 17377, global_step = 113262, learning_rate = 1.000000e-02, loss = 0.545394, l2 = 0.008363, auc = 0.796440
elapsed : 0:50:55, ETA : 0:27:12
epoch 7 / 10, batch 9000 / 17377, global_step = 113262, learning_rate = 1.000000e-02, loss = 0.545394, l2 = 0.008363, auc = 0.796440
[[34m2023-05-13 18:02:59[0m] elapsed : 0:51:12, ETA : 0:26:39
[[34m2023-05-13 18:02:59[0m] epoch 7 / 10, batch 10000 / 17377, global_step = 114262, learning_rate = 1.000000e-02, loss = 0.544156, l2 = 0.008344, auc = 0.797525
elapsed : 0:51:12, ETA : 0:26:39
epoch 7 / 10, batch 10000 / 17377, global_step = 114262, learning_rate = 1.000000e-02, loss = 0.544156, l2 = 0.008344, auc = 0.797525
[[34m2023-05-13 18:03:17[0m] elapsed : 0:51:30, ETA : 0:26:08
[[34m2023-05-13 18:03:17[0m] epoch 7 / 10, batch 11000 / 17377, global_step = 115262, learning_rate = 1.000000e-02, loss = 0.546069, l2 = 0.008325, auc = 0.795703
elapsed : 0:51:30, ETA : 0:26:08
epoch 7 / 10, batch 11000 / 17377, global_step = 115262, learning_rate = 1.000000e-02, loss = 0.546069, l2 = 0.008325, auc = 0.795703
[[34m2023-05-13 18:03:35[0m] elapsed : 0:51:49, ETA : 0:25:37
[[34m2023-05-13 18:03:35[0m] epoch 7 / 10, batch 12000 / 17377, global_step = 116262, learning_rate = 1.000000e-02, loss = 0.545674, l2 = 0.008286, auc = 0.796106
elapsed : 0:51:49, ETA : 0:25:37
epoch 7 / 10, batch 12000 / 17377, global_step = 116262, learning_rate = 1.000000e-02, loss = 0.545674, l2 = 0.008286, auc = 0.796106
[[34m2023-05-13 18:03:53[0m] elapsed : 0:52:07, ETA : 0:25:06
[[34m2023-05-13 18:03:53[0m] epoch 7 / 10, batch 13000 / 17377, global_step = 117262, learning_rate = 1.000000e-02, loss = 0.545723, l2 = 0.008278, auc = 0.796019
elapsed : 0:52:07, ETA : 0:25:06
epoch 7 / 10, batch 13000 / 17377, global_step = 117262, learning_rate = 1.000000e-02, loss = 0.545723, l2 = 0.008278, auc = 0.796019
[[34m2023-05-13 18:04:11[0m] elapsed : 0:52:24, ETA : 0:24:35
[[34m2023-05-13 18:04:11[0m] epoch 7 / 10, batch 14000 / 17377, global_step = 118262, learning_rate = 1.000000e-02, loss = 0.545131, l2 = 0.008266, auc = 0.796723
elapsed : 0:52:24, ETA : 0:24:35
epoch 7 / 10, batch 14000 / 17377, global_step = 118262, learning_rate = 1.000000e-02, loss = 0.545131, l2 = 0.008266, auc = 0.796723
[[34m2023-05-13 18:04:29[0m] elapsed : 0:52:42, ETA : 0:24:05
[[34m2023-05-13 18:04:29[0m] epoch 7 / 10, batch 15000 / 17377, global_step = 119262, learning_rate = 1.000000e-02, loss = 0.546645, l2 = 0.008272, auc = 0.795202
elapsed : 0:52:42, ETA : 0:24:05
epoch 7 / 10, batch 15000 / 17377, global_step = 119262, learning_rate = 1.000000e-02, loss = 0.546645, l2 = 0.008272, auc = 0.795202
[[34m2023-05-13 18:04:47[0m] elapsed : 0:53:00, ETA : 0:23:34
[[34m2023-05-13 18:04:47[0m] epoch 7 / 10, batch 16000 / 17377, global_step = 120262, learning_rate = 1.000000e-02, loss = 0.545181, l2 = 0.008259, auc = 0.796529
elapsed : 0:53:00, ETA : 0:23:34
epoch 7 / 10, batch 16000 / 17377, global_step = 120262, learning_rate = 1.000000e-02, loss = 0.545181, l2 = 0.008259, auc = 0.796529
[[34m2023-05-13 18:05:05[0m] elapsed : 0:53:18, ETA : 0:23:04
[[34m2023-05-13 18:05:05[0m] epoch 7 / 10, batch 17000 / 17377, global_step = 121262, learning_rate = 1.000000e-02, loss = 0.545147, l2 = 0.008233, auc = 0.796554
elapsed : 0:53:18, ETA : 0:23:04
epoch 7 / 10, batch 17000 / 17377, global_step = 121262, learning_rate = 1.000000e-02, loss = 0.545147, l2 = 0.008233, auc = 0.796554
[[34m2023-05-13 18:05:14[0m] running test...
on disk...
[[34m2023-05-13 18:05:18[0m] evaluated batches: 1000, 0:00:03
[[34m2023-05-13 18:05:21[0m] evaluated batches: 2000, 0:00:03
[[34m2023-05-13 18:05:25[0m] evaluated batches: 3000, 0:00:03
[[34m2023-05-13 18:05:28[0m] evaluated batches: 4000, 0:00:03
[[34m2023-05-13 18:05:32[0m] evaluated batches: 5000, 0:00:03
[[34m2023-05-13 18:05:35[0m] evaluated batches: 6000, 0:00:03
[[34m2023-05-13 18:05:38[0m] evaluated batches: 7000, 0:00:03
[[34m2023-05-13 18:05:41[0m] evaluated batches: 8000, 0:00:03
[[34m2023-05-13 18:05:45[0m] evaluated batches: 9000, 0:00:03
[[34m2023-05-13 18:05:48[0m] evaluated batches: 10000, 0:00:03
[[34m2023-05-13 18:05:52[0m] evaluated batches: 11000, 0:00:03
[[34m2023-05-13 18:05:55[0m] evaluated batches: 12000, 0:00:03
[[34m2023-05-13 18:05:59[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 18:06:02[0m] evaluated batches: 14000, 0:00:03
[[34m2023-05-13 18:06:06[0m] evaluated batches: 15000, 0:00:03
[[34m2023-05-13 18:06:09[0m] evaluated batches: 16000, 0:00:03
[[34m2023-05-13 18:06:13[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 18:06:16[0m] evaluated batches: 18000, 0:00:03
[[34m2023-05-13 18:06:19[0m] evaluated batches: 19000, 0:00:03
[[34m2023-05-13 18:06:23[0m] evaluated batches: 20000, 0:00:03
[[34m2023-05-13 18:06:26[0m] evaluated batches: 21000, 0:00:03
[[34m2023-05-13 18:06:30[0m] evaluated batches: 22000, 0:00:03
[[34m2023-05-13 18:06:33[0m] evaluated batches: 23000, 0:00:03
[[34m2023-05-13 18:06:36[0m] evaluated batches: 24000, 0:00:03
[[34m2023-05-13 18:06:40[0m] evaluated batches: 25000, 0:00:03
[[34m2023-05-13 18:06:48[0m] test loss = 0.546438, test auc = 0.795274
[[34m2023-05-13 18:06:48[0m] evaluated time: 0:01:34
[[34m2023-05-13 18:06:48[0m] analyse_structure
[[34m2023-05-13 18:07:13[0m] elapsed : 0:55:27, ETA : 0:23:07
[[34m2023-05-13 18:07:13[0m] epoch 8 / 10, batch 1000 / 17377, global_step = 122639, learning_rate = 1.000000e-02, loss = 0.750021, l2 = 0.011327, auc = 0.797099
elapsed : 0:55:27, ETA : 0:23:07
epoch 8 / 10, batch 1000 / 17377, global_step = 122639, learning_rate = 1.000000e-02, loss = 0.750021, l2 = 0.011327, auc = 0.797099
[[34m2023-05-13 18:07:39[0m] elapsed : 0:55:52, ETA : 0:22:39
[[34m2023-05-13 18:07:39[0m] epoch 8 / 10, batch 2000 / 17377, global_step = 123639, learning_rate = 1.000000e-02, loss = 0.544983, l2 = 0.008204, auc = 0.796641
elapsed : 0:55:52, ETA : 0:22:39
epoch 8 / 10, batch 2000 / 17377, global_step = 123639, learning_rate = 1.000000e-02, loss = 0.544983, l2 = 0.008204, auc = 0.796641
[[34m2023-05-13 18:08:04[0m] elapsed : 0:56:17, ETA : 0:22:11
[[34m2023-05-13 18:08:04[0m] epoch 8 / 10, batch 3000 / 17377, global_step = 124639, learning_rate = 1.000000e-02, loss = 0.545438, l2 = 0.008183, auc = 0.796344
elapsed : 0:56:17, ETA : 0:22:11
epoch 8 / 10, batch 3000 / 17377, global_step = 124639, learning_rate = 1.000000e-02, loss = 0.545438, l2 = 0.008183, auc = 0.796344
[[34m2023-05-13 18:08:29[0m] elapsed : 0:56:42, ETA : 0:21:43
[[34m2023-05-13 18:08:29[0m] epoch 8 / 10, batch 4000 / 17377, global_step = 125639, learning_rate = 1.000000e-02, loss = 0.544633, l2 = 0.008180, auc = 0.797156
elapsed : 0:56:42, ETA : 0:21:43
epoch 8 / 10, batch 4000 / 17377, global_step = 125639, learning_rate = 1.000000e-02, loss = 0.544633, l2 = 0.008180, auc = 0.797156
[[34m2023-05-13 18:08:54[0m] elapsed : 0:57:07, ETA : 0:21:15
[[34m2023-05-13 18:08:54[0m] epoch 8 / 10, batch 5000 / 17377, global_step = 126639, learning_rate = 1.000000e-02, loss = 0.544876, l2 = 0.008164, auc = 0.796898
elapsed : 0:57:07, ETA : 0:21:15
epoch 8 / 10, batch 5000 / 17377, global_step = 126639, learning_rate = 1.000000e-02, loss = 0.544876, l2 = 0.008164, auc = 0.796898
[[34m2023-05-13 18:09:19[0m] elapsed : 0:57:32, ETA : 0:20:47
[[34m2023-05-13 18:09:19[0m] epoch 8 / 10, batch 6000 / 17377, global_step = 127639, learning_rate = 1.000000e-02, loss = 0.545453, l2 = 0.008171, auc = 0.796360
elapsed : 0:57:32, ETA : 0:20:47
epoch 8 / 10, batch 6000 / 17377, global_step = 127639, learning_rate = 1.000000e-02, loss = 0.545453, l2 = 0.008171, auc = 0.796360
[[34m2023-05-13 18:09:41[0m] elapsed : 0:57:54, ETA : 0:20:18
[[34m2023-05-13 18:09:41[0m] epoch 8 / 10, batch 7000 / 17377, global_step = 128639, learning_rate = 1.000000e-02, loss = 0.544847, l2 = 0.008155, auc = 0.796850
elapsed : 0:57:54, ETA : 0:20:18
epoch 8 / 10, batch 7000 / 17377, global_step = 128639, learning_rate = 1.000000e-02, loss = 0.544847, l2 = 0.008155, auc = 0.796850
[[34m2023-05-13 18:09:59[0m] elapsed : 0:58:13, ETA : 0:19:49
[[34m2023-05-13 18:09:59[0m] epoch 8 / 10, batch 8000 / 17377, global_step = 129639, learning_rate = 1.000000e-02, loss = 0.545069, l2 = 0.008136, auc = 0.796539
elapsed : 0:58:13, ETA : 0:19:49
epoch 8 / 10, batch 8000 / 17377, global_step = 129639, learning_rate = 1.000000e-02, loss = 0.545069, l2 = 0.008136, auc = 0.796539
[[34m2023-05-13 18:10:17[0m] elapsed : 0:58:30, ETA : 0:19:18
[[34m2023-05-13 18:10:17[0m] epoch 8 / 10, batch 9000 / 17377, global_step = 130639, learning_rate = 1.000000e-02, loss = 0.544431, l2 = 0.008113, auc = 0.797284
elapsed : 0:58:30, ETA : 0:19:18
epoch 8 / 10, batch 9000 / 17377, global_step = 130639, learning_rate = 1.000000e-02, loss = 0.544431, l2 = 0.008113, auc = 0.797284
[[34m2023-05-13 18:10:35[0m] elapsed : 0:58:48, ETA : 0:18:49
[[34m2023-05-13 18:10:35[0m] epoch 8 / 10, batch 10000 / 17377, global_step = 131639, learning_rate = 1.000000e-02, loss = 0.545464, l2 = 0.008098, auc = 0.796336
elapsed : 0:58:48, ETA : 0:18:49
epoch 8 / 10, batch 10000 / 17377, global_step = 131639, learning_rate = 1.000000e-02, loss = 0.545464, l2 = 0.008098, auc = 0.796336
[[34m2023-05-13 18:10:53[0m] elapsed : 0:59:06, ETA : 0:18:19
[[34m2023-05-13 18:10:53[0m] epoch 8 / 10, batch 11000 / 17377, global_step = 132639, learning_rate = 1.000000e-02, loss = 0.544470, l2 = 0.008081, auc = 0.797259
elapsed : 0:59:06, ETA : 0:18:19
epoch 8 / 10, batch 11000 / 17377, global_step = 132639, learning_rate = 1.000000e-02, loss = 0.544470, l2 = 0.008081, auc = 0.797259
[[34m2023-05-13 18:11:11[0m] elapsed : 0:59:24, ETA : 0:17:50
[[34m2023-05-13 18:11:11[0m] epoch 8 / 10, batch 12000 / 17377, global_step = 133639, learning_rate = 1.000000e-02, loss = 0.544891, l2 = 0.008072, auc = 0.796899
elapsed : 0:59:24, ETA : 0:17:50
epoch 8 / 10, batch 12000 / 17377, global_step = 133639, learning_rate = 1.000000e-02, loss = 0.544891, l2 = 0.008072, auc = 0.796899
[[34m2023-05-13 18:11:29[0m] elapsed : 0:59:42, ETA : 0:17:21
[[34m2023-05-13 18:11:29[0m] epoch 8 / 10, batch 13000 / 17377, global_step = 134639, learning_rate = 1.000000e-02, loss = 0.544761, l2 = 0.008060, auc = 0.797043
elapsed : 0:59:42, ETA : 0:17:21
epoch 8 / 10, batch 13000 / 17377, global_step = 134639, learning_rate = 1.000000e-02, loss = 0.544761, l2 = 0.008060, auc = 0.797043
[[34m2023-05-13 18:11:47[0m] elapsed : 1:00:00, ETA : 0:16:52
[[34m2023-05-13 18:11:47[0m] epoch 8 / 10, batch 14000 / 17377, global_step = 135639, learning_rate = 1.000000e-02, loss = 0.544909, l2 = 0.008048, auc = 0.796772
elapsed : 1:00:00, ETA : 0:16:52
epoch 8 / 10, batch 14000 / 17377, global_step = 135639, learning_rate = 1.000000e-02, loss = 0.544909, l2 = 0.008048, auc = 0.796772
[[34m2023-05-13 18:12:05[0m] elapsed : 1:00:18, ETA : 0:16:23
[[34m2023-05-13 18:12:05[0m] epoch 8 / 10, batch 15000 / 17377, global_step = 136639, learning_rate = 1.000000e-02, loss = 0.544138, l2 = 0.008022, auc = 0.797468
elapsed : 1:00:18, ETA : 0:16:23
epoch 8 / 10, batch 15000 / 17377, global_step = 136639, learning_rate = 1.000000e-02, loss = 0.544138, l2 = 0.008022, auc = 0.797468
[[34m2023-05-13 18:12:22[0m] elapsed : 1:00:35, ETA : 0:15:54
[[34m2023-05-13 18:12:22[0m] epoch 8 / 10, batch 16000 / 17377, global_step = 137639, learning_rate = 1.000000e-02, loss = 0.546626, l2 = 0.008010, auc = 0.795158
elapsed : 1:00:35, ETA : 0:15:54
epoch 8 / 10, batch 16000 / 17377, global_step = 137639, learning_rate = 1.000000e-02, loss = 0.546626, l2 = 0.008010, auc = 0.795158
[[34m2023-05-13 18:12:46[0m] elapsed : 1:01:00, ETA : 0:15:27
[[34m2023-05-13 18:12:46[0m] epoch 8 / 10, batch 17000 / 17377, global_step = 138639, learning_rate = 1.000000e-02, loss = 0.545286, l2 = 0.008017, auc = 0.796476
elapsed : 1:01:00, ETA : 0:15:27
epoch 8 / 10, batch 17000 / 17377, global_step = 138639, learning_rate = 1.000000e-02, loss = 0.545286, l2 = 0.008017, auc = 0.796476
[[34m2023-05-13 18:12:56[0m] running test...
on disk...
[[34m2023-05-13 18:12:59[0m] evaluated batches: 1000, 0:00:03
[[34m2023-05-13 18:13:03[0m] evaluated batches: 2000, 0:00:03
[[34m2023-05-13 18:13:06[0m] evaluated batches: 3000, 0:00:03
[[34m2023-05-13 18:13:09[0m] evaluated batches: 4000, 0:00:03
[[34m2023-05-13 18:13:13[0m] evaluated batches: 5000, 0:00:03
[[34m2023-05-13 18:13:16[0m] evaluated batches: 6000, 0:00:03
[[34m2023-05-13 18:13:20[0m] evaluated batches: 7000, 0:00:03
[[34m2023-05-13 18:13:23[0m] evaluated batches: 8000, 0:00:03
[[34m2023-05-13 18:13:27[0m] evaluated batches: 9000, 0:00:03
[[34m2023-05-13 18:13:30[0m] evaluated batches: 10000, 0:00:03
[[34m2023-05-13 18:13:33[0m] evaluated batches: 11000, 0:00:03
[[34m2023-05-13 18:13:37[0m] evaluated batches: 12000, 0:00:03
[[34m2023-05-13 18:13:40[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 18:13:44[0m] evaluated batches: 14000, 0:00:03
[[34m2023-05-13 18:13:47[0m] evaluated batches: 15000, 0:00:03
[[34m2023-05-13 18:13:50[0m] evaluated batches: 16000, 0:00:03
[[34m2023-05-13 18:13:54[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 18:13:57[0m] evaluated batches: 18000, 0:00:03
[[34m2023-05-13 18:14:01[0m] evaluated batches: 19000, 0:00:03
[[34m2023-05-13 18:14:04[0m] evaluated batches: 20000, 0:00:03
[[34m2023-05-13 18:14:08[0m] evaluated batches: 21000, 0:00:03
[[34m2023-05-13 18:14:11[0m] evaluated batches: 22000, 0:00:03
[[34m2023-05-13 18:14:14[0m] evaluated batches: 23000, 0:00:03
[[34m2023-05-13 18:14:18[0m] evaluated batches: 24000, 0:00:03
[[34m2023-05-13 18:14:21[0m] evaluated batches: 25000, 0:00:03
[[34m2023-05-13 18:14:30[0m] test loss = 0.546638, test auc = 0.795929
[[34m2023-05-13 18:14:30[0m] evaluated time: 0:01:34
[[34m2023-05-13 18:14:30[0m] analyse_structure
[[34m2023-05-13 18:14:56[0m] elapsed : 1:03:09, ETA : 0:15:13
[[34m2023-05-13 18:14:56[0m] epoch 9 / 10, batch 1000 / 17377, global_step = 140016, learning_rate = 1.000000e-02, loss = 0.750328, l2 = 0.011003, auc = 0.796807
elapsed : 1:03:09, ETA : 0:15:13
epoch 9 / 10, batch 1000 / 17377, global_step = 140016, learning_rate = 1.000000e-02, loss = 0.750328, l2 = 0.011003, auc = 0.796807
[[34m2023-05-13 18:15:20[0m] elapsed : 1:03:34, ETA : 0:14:45
[[34m2023-05-13 18:15:20[0m] epoch 9 / 10, batch 2000 / 17377, global_step = 141016, learning_rate = 1.000000e-02, loss = 0.544632, l2 = 0.008004, auc = 0.797013
elapsed : 1:03:34, ETA : 0:14:45
epoch 9 / 10, batch 2000 / 17377, global_step = 141016, learning_rate = 1.000000e-02, loss = 0.544632, l2 = 0.008004, auc = 0.797013
[[34m2023-05-13 18:15:46[0m] elapsed : 1:03:59, ETA : 0:14:18
[[34m2023-05-13 18:15:46[0m] epoch 9 / 10, batch 3000 / 17377, global_step = 142016, learning_rate = 1.000000e-02, loss = 0.544411, l2 = 0.007999, auc = 0.797463
elapsed : 1:03:59, ETA : 0:14:18
epoch 9 / 10, batch 3000 / 17377, global_step = 142016, learning_rate = 1.000000e-02, loss = 0.544411, l2 = 0.007999, auc = 0.797463
[[34m2023-05-13 18:16:10[0m] elapsed : 1:04:24, ETA : 0:13:50
[[34m2023-05-13 18:16:10[0m] epoch 9 / 10, batch 4000 / 17377, global_step = 143016, learning_rate = 1.000000e-02, loss = 0.544283, l2 = 0.007977, auc = 0.797434
elapsed : 1:04:24, ETA : 0:13:50
epoch 9 / 10, batch 4000 / 17377, global_step = 143016, learning_rate = 1.000000e-02, loss = 0.544283, l2 = 0.007977, auc = 0.797434
[[34m2023-05-13 18:16:36[0m] elapsed : 1:04:49, ETA : 0:13:23
[[34m2023-05-13 18:16:36[0m] epoch 9 / 10, batch 5000 / 17377, global_step = 144016, learning_rate = 1.000000e-02, loss = 0.544605, l2 = 0.007950, auc = 0.797257
elapsed : 1:04:49, ETA : 0:13:23
epoch 9 / 10, batch 5000 / 17377, global_step = 144016, learning_rate = 1.000000e-02, loss = 0.544605, l2 = 0.007950, auc = 0.797257
[[34m2023-05-13 18:16:59[0m] elapsed : 1:05:12, ETA : 0:12:55
[[34m2023-05-13 18:16:59[0m] epoch 9 / 10, batch 6000 / 17377, global_step = 145016, learning_rate = 1.000000e-02, loss = 0.544380, l2 = 0.007953, auc = 0.797450
elapsed : 1:05:12, ETA : 0:12:55
epoch 9 / 10, batch 6000 / 17377, global_step = 145016, learning_rate = 1.000000e-02, loss = 0.544380, l2 = 0.007953, auc = 0.797450
[[34m2023-05-13 18:17:17[0m] elapsed : 1:05:30, ETA : 0:12:26
[[34m2023-05-13 18:17:17[0m] epoch 9 / 10, batch 7000 / 17377, global_step = 146016, learning_rate = 1.000000e-02, loss = 0.544458, l2 = 0.007929, auc = 0.797186
elapsed : 1:05:30, ETA : 0:12:26
epoch 9 / 10, batch 7000 / 17377, global_step = 146016, learning_rate = 1.000000e-02, loss = 0.544458, l2 = 0.007929, auc = 0.797186
[[34m2023-05-13 18:17:35[0m] elapsed : 1:05:48, ETA : 0:11:58
[[34m2023-05-13 18:17:35[0m] epoch 9 / 10, batch 8000 / 17377, global_step = 147016, learning_rate = 1.000000e-02, loss = 0.544158, l2 = 0.007922, auc = 0.797634
elapsed : 1:05:48, ETA : 0:11:58
epoch 9 / 10, batch 8000 / 17377, global_step = 147016, learning_rate = 1.000000e-02, loss = 0.544158, l2 = 0.007922, auc = 0.797634
[[34m2023-05-13 18:17:53[0m] elapsed : 1:06:06, ETA : 0:11:30
[[34m2023-05-13 18:17:53[0m] epoch 9 / 10, batch 9000 / 17377, global_step = 148016, learning_rate = 1.000000e-02, loss = 0.544359, l2 = 0.007912, auc = 0.797471
elapsed : 1:06:06, ETA : 0:11:30
epoch 9 / 10, batch 9000 / 17377, global_step = 148016, learning_rate = 1.000000e-02, loss = 0.544359, l2 = 0.007912, auc = 0.797471
[[34m2023-05-13 18:18:11[0m] elapsed : 1:06:24, ETA : 0:11:01
[[34m2023-05-13 18:18:11[0m] epoch 9 / 10, batch 10000 / 17377, global_step = 149016, learning_rate = 1.000000e-02, loss = 0.543445, l2 = 0.007906, auc = 0.798317
elapsed : 1:06:24, ETA : 0:11:01
epoch 9 / 10, batch 10000 / 17377, global_step = 149016, learning_rate = 1.000000e-02, loss = 0.543445, l2 = 0.007906, auc = 0.798317
[[34m2023-05-13 18:18:29[0m] elapsed : 1:06:42, ETA : 0:10:33
[[34m2023-05-13 18:18:29[0m] epoch 9 / 10, batch 11000 / 17377, global_step = 150016, learning_rate = 1.000000e-02, loss = 0.544233, l2 = 0.007890, auc = 0.797462
elapsed : 1:06:42, ETA : 0:10:33
epoch 9 / 10, batch 11000 / 17377, global_step = 150016, learning_rate = 1.000000e-02, loss = 0.544233, l2 = 0.007890, auc = 0.797462
[[34m2023-05-13 18:18:47[0m] elapsed : 1:07:00, ETA : 0:10:05
[[34m2023-05-13 18:18:47[0m] epoch 9 / 10, batch 12000 / 17377, global_step = 151016, learning_rate = 1.000000e-02, loss = 0.543447, l2 = 0.007886, auc = 0.798324
elapsed : 1:07:00, ETA : 0:10:05
epoch 9 / 10, batch 12000 / 17377, global_step = 151016, learning_rate = 1.000000e-02, loss = 0.543447, l2 = 0.007886, auc = 0.798324
[[34m2023-05-13 18:19:05[0m] elapsed : 1:07:18, ETA : 0:09:37
[[34m2023-05-13 18:19:05[0m] epoch 9 / 10, batch 13000 / 17377, global_step = 152016, learning_rate = 1.000000e-02, loss = 0.544212, l2 = 0.007880, auc = 0.797481
elapsed : 1:07:18, ETA : 0:09:37
epoch 9 / 10, batch 13000 / 17377, global_step = 152016, learning_rate = 1.000000e-02, loss = 0.544212, l2 = 0.007880, auc = 0.797481
[[34m2023-05-13 18:19:23[0m] elapsed : 1:07:36, ETA : 0:09:10
[[34m2023-05-13 18:19:23[0m] epoch 9 / 10, batch 14000 / 17377, global_step = 153016, learning_rate = 1.000000e-02, loss = 0.543944, l2 = 0.007870, auc = 0.797746
elapsed : 1:07:36, ETA : 0:09:10
epoch 9 / 10, batch 14000 / 17377, global_step = 153016, learning_rate = 1.000000e-02, loss = 0.543944, l2 = 0.007870, auc = 0.797746
[[34m2023-05-13 18:19:38[0m] elapsed : 1:07:51, ETA : 0:08:42
[[34m2023-05-13 18:19:38[0m] epoch 9 / 10, batch 15000 / 17377, global_step = 154016, learning_rate = 1.000000e-02, loss = 0.543946, l2 = 0.007864, auc = 0.797613
elapsed : 1:07:51, ETA : 0:08:42
epoch 9 / 10, batch 15000 / 17377, global_step = 154016, learning_rate = 1.000000e-02, loss = 0.543946, l2 = 0.007864, auc = 0.797613
[[34m2023-05-13 18:20:03[0m] elapsed : 1:08:16, ETA : 0:08:15
[[34m2023-05-13 18:20:03[0m] epoch 9 / 10, batch 16000 / 17377, global_step = 155016, learning_rate = 1.000000e-02, loss = 0.544827, l2 = 0.007849, auc = 0.797017
elapsed : 1:08:16, ETA : 0:08:15
epoch 9 / 10, batch 16000 / 17377, global_step = 155016, learning_rate = 1.000000e-02, loss = 0.544827, l2 = 0.007849, auc = 0.797017
[[34m2023-05-13 18:20:28[0m] elapsed : 1:08:41, ETA : 0:07:48
[[34m2023-05-13 18:20:28[0m] epoch 9 / 10, batch 17000 / 17377, global_step = 156016, learning_rate = 1.000000e-02, loss = 0.543106, l2 = 0.007847, auc = 0.798565
elapsed : 1:08:41, ETA : 0:07:48
epoch 9 / 10, batch 17000 / 17377, global_step = 156016, learning_rate = 1.000000e-02, loss = 0.543106, l2 = 0.007847, auc = 0.798565
[[34m2023-05-13 18:20:37[0m] running test...
on disk...
[[34m2023-05-13 18:20:41[0m] evaluated batches: 1000, 0:00:03
[[34m2023-05-13 18:20:45[0m] evaluated batches: 2000, 0:00:03
[[34m2023-05-13 18:20:48[0m] evaluated batches: 3000, 0:00:03
[[34m2023-05-13 18:20:51[0m] evaluated batches: 4000, 0:00:03
[[34m2023-05-13 18:20:55[0m] evaluated batches: 5000, 0:00:03
[[34m2023-05-13 18:20:58[0m] evaluated batches: 6000, 0:00:03
[[34m2023-05-13 18:21:01[0m] evaluated batches: 7000, 0:00:03
[[34m2023-05-13 18:21:05[0m] evaluated batches: 8000, 0:00:03
[[34m2023-05-13 18:21:08[0m] evaluated batches: 9000, 0:00:03
[[34m2023-05-13 18:21:12[0m] evaluated batches: 10000, 0:00:03
[[34m2023-05-13 18:21:15[0m] evaluated batches: 11000, 0:00:03
[[34m2023-05-13 18:21:18[0m] evaluated batches: 12000, 0:00:03
[[34m2023-05-13 18:21:22[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 18:21:25[0m] evaluated batches: 14000, 0:00:03
[[34m2023-05-13 18:21:29[0m] evaluated batches: 15000, 0:00:03
[[34m2023-05-13 18:21:32[0m] evaluated batches: 16000, 0:00:03
[[34m2023-05-13 18:21:36[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 18:21:39[0m] evaluated batches: 18000, 0:00:03
[[34m2023-05-13 18:21:42[0m] evaluated batches: 19000, 0:00:03
[[34m2023-05-13 18:21:46[0m] evaluated batches: 20000, 0:00:03
[[34m2023-05-13 18:21:49[0m] evaluated batches: 21000, 0:00:03
[[34m2023-05-13 18:21:53[0m] evaluated batches: 22000, 0:00:03
[[34m2023-05-13 18:21:56[0m] evaluated batches: 23000, 0:00:03
[[34m2023-05-13 18:21:59[0m] evaluated batches: 24000, 0:00:03
[[34m2023-05-13 18:22:03[0m] evaluated batches: 25000, 0:00:03
[[34m2023-05-13 18:22:12[0m] test loss = 0.545665, test auc = 0.796319
[[34m2023-05-13 18:22:12[0m] evaluated time: 0:01:34
[[34m2023-05-13 18:22:12[0m] analyse_structure
[[34m2023-05-13 18:22:37[0m] elapsed : 1:10:50, ETA : 0:07:22
[[34m2023-05-13 18:22:37[0m] epoch 10 / 10, batch 1000 / 17377, global_step = 157393, learning_rate = 1.000000e-02, loss = 0.749317, l2 = 0.010766, auc = 0.797557
elapsed : 1:10:50, ETA : 0:07:22
epoch 10 / 10, batch 1000 / 17377, global_step = 157393, learning_rate = 1.000000e-02, loss = 0.749317, l2 = 0.010766, auc = 0.797557
[[34m2023-05-13 18:23:02[0m] elapsed : 1:11:15, ETA : 0:06:55
[[34m2023-05-13 18:23:02[0m] epoch 10 / 10, batch 2000 / 17377, global_step = 158393, learning_rate = 1.000000e-02, loss = 0.543920, l2 = 0.007821, auc = 0.797774
elapsed : 1:11:15, ETA : 0:06:55
epoch 10 / 10, batch 2000 / 17377, global_step = 158393, learning_rate = 1.000000e-02, loss = 0.543920, l2 = 0.007821, auc = 0.797774
[[34m2023-05-13 18:23:27[0m] elapsed : 1:11:40, ETA : 0:06:27
[[34m2023-05-13 18:23:27[0m] epoch 10 / 10, batch 3000 / 17377, global_step = 159393, learning_rate = 1.000000e-02, loss = 0.543889, l2 = 0.007786, auc = 0.797673
elapsed : 1:11:40, ETA : 0:06:27
epoch 10 / 10, batch 3000 / 17377, global_step = 159393, learning_rate = 1.000000e-02, loss = 0.543889, l2 = 0.007786, auc = 0.797673
[[34m2023-05-13 18:23:52[0m] elapsed : 1:12:06, ETA : 0:06:00
[[34m2023-05-13 18:23:52[0m] epoch 10 / 10, batch 4000 / 17377, global_step = 160393, learning_rate = 1.000000e-02, loss = 0.542655, l2 = 0.007786, auc = 0.798818
elapsed : 1:12:06, ETA : 0:06:00
epoch 10 / 10, batch 4000 / 17377, global_step = 160393, learning_rate = 1.000000e-02, loss = 0.542655, l2 = 0.007786, auc = 0.798818
[[34m2023-05-13 18:24:16[0m] elapsed : 1:12:29, ETA : 0:05:33
[[34m2023-05-13 18:24:16[0m] epoch 10 / 10, batch 5000 / 17377, global_step = 161393, learning_rate = 1.000000e-02, loss = 0.543639, l2 = 0.007778, auc = 0.798075
elapsed : 1:12:29, ETA : 0:05:33
epoch 10 / 10, batch 5000 / 17377, global_step = 161393, learning_rate = 1.000000e-02, loss = 0.543639, l2 = 0.007778, auc = 0.798075
[[34m2023-05-13 18:24:34[0m] elapsed : 1:12:47, ETA : 0:05:05
[[34m2023-05-13 18:24:34[0m] epoch 10 / 10, batch 6000 / 17377, global_step = 162393, learning_rate = 1.000000e-02, loss = 0.543505, l2 = 0.007772, auc = 0.798100
elapsed : 1:12:47, ETA : 0:05:05
epoch 10 / 10, batch 6000 / 17377, global_step = 162393, learning_rate = 1.000000e-02, loss = 0.543505, l2 = 0.007772, auc = 0.798100
[[34m2023-05-13 18:24:52[0m] elapsed : 1:13:05, ETA : 0:04:38
[[34m2023-05-13 18:24:52[0m] epoch 10 / 10, batch 7000 / 17377, global_step = 163393, learning_rate = 1.000000e-02, loss = 0.543441, l2 = 0.007769, auc = 0.798063
elapsed : 1:13:05, ETA : 0:04:38
epoch 10 / 10, batch 7000 / 17377, global_step = 163393, learning_rate = 1.000000e-02, loss = 0.543441, l2 = 0.007769, auc = 0.798063
[[34m2023-05-13 18:25:10[0m] elapsed : 1:13:23, ETA : 0:04:11
[[34m2023-05-13 18:25:10[0m] epoch 10 / 10, batch 8000 / 17377, global_step = 164393, learning_rate = 1.000000e-02, loss = 0.542908, l2 = 0.007751, auc = 0.798638
elapsed : 1:13:23, ETA : 0:04:11
epoch 10 / 10, batch 8000 / 17377, global_step = 164393, learning_rate = 1.000000e-02, loss = 0.542908, l2 = 0.007751, auc = 0.798638
[[34m2023-05-13 18:25:28[0m] elapsed : 1:13:41, ETA : 0:03:43
[[34m2023-05-13 18:25:28[0m] epoch 10 / 10, batch 9000 / 17377, global_step = 165393, learning_rate = 1.000000e-02, loss = 0.543112, l2 = 0.007727, auc = 0.798594
elapsed : 1:13:41, ETA : 0:03:43
epoch 10 / 10, batch 9000 / 17377, global_step = 165393, learning_rate = 1.000000e-02, loss = 0.543112, l2 = 0.007727, auc = 0.798594
[[34m2023-05-13 18:25:46[0m] elapsed : 1:13:59, ETA : 0:03:16
[[34m2023-05-13 18:25:46[0m] epoch 10 / 10, batch 10000 / 17377, global_step = 166393, learning_rate = 1.000000e-02, loss = 0.542638, l2 = 0.007726, auc = 0.798950
elapsed : 1:13:59, ETA : 0:03:16
epoch 10 / 10, batch 10000 / 17377, global_step = 166393, learning_rate = 1.000000e-02, loss = 0.542638, l2 = 0.007726, auc = 0.798950
[[34m2023-05-13 18:26:04[0m] elapsed : 1:14:17, ETA : 0:02:49
[[34m2023-05-13 18:26:04[0m] epoch 10 / 10, batch 11000 / 17377, global_step = 167393, learning_rate = 1.000000e-02, loss = 0.544739, l2 = 0.007732, auc = 0.796969
elapsed : 1:14:17, ETA : 0:02:49
epoch 10 / 10, batch 11000 / 17377, global_step = 167393, learning_rate = 1.000000e-02, loss = 0.544739, l2 = 0.007732, auc = 0.796969
[[34m2023-05-13 18:26:22[0m] elapsed : 1:14:35, ETA : 0:02:22
[[34m2023-05-13 18:26:22[0m] epoch 10 / 10, batch 12000 / 17377, global_step = 168393, learning_rate = 1.000000e-02, loss = 0.543468, l2 = 0.007712, auc = 0.798138
elapsed : 1:14:35, ETA : 0:02:22
epoch 10 / 10, batch 12000 / 17377, global_step = 168393, learning_rate = 1.000000e-02, loss = 0.543468, l2 = 0.007712, auc = 0.798138
[[34m2023-05-13 18:26:40[0m] elapsed : 1:14:53, ETA : 0:01:56
[[34m2023-05-13 18:26:40[0m] epoch 10 / 10, batch 13000 / 17377, global_step = 169393, learning_rate = 1.000000e-02, loss = 0.542539, l2 = 0.007699, auc = 0.799080
elapsed : 1:14:53, ETA : 0:01:56
epoch 10 / 10, batch 13000 / 17377, global_step = 169393, learning_rate = 1.000000e-02, loss = 0.542539, l2 = 0.007699, auc = 0.799080
[[34m2023-05-13 18:26:55[0m] elapsed : 1:15:09, ETA : 0:01:29
[[34m2023-05-13 18:26:55[0m] epoch 10 / 10, batch 14000 / 17377, global_step = 170393, learning_rate = 1.000000e-02, loss = 0.543637, l2 = 0.007698, auc = 0.798125
elapsed : 1:15:09, ETA : 0:01:29
epoch 10 / 10, batch 14000 / 17377, global_step = 170393, learning_rate = 1.000000e-02, loss = 0.543637, l2 = 0.007698, auc = 0.798125
[[34m2023-05-13 18:27:05[0m] elapsed : 1:15:19, ETA : 0:01:02
[[34m2023-05-13 18:27:05[0m] epoch 10 / 10, batch 15000 / 17377, global_step = 171393, learning_rate = 1.000000e-02, loss = 0.543140, l2 = 0.007689, auc = 0.798461
elapsed : 1:15:19, ETA : 0:01:02
epoch 10 / 10, batch 15000 / 17377, global_step = 171393, learning_rate = 1.000000e-02, loss = 0.543140, l2 = 0.007689, auc = 0.798461
[[34m2023-05-13 18:27:15[0m] elapsed : 1:15:29, ETA : 0:00:36
[[34m2023-05-13 18:27:15[0m] epoch 10 / 10, batch 16000 / 17377, global_step = 172393, learning_rate = 1.000000e-02, loss = 0.541924, l2 = 0.007686, auc = 0.799346
elapsed : 1:15:29, ETA : 0:00:36
epoch 10 / 10, batch 16000 / 17377, global_step = 172393, learning_rate = 1.000000e-02, loss = 0.541924, l2 = 0.007686, auc = 0.799346
[[34m2023-05-13 18:27:25[0m] elapsed : 1:15:38, ETA : 0:00:09
[[34m2023-05-13 18:27:25[0m] epoch 10 / 10, batch 17000 / 17377, global_step = 173393, learning_rate = 1.000000e-02, loss = 0.543341, l2 = 0.007684, auc = 0.798253
elapsed : 1:15:38, ETA : 0:00:09
epoch 10 / 10, batch 17000 / 17377, global_step = 173393, learning_rate = 1.000000e-02, loss = 0.543341, l2 = 0.007684, auc = 0.798253
[[34m2023-05-13 18:27:29[0m] new iteration
new iteration
on disk...
[[34m2023-05-13 18:27:29[0m] running test...
on disk...
[[34m2023-05-13 18:27:32[0m] evaluated batches: 1000, 0:00:02
[[34m2023-05-13 18:27:34[0m] evaluated batches: 2000, 0:00:02
[[34m2023-05-13 18:27:36[0m] evaluated batches: 3000, 0:00:02
[[34m2023-05-13 18:27:38[0m] evaluated batches: 4000, 0:00:02
[[34m2023-05-13 18:27:40[0m] evaluated batches: 5000, 0:00:02
[[34m2023-05-13 18:27:42[0m] evaluated batches: 6000, 0:00:02
[[34m2023-05-13 18:27:44[0m] evaluated batches: 7000, 0:00:02
[[34m2023-05-13 18:27:46[0m] evaluated batches: 8000, 0:00:02
[[34m2023-05-13 18:27:49[0m] evaluated batches: 9000, 0:00:02
[[34m2023-05-13 18:27:51[0m] evaluated batches: 10000, 0:00:02
[[34m2023-05-13 18:27:53[0m] evaluated batches: 11000, 0:00:02
[[34m2023-05-13 18:27:55[0m] evaluated batches: 12000, 0:00:02
[[34m2023-05-13 18:27:57[0m] evaluated batches: 13000, 0:00:02
[[34m2023-05-13 18:27:59[0m] evaluated batches: 14000, 0:00:02
[[34m2023-05-13 18:28:01[0m] evaluated batches: 15000, 0:00:02
[[34m2023-05-13 18:28:03[0m] evaluated batches: 16000, 0:00:02
[[34m2023-05-13 18:28:06[0m] evaluated batches: 17000, 0:00:02
[[34m2023-05-13 18:28:08[0m] evaluated batches: 18000, 0:00:02
[[34m2023-05-13 18:28:10[0m] evaluated batches: 19000, 0:00:02
[[34m2023-05-13 18:28:12[0m] evaluated batches: 20000, 0:00:02
[[34m2023-05-13 18:28:14[0m] evaluated batches: 21000, 0:00:02
[[34m2023-05-13 18:28:16[0m] evaluated batches: 22000, 0:00:02
[[34m2023-05-13 18:28:18[0m] evaluated batches: 23000, 0:00:02
[[34m2023-05-13 18:28:21[0m] evaluated batches: 24000, 0:00:02
[[34m2023-05-13 18:28:23[0m] evaluated batches: 25000, 0:00:02
[[34m2023-05-13 18:28:31[0m] test loss = 0.545566, test auc = 0.796539
[[34m2023-05-13 18:28:31[0m] evaluated time: 0:01:01
[[34m2023-05-13 18:28:31[0m] analyse_structure
[[34m2023-05-13 18:28:31[0m] Done!
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:       batch_size â–
wandb:               lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         test_auc â–â–ƒâ–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:    test_log_loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–â–‚â–â–
wandb:    train_l2_loss â–ˆâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_loss â–‚â–‚â–‚â–â–ˆâ–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: train_moving_auc â–â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: batch_size 500
wandb:         lr 0.01
wandb: 
wandb: ðŸš€ View run criteo-BS-500-003-retrain_irazor-2023-05-13 17:11:37 at: https://wandb.ai/yao-yao/irazor/runs/q6xr2yao
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20230513_171138-q6xr2yao/logs
