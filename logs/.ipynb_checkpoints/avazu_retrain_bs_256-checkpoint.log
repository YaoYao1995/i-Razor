nohup: ignoring input
2023-05-13 13:52:39.825430: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-13 13:52:40.768872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Got hdf Avazu data set, getting metadata...
Initialization finished!
24
field-2: dim-1
field-3: dim-12
field-4: dim-18
field-6: dim-18
field-7: dim-1
field-8: dim-1
field-9: dim-5
field-10: dim-22
field-11: dim-18
field-13: dim-1
field-14: dim-5
field-17: dim-6
field-20: dim-6
field-21: dim-3
field-22: dim-2
field-23: dim-3
full_emb_for_1_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_2_emb_size_12 initialized from: -0.043664375559957246 0.043664375559957246
full_emb_for_3_emb_size_18 initialized from: -0.04137439097129242 0.04137439097129242
full_emb_for_5_emb_size_18 initialized from: -0.03863337046431279 0.03863337046431279
full_emb_for_6_emb_size_1 initialized from: -0.15399810070180361 0.15399810070180361
full_emb_for_7_emb_size_1 initialized from: -0.454858826147342 0.454858826147342
full_emb_for_8_emb_size_5 initialized from: -0.007690260262421491 0.007690260262421491
full_emb_for_9_emb_size_22 initialized from: -0.003384829723893765 0.003384829723893765
full_emb_for_10_emb_size_18 initialized from: -0.03177406356760468 0.03177406356760468
full_emb_for_12_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_13_emb_size_5 initialized from: -0.04977239691468088 0.04977239691468088
full_emb_for_16_emb_size_6 initialized from: -0.11785113019775792 0.11785113019775792
full_emb_for_19_emb_size_6 initialized from: -0.1867718419094071 0.1867718419094071
full_emb_for_20_emb_size_3 initialized from: -0.3086066999241838 0.3086066999241838
full_emb_for_21_emb_size_2 initialized from: -0.7071067811865476 0.7071067811865476
full_emb_for_22_emb_size_3 initialized from: -0.4714045207910317 0.4714045207910317
full_emb_for_0_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_1_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_2_emb_size_1 initialized from: -0.043740888263985325 0.043740888263985325
full_emb_for_3_emb_size_1 initialized from: -0.04147509477069983 0.04147509477069983
full_emb_for_4_emb_size_1 initialized from: -0.4898979485566356 0.4898979485566356
full_emb_for_5_emb_size_1 initialized from: -0.038715317938997504 0.038715317938997504
full_emb_for_6_emb_size_1 initialized from: -0.15399810070180361 0.15399810070180361
full_emb_for_7_emb_size_1 initialized from: -0.454858826147342 0.454858826147342
full_emb_for_8_emb_size_1 initialized from: -0.007690411867832244 0.007690411867832244
full_emb_for_9_emb_size_1 initialized from: -0.003384897591352657 0.003384897591352657
full_emb_for_10_emb_size_1 initialized from: -0.031819606281476856 0.031819606281476856
full_emb_for_11_emb_size_1 initialized from: -1.0 1.0
full_emb_for_12_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_13_emb_size_1 initialized from: -0.04981354813867179 0.04981354813867179
full_emb_for_14_emb_size_1 initialized from: -0.816496580927726 0.816496580927726
full_emb_for_15_emb_size_1 initialized from: -0.7745966692414834 0.7745966692414834
full_emb_for_16_emb_size_1 initialized from: -0.11853911695403994 0.11853911695403994
full_emb_for_17_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_18_emb_size_1 initialized from: -0.2970442628930023 0.2970442628930023
full_emb_for_19_emb_size_1 initialized from: -0.18954720708196904 0.18954720708196904
full_emb_for_20_emb_size_1 initialized from: -0.31362502409359 0.31362502409359
full_emb_for_21_emb_size_1 initialized from: -0.7385489458759964 0.7385489458759964
full_emb_for_22_emb_size_1 initialized from: -0.4898979485566356 0.4898979485566356
full_emb_for_23_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
w_0 initialized from: -0.0854357657716761 0.0854357657716761
(122, 700) (700,)
relu
w_1 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_2 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_3 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_4 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_5 initialized from: -0.0925159507394634 0.0925159507394634
(700, 1) (1,)
none
[[34m2023-05-13 13:52:43[0m] Experiment directory created at ~/results/retrain_irazor/avazu/001-retrain_irazor
[[34m2023-05-13 13:52:43[0m] Batchsize: 256
wandb: Currently logged in as: yao-yao. Use `wandb login --relogin` to force relogin
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /tmp/wandb/run-20230513_135244-dan0uqfv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avazu-BS-256-001-retrain_irazor-2023-05-13 13:52:43
wandb: ⭐️ View project at https://wandb.ai/yao-yao/irazor
wandb: 🚀 View run at https://wandb.ai/yao-yao/irazor/runs/dan0uqfv
2023-05-13 13:52:48.976465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 13:52:48.978083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 13:52:48.978878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 13:52:50.551491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 13:52:50.552606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 13:52:50.553429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 13:52:50.554228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 36121 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
[[34m2023-05-13 13:52:50[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
add l2 0.001 Tensor("concat:0", shape=(?, 122), dtype=float32)
add l2 0.001 Tensor("concat_1:0", shape=(?, 24), dtype=float32)
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
[[34m2023-05-13 13:52:51[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2023-05-13 13:52:51.659869: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
[[34m2023-05-13 13:52:51[0m] total batches: 126350	batch per epoch: 12635
[[34m2023-05-13 13:52:51[0m] new iteration
new iteration
on disk...
2023-05-13 13:52:53.495066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
[[34m2023-05-13 13:53:08[0m] elapsed : 0:00:16, ETA : 0:33:25
[[34m2023-05-13 13:53:08[0m] epoch 1 / 10, batch 1000 / 12635, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.439764, l2 = 0.182616, auc = 0.665648
elapsed : 0:00:16, ETA : 0:33:25
epoch 1 / 10, batch 1000 / 12635, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.439764, l2 = 0.182616, auc = 0.665648
[[34m2023-05-13 13:53:24[0m] elapsed : 0:00:32, ETA : 0:33:09
[[34m2023-05-13 13:53:24[0m] epoch 1 / 10, batch 2000 / 12635, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.403309, l2 = 0.049449, auc = 0.737083
elapsed : 0:00:32, ETA : 0:33:09
epoch 1 / 10, batch 2000 / 12635, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.403309, l2 = 0.049449, auc = 0.737083
[[34m2023-05-13 13:53:39[0m] elapsed : 0:00:47, ETA : 0:32:12
[[34m2023-05-13 13:53:39[0m] epoch 1 / 10, batch 3000 / 12635, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.398698, l2 = 0.022060, auc = 0.745051
elapsed : 0:00:47, ETA : 0:32:12
epoch 1 / 10, batch 3000 / 12635, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.398698, l2 = 0.022060, auc = 0.745051
