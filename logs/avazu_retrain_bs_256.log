nohup: ignoring input
2023-05-13 14:03:12.098204: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-13 14:03:13.049293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Got hdf Avazu data set, getting metadata...
Initialization finished!
24
field-2: dim-1
field-3: dim-12
field-4: dim-18
field-6: dim-18
field-7: dim-1
field-8: dim-1
field-9: dim-5
field-10: dim-22
field-11: dim-18
field-13: dim-1
field-14: dim-5
field-17: dim-6
field-20: dim-6
field-21: dim-3
field-22: dim-2
field-23: dim-3
full_emb_for_1_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_2_emb_size_12 initialized from: -0.043664375559957246 0.043664375559957246
full_emb_for_3_emb_size_18 initialized from: -0.04137439097129242 0.04137439097129242
full_emb_for_5_emb_size_18 initialized from: -0.03863337046431279 0.03863337046431279
full_emb_for_6_emb_size_1 initialized from: -0.15399810070180361 0.15399810070180361
full_emb_for_7_emb_size_1 initialized from: -0.454858826147342 0.454858826147342
full_emb_for_8_emb_size_5 initialized from: -0.007690260262421491 0.007690260262421491
full_emb_for_9_emb_size_22 initialized from: -0.003384829723893765 0.003384829723893765
full_emb_for_10_emb_size_18 initialized from: -0.03177406356760468 0.03177406356760468
full_emb_for_12_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_13_emb_size_5 initialized from: -0.04977239691468088 0.04977239691468088
full_emb_for_16_emb_size_6 initialized from: -0.11785113019775792 0.11785113019775792
full_emb_for_19_emb_size_6 initialized from: -0.1867718419094071 0.1867718419094071
full_emb_for_20_emb_size_3 initialized from: -0.3086066999241838 0.3086066999241838
full_emb_for_21_emb_size_2 initialized from: -0.7071067811865476 0.7071067811865476
full_emb_for_22_emb_size_3 initialized from: -0.4714045207910317 0.4714045207910317
full_emb_for_0_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_1_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_2_emb_size_1 initialized from: -0.043740888263985325 0.043740888263985325
full_emb_for_3_emb_size_1 initialized from: -0.04147509477069983 0.04147509477069983
full_emb_for_4_emb_size_1 initialized from: -0.4898979485566356 0.4898979485566356
full_emb_for_5_emb_size_1 initialized from: -0.038715317938997504 0.038715317938997504
full_emb_for_6_emb_size_1 initialized from: -0.15399810070180361 0.15399810070180361
full_emb_for_7_emb_size_1 initialized from: -0.454858826147342 0.454858826147342
full_emb_for_8_emb_size_1 initialized from: -0.007690411867832244 0.007690411867832244
full_emb_for_9_emb_size_1 initialized from: -0.003384897591352657 0.003384897591352657
full_emb_for_10_emb_size_1 initialized from: -0.031819606281476856 0.031819606281476856
full_emb_for_11_emb_size_1 initialized from: -1.0 1.0
full_emb_for_12_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_13_emb_size_1 initialized from: -0.04981354813867179 0.04981354813867179
full_emb_for_14_emb_size_1 initialized from: -0.816496580927726 0.816496580927726
full_emb_for_15_emb_size_1 initialized from: -0.7745966692414834 0.7745966692414834
full_emb_for_16_emb_size_1 initialized from: -0.11853911695403994 0.11853911695403994
full_emb_for_17_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_18_emb_size_1 initialized from: -0.2970442628930023 0.2970442628930023
full_emb_for_19_emb_size_1 initialized from: -0.18954720708196904 0.18954720708196904
full_emb_for_20_emb_size_1 initialized from: -0.31362502409359 0.31362502409359
full_emb_for_21_emb_size_1 initialized from: -0.7385489458759964 0.7385489458759964
full_emb_for_22_emb_size_1 initialized from: -0.4898979485566356 0.4898979485566356
full_emb_for_23_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
w_0 initialized from: -0.0854357657716761 0.0854357657716761
(122, 700) (700,)
relu
w_1 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_2 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_3 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_4 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_5 initialized from: -0.0925159507394634 0.0925159507394634
(700, 1) (1,)
none
[[34m2023-05-13 14:03:15[0m] Experiment directory created at /home/ubuntu/results/retrain_irazor/avazu/001-retrain_irazor
[[34m2023-05-13 14:03:15[0m] Batchsize: 256
wandb: Currently logged in as: yao-yao. Use `wandb login --relogin` to force relogin
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /tmp/wandb/run-20230513_140316-gtew4j0b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avazu-BS-256-001-retrain_irazor-2023-05-13 14:03:15
wandb: ⭐️ View project at https://wandb.ai/yao-yao/irazor
wandb: 🚀 View run at https://wandb.ai/yao-yao/irazor/runs/gtew4j0b
2023-05-13 14:03:21.153631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:21.155221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:21.156068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:22.884227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:22.885267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:22.886072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:22.887003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 36121 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
[[34m2023-05-13 14:03:22[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
add l2 0.001 Tensor("concat:0", shape=(?, 122), dtype=float32)
add l2 0.001 Tensor("concat_1:0", shape=(?, 24), dtype=float32)
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
[[34m2023-05-13 14:03:23[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2023-05-13 14:03:23.981734: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
[[34m2023-05-13 14:03:24[0m] total batches: 126350	batch per epoch: 12635
[[34m2023-05-13 14:03:24[0m] new iteration
new iteration
on disk...
2023-05-13 14:03:25.896742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
[[34m2023-05-13 14:03:41[0m] elapsed : 0:00:16, ETA : 0:33:25
[[34m2023-05-13 14:03:41[0m] epoch 1 / 10, batch 1000 / 12635, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.439753, l2 = 0.182615, auc = 0.665691
elapsed : 0:00:16, ETA : 0:33:25
epoch 1 / 10, batch 1000 / 12635, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.439753, l2 = 0.182615, auc = 0.665691
[[34m2023-05-13 14:04:03[0m] elapsed : 0:00:39, ETA : 0:40:24
[[34m2023-05-13 14:04:03[0m] epoch 1 / 10, batch 2000 / 12635, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.403289, l2 = 0.049450, auc = 0.737119
elapsed : 0:00:39, ETA : 0:40:24
epoch 1 / 10, batch 2000 / 12635, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.403289, l2 = 0.049450, auc = 0.737119
[[34m2023-05-13 14:04:38[0m] elapsed : 0:01:13, ETA : 0:50:01
[[34m2023-05-13 14:04:38[0m] epoch 1 / 10, batch 3000 / 12635, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.398678, l2 = 0.022059, auc = 0.745105
elapsed : 0:01:13, ETA : 0:50:01
epoch 1 / 10, batch 3000 / 12635, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.398678, l2 = 0.022059, auc = 0.745105
[[34m2023-05-13 14:05:12[0m] elapsed : 0:01:48, ETA : 0:55:03
[[34m2023-05-13 14:05:12[0m] epoch 1 / 10, batch 4000 / 12635, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.395752, l2 = 0.012196, auc = 0.749865
elapsed : 0:01:48, ETA : 0:55:03
epoch 1 / 10, batch 4000 / 12635, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.395752, l2 = 0.012196, auc = 0.749865
[[34m2023-05-13 14:05:47[0m] elapsed : 0:02:23, ETA : 0:57:50
[[34m2023-05-13 14:05:47[0m] epoch 1 / 10, batch 5000 / 12635, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.395245, l2 = 0.007998, auc = 0.749922
elapsed : 0:02:23, ETA : 0:57:50
epoch 1 / 10, batch 5000 / 12635, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.395245, l2 = 0.007998, auc = 0.749922
[[34m2023-05-13 14:06:21[0m] elapsed : 0:02:57, ETA : 0:59:10
[[34m2023-05-13 14:06:21[0m] epoch 1 / 10, batch 6000 / 12635, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.394784, l2 = 0.005976, auc = 0.752494
elapsed : 0:02:57, ETA : 0:59:10
epoch 1 / 10, batch 6000 / 12635, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.394784, l2 = 0.005976, auc = 0.752494
[[34m2023-05-13 14:06:56[0m] elapsed : 0:03:31, ETA : 0:59:57
[[34m2023-05-13 14:06:56[0m] epoch 1 / 10, batch 7000 / 12635, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.393944, l2 = 0.004856, auc = 0.752705
elapsed : 0:03:31, ETA : 0:59:57
epoch 1 / 10, batch 7000 / 12635, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.393944, l2 = 0.004856, auc = 0.752705
[[34m2023-05-13 14:07:31[0m] elapsed : 0:04:06, ETA : 1:00:39
[[34m2023-05-13 14:07:31[0m] epoch 1 / 10, batch 8000 / 12635, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.393495, l2 = 0.004262, auc = 0.755756
elapsed : 0:04:06, ETA : 1:00:39
epoch 1 / 10, batch 8000 / 12635, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.393495, l2 = 0.004262, auc = 0.755756
[[34m2023-05-13 14:08:05[0m] elapsed : 0:04:41, ETA : 1:01:03
[[34m2023-05-13 14:08:05[0m] epoch 1 / 10, batch 9000 / 12635, global_step = 9000, learning_rate = 1.000000e-02, loss = 0.391993, l2 = 0.003892, auc = 0.757056
elapsed : 0:04:41, ETA : 1:01:03
epoch 1 / 10, batch 9000 / 12635, global_step = 9000, learning_rate = 1.000000e-02, loss = 0.391993, l2 = 0.003892, auc = 0.757056
[[34m2023-05-13 14:08:40[0m] elapsed : 0:05:15, ETA : 1:01:05
[[34m2023-05-13 14:08:40[0m] epoch 1 / 10, batch 10000 / 12635, global_step = 10000, learning_rate = 1.000000e-02, loss = 0.390377, l2 = 0.003670, auc = 0.758006
elapsed : 0:05:15, ETA : 1:01:05
epoch 1 / 10, batch 10000 / 12635, global_step = 10000, learning_rate = 1.000000e-02, loss = 0.390377, l2 = 0.003670, auc = 0.758006
[[34m2023-05-13 14:09:14[0m] elapsed : 0:05:50, ETA : 1:01:10
[[34m2023-05-13 14:09:14[0m] epoch 1 / 10, batch 11000 / 12635, global_step = 11000, learning_rate = 1.000000e-02, loss = 0.390789, l2 = 0.003467, auc = 0.758369
elapsed : 0:05:50, ETA : 1:01:10
epoch 1 / 10, batch 11000 / 12635, global_step = 11000, learning_rate = 1.000000e-02, loss = 0.390789, l2 = 0.003467, auc = 0.758369
[[34m2023-05-13 14:09:48[0m] elapsed : 0:06:24, ETA : 1:00:59
[[34m2023-05-13 14:09:48[0m] epoch 1 / 10, batch 12000 / 12635, global_step = 12000, learning_rate = 1.000000e-02, loss = 0.389236, l2 = 0.003352, auc = 0.761636
elapsed : 0:06:24, ETA : 1:00:59
epoch 1 / 10, batch 12000 / 12635, global_step = 12000, learning_rate = 1.000000e-02, loss = 0.389236, l2 = 0.003352, auc = 0.761636
[[34m2023-05-13 14:10:10[0m] running test...
on disk...
[[34m2023-05-13 14:10:15[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 14:10:19[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:10:23[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 14:10:27[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:10:31[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 14:10:35[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:10:40[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 14:10:44[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:10:48[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 14:10:52[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:10:56[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 14:11:01[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 14:11:05[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-13 14:11:09[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-13 14:11:13[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-13 14:11:17[0m] evaluated batches: 16000, 0:00:04
[[34m2023-05-13 14:11:21[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-13 14:11:25[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-13 14:11:30[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 14:11:34[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-13 14:11:38[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 14:11:42[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 14:11:46[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 14:11:50[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-13 14:11:55[0m] evaluated batches: 25000, 0:00:04
[[34m2023-05-13 14:11:59[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-13 14:12:03[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 14:12:07[0m] evaluated batches: 28000, 0:00:04
[[34m2023-05-13 14:12:11[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 14:12:15[0m] evaluated batches: 30000, 0:00:03
[[34m2023-05-13 14:12:19[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 14:12:26[0m] test loss = 0.390981, test auc = 0.761168
[[34m2023-05-13 14:12:26[0m] evaluated time: 0:02:15
[[34m2023-05-13 14:12:26[0m] analyse_structure
[[34m2023-05-13 14:13:01[0m] elapsed : 0:09:37, ETA : 1:19:29
[[34m2023-05-13 14:13:01[0m] epoch 2 / 10, batch 1000 / 12635, global_step = 13635, learning_rate = 1.000000e-02, loss = 0.639639, l2 = 0.005317, auc = 0.760608
elapsed : 0:09:37, ETA : 1:19:29
epoch 2 / 10, batch 1000 / 12635, global_step = 13635, learning_rate = 1.000000e-02, loss = 0.639639, l2 = 0.005317, auc = 0.760608
[[34m2023-05-13 14:13:36[0m] elapsed : 0:10:11, ETA : 1:17:44
[[34m2023-05-13 14:13:36[0m] epoch 2 / 10, batch 2000 / 12635, global_step = 14635, learning_rate = 1.000000e-02, loss = 0.387766, l2 = 0.003175, auc = 0.765121
elapsed : 0:10:11, ETA : 1:17:44
epoch 2 / 10, batch 2000 / 12635, global_step = 14635, learning_rate = 1.000000e-02, loss = 0.387766, l2 = 0.003175, auc = 0.765121
[[34m2023-05-13 14:14:09[0m] elapsed : 0:10:45, ETA : 1:16:07
[[34m2023-05-13 14:14:09[0m] epoch 2 / 10, batch 3000 / 12635, global_step = 15635, learning_rate = 1.000000e-02, loss = 0.388000, l2 = 0.003130, auc = 0.763320
elapsed : 0:10:45, ETA : 1:16:07
epoch 2 / 10, batch 3000 / 12635, global_step = 15635, learning_rate = 1.000000e-02, loss = 0.388000, l2 = 0.003130, auc = 0.763320
[[34m2023-05-13 14:14:44[0m] elapsed : 0:11:19, ETA : 1:14:38
[[34m2023-05-13 14:14:44[0m] epoch 2 / 10, batch 4000 / 12635, global_step = 16635, learning_rate = 1.000000e-02, loss = 0.388074, l2 = 0.003075, auc = 0.763757
elapsed : 0:11:19, ETA : 1:14:38
epoch 2 / 10, batch 4000 / 12635, global_step = 16635, learning_rate = 1.000000e-02, loss = 0.388074, l2 = 0.003075, auc = 0.763757
[[34m2023-05-13 14:15:18[0m] elapsed : 0:11:54, ETA : 1:13:21
[[34m2023-05-13 14:15:18[0m] epoch 2 / 10, batch 5000 / 12635, global_step = 17635, learning_rate = 1.000000e-02, loss = 0.388992, l2 = 0.003037, auc = 0.762789
elapsed : 0:11:54, ETA : 1:13:21
epoch 2 / 10, batch 5000 / 12635, global_step = 17635, learning_rate = 1.000000e-02, loss = 0.388992, l2 = 0.003037, auc = 0.762789
[[34m2023-05-13 14:15:52[0m] elapsed : 0:12:28, ETA : 1:12:03
[[34m2023-05-13 14:15:52[0m] epoch 2 / 10, batch 6000 / 12635, global_step = 18635, learning_rate = 1.000000e-02, loss = 0.389516, l2 = 0.003006, auc = 0.763765
elapsed : 0:12:28, ETA : 1:12:03
epoch 2 / 10, batch 6000 / 12635, global_step = 18635, learning_rate = 1.000000e-02, loss = 0.389516, l2 = 0.003006, auc = 0.763765
[[34m2023-05-13 14:16:26[0m] elapsed : 0:13:02, ETA : 1:10:50
[[34m2023-05-13 14:16:26[0m] epoch 2 / 10, batch 7000 / 12635, global_step = 19635, learning_rate = 1.000000e-02, loss = 0.387897, l2 = 0.002974, auc = 0.765715
elapsed : 0:13:02, ETA : 1:10:50
epoch 2 / 10, batch 7000 / 12635, global_step = 19635, learning_rate = 1.000000e-02, loss = 0.387897, l2 = 0.002974, auc = 0.765715
[[34m2023-05-13 14:17:01[0m] elapsed : 0:13:36, ETA : 1:09:40
[[34m2023-05-13 14:17:01[0m] epoch 2 / 10, batch 8000 / 12635, global_step = 20635, learning_rate = 1.000000e-02, loss = 0.389644, l2 = 0.002936, auc = 0.764400
elapsed : 0:13:36, ETA : 1:09:40
epoch 2 / 10, batch 8000 / 12635, global_step = 20635, learning_rate = 1.000000e-02, loss = 0.389644, l2 = 0.002936, auc = 0.764400
[[34m2023-05-13 14:17:35[0m] elapsed : 0:14:11, ETA : 1:08:38
[[34m2023-05-13 14:17:35[0m] epoch 2 / 10, batch 9000 / 12635, global_step = 21635, learning_rate = 1.000000e-02, loss = 0.387672, l2 = 0.002906, auc = 0.765749
elapsed : 0:14:11, ETA : 1:08:38
epoch 2 / 10, batch 9000 / 12635, global_step = 21635, learning_rate = 1.000000e-02, loss = 0.387672, l2 = 0.002906, auc = 0.765749
[[34m2023-05-13 14:18:10[0m] elapsed : 0:14:45, ETA : 1:07:35
[[34m2023-05-13 14:18:10[0m] epoch 2 / 10, batch 10000 / 12635, global_step = 22635, learning_rate = 1.000000e-02, loss = 0.384913, l2 = 0.002883, auc = 0.768054
elapsed : 0:14:45, ETA : 1:07:35
epoch 2 / 10, batch 10000 / 12635, global_step = 22635, learning_rate = 1.000000e-02, loss = 0.384913, l2 = 0.002883, auc = 0.768054
[[34m2023-05-13 14:18:45[0m] elapsed : 0:15:20, ETA : 1:06:38
[[34m2023-05-13 14:18:45[0m] epoch 2 / 10, batch 11000 / 12635, global_step = 23635, learning_rate = 1.000000e-02, loss = 0.386349, l2 = 0.002856, auc = 0.766741
elapsed : 0:15:20, ETA : 1:06:38
epoch 2 / 10, batch 11000 / 12635, global_step = 23635, learning_rate = 1.000000e-02, loss = 0.386349, l2 = 0.002856, auc = 0.766741
[[34m2023-05-13 14:19:19[0m] elapsed : 0:15:55, ETA : 1:05:43
[[34m2023-05-13 14:19:19[0m] epoch 2 / 10, batch 12000 / 12635, global_step = 24635, learning_rate = 1.000000e-02, loss = 0.384452, l2 = 0.002840, auc = 0.767753
elapsed : 0:15:55, ETA : 1:05:43
epoch 2 / 10, batch 12000 / 12635, global_step = 24635, learning_rate = 1.000000e-02, loss = 0.384452, l2 = 0.002840, auc = 0.767753
[[34m2023-05-13 14:19:41[0m] running test...
on disk...
[[34m2023-05-13 14:19:46[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 14:19:50[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:19:54[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 14:19:58[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:20:02[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 14:20:06[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:20:10[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 14:20:15[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:20:19[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 14:20:23[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:20:27[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 14:20:31[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 14:20:35[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-13 14:20:39[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-13 14:20:44[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-13 14:20:48[0m] evaluated batches: 16000, 0:00:04
[[34m2023-05-13 14:20:52[0m] evaluated batches: 17000, 0:00:03
[[34m2023-05-13 14:20:55[0m] evaluated batches: 18000, 0:00:03
[[34m2023-05-13 14:20:59[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 14:21:04[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-13 14:21:08[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 14:21:12[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 14:21:16[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 14:21:20[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-13 14:21:24[0m] evaluated batches: 25000, 0:00:04
[[34m2023-05-13 14:21:29[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-13 14:21:33[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 14:21:37[0m] evaluated batches: 28000, 0:00:03
[[34m2023-05-13 14:21:41[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 14:21:45[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-13 14:21:49[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 14:21:56[0m] test loss = 0.387478, test auc = 0.767557
[[34m2023-05-13 14:21:56[0m] evaluated time: 0:02:14
[[34m2023-05-13 14:21:56[0m] analyse_structure
[[34m2023-05-13 14:22:32[0m] elapsed : 0:19:07, ETA : 1:12:49
[[34m2023-05-13 14:22:32[0m] epoch 3 / 10, batch 1000 / 12635, global_step = 26270, learning_rate = 1.000000e-02, loss = 0.633553, l2 = 0.004618, auc = 0.765667
elapsed : 0:19:07, ETA : 1:12:49
epoch 3 / 10, batch 1000 / 12635, global_step = 26270, learning_rate = 1.000000e-02, loss = 0.633553, l2 = 0.004618, auc = 0.765667
[[34m2023-05-13 14:23:07[0m] elapsed : 0:19:43, ETA : 1:11:38
[[34m2023-05-13 14:23:07[0m] epoch 3 / 10, batch 2000 / 12635, global_step = 27270, learning_rate = 1.000000e-02, loss = 0.388063, l2 = 0.002788, auc = 0.768253
elapsed : 0:19:43, ETA : 1:11:38
epoch 3 / 10, batch 2000 / 12635, global_step = 27270, learning_rate = 1.000000e-02, loss = 0.388063, l2 = 0.002788, auc = 0.768253
[[34m2023-05-13 14:23:41[0m] elapsed : 0:20:17, ETA : 1:10:22
[[34m2023-05-13 14:23:41[0m] epoch 3 / 10, batch 3000 / 12635, global_step = 28270, learning_rate = 1.000000e-02, loss = 0.386621, l2 = 0.002777, auc = 0.767913
elapsed : 0:20:17, ETA : 1:10:22
epoch 3 / 10, batch 3000 / 12635, global_step = 28270, learning_rate = 1.000000e-02, loss = 0.386621, l2 = 0.002777, auc = 0.767913
[[34m2023-05-13 14:24:14[0m] elapsed : 0:20:49, ETA : 1:09:02
[[34m2023-05-13 14:24:14[0m] epoch 3 / 10, batch 4000 / 12635, global_step = 29270, learning_rate = 1.000000e-02, loss = 0.385318, l2 = 0.002758, auc = 0.767588
elapsed : 0:20:49, ETA : 1:09:02
epoch 3 / 10, batch 4000 / 12635, global_step = 29270, learning_rate = 1.000000e-02, loss = 0.385318, l2 = 0.002758, auc = 0.767588
[[34m2023-05-13 14:24:47[0m] elapsed : 0:21:23, ETA : 1:07:52
[[34m2023-05-13 14:24:47[0m] epoch 3 / 10, batch 5000 / 12635, global_step = 30270, learning_rate = 1.000000e-02, loss = 0.387571, l2 = 0.002751, auc = 0.767291
elapsed : 0:21:23, ETA : 1:07:52
epoch 3 / 10, batch 5000 / 12635, global_step = 30270, learning_rate = 1.000000e-02, loss = 0.387571, l2 = 0.002751, auc = 0.767291
[[34m2023-05-13 14:25:21[0m] elapsed : 0:21:57, ETA : 1:06:44
[[34m2023-05-13 14:25:21[0m] epoch 3 / 10, batch 6000 / 12635, global_step = 31270, learning_rate = 1.000000e-02, loss = 0.385781, l2 = 0.002737, auc = 0.769963
elapsed : 0:21:57, ETA : 1:06:44
epoch 3 / 10, batch 6000 / 12635, global_step = 31270, learning_rate = 1.000000e-02, loss = 0.385781, l2 = 0.002737, auc = 0.769963
[[34m2023-05-13 14:25:56[0m] elapsed : 0:22:31, ETA : 1:05:38
[[34m2023-05-13 14:25:56[0m] epoch 3 / 10, batch 7000 / 12635, global_step = 32270, learning_rate = 1.000000e-02, loss = 0.387129, l2 = 0.002714, auc = 0.767779
elapsed : 0:22:31, ETA : 1:05:38
epoch 3 / 10, batch 7000 / 12635, global_step = 32270, learning_rate = 1.000000e-02, loss = 0.387129, l2 = 0.002714, auc = 0.767779
[[34m2023-05-13 14:26:30[0m] elapsed : 0:23:05, ETA : 1:04:34
[[34m2023-05-13 14:26:30[0m] epoch 3 / 10, batch 8000 / 12635, global_step = 33270, learning_rate = 1.000000e-02, loss = 0.386045, l2 = 0.002695, auc = 0.766848
elapsed : 0:23:05, ETA : 1:04:34
epoch 3 / 10, batch 8000 / 12635, global_step = 33270, learning_rate = 1.000000e-02, loss = 0.386045, l2 = 0.002695, auc = 0.766848
[[34m2023-05-13 14:27:04[0m] elapsed : 0:23:40, ETA : 1:03:35
[[34m2023-05-13 14:27:04[0m] epoch 3 / 10, batch 9000 / 12635, global_step = 34270, learning_rate = 1.000000e-02, loss = 0.384619, l2 = 0.002694, auc = 0.771518
elapsed : 0:23:40, ETA : 1:03:35
epoch 3 / 10, batch 9000 / 12635, global_step = 34270, learning_rate = 1.000000e-02, loss = 0.384619, l2 = 0.002694, auc = 0.771518
[[34m2023-05-13 14:27:39[0m] elapsed : 0:24:14, ETA : 1:02:34
[[34m2023-05-13 14:27:39[0m] epoch 3 / 10, batch 10000 / 12635, global_step = 35270, learning_rate = 1.000000e-02, loss = 0.385716, l2 = 0.002676, auc = 0.769320
elapsed : 0:24:14, ETA : 1:02:34
epoch 3 / 10, batch 10000 / 12635, global_step = 35270, learning_rate = 1.000000e-02, loss = 0.385716, l2 = 0.002676, auc = 0.769320
[[34m2023-05-13 14:28:13[0m] elapsed : 0:24:49, ETA : 1:01:38
[[34m2023-05-13 14:28:13[0m] epoch 3 / 10, batch 11000 / 12635, global_step = 36270, learning_rate = 1.000000e-02, loss = 0.384077, l2 = 0.002669, auc = 0.771471
elapsed : 0:24:49, ETA : 1:01:38
epoch 3 / 10, batch 11000 / 12635, global_step = 36270, learning_rate = 1.000000e-02, loss = 0.384077, l2 = 0.002669, auc = 0.771471
[[34m2023-05-13 14:28:47[0m] elapsed : 0:25:23, ETA : 1:00:40
[[34m2023-05-13 14:28:47[0m] epoch 3 / 10, batch 12000 / 12635, global_step = 37270, learning_rate = 1.000000e-02, loss = 0.383935, l2 = 0.002648, auc = 0.771653
elapsed : 0:25:23, ETA : 1:00:40
epoch 3 / 10, batch 12000 / 12635, global_step = 37270, learning_rate = 1.000000e-02, loss = 0.383935, l2 = 0.002648, auc = 0.771653
[[34m2023-05-13 14:29:09[0m] running test...
on disk...
[[34m2023-05-13 14:29:13[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 14:29:17[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:29:22[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 14:29:26[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:29:30[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 14:29:34[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:29:38[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 14:29:42[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:29:47[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 14:29:51[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:29:55[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 14:29:59[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 14:30:03[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-13 14:30:07[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-13 14:30:11[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-13 14:30:16[0m] evaluated batches: 16000, 0:00:04
[[34m2023-05-13 14:30:20[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-13 14:30:24[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-13 14:30:28[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 14:30:32[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-13 14:30:36[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 14:30:41[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 14:30:45[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 14:30:49[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-13 14:30:53[0m] evaluated batches: 25000, 0:00:04
[[34m2023-05-13 14:30:57[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-13 14:31:01[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 14:31:06[0m] evaluated batches: 28000, 0:00:04
[[34m2023-05-13 14:31:10[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 14:31:14[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-13 14:31:18[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 14:31:25[0m] test loss = 0.385061, test auc = 0.771281
[[34m2023-05-13 14:31:25[0m] evaluated time: 0:02:15
[[34m2023-05-13 14:31:25[0m] analyse_structure
[[34m2023-05-13 14:32:00[0m] elapsed : 0:28:35, ETA : 1:04:14
[[34m2023-05-13 14:32:00[0m] epoch 4 / 10, batch 1000 / 12635, global_step = 38905, learning_rate = 1.000000e-02, loss = 0.630809, l2 = 0.004326, auc = 0.770170
elapsed : 0:28:35, ETA : 1:04:14
epoch 4 / 10, batch 1000 / 12635, global_step = 38905, learning_rate = 1.000000e-02, loss = 0.630809, l2 = 0.004326, auc = 0.770170
[[34m2023-05-13 14:32:35[0m] elapsed : 0:29:10, ETA : 1:03:10
[[34m2023-05-13 14:32:35[0m] epoch 4 / 10, batch 2000 / 12635, global_step = 39905, learning_rate = 1.000000e-02, loss = 0.384907, l2 = 0.002634, auc = 0.772099
elapsed : 0:29:10, ETA : 1:03:10
epoch 4 / 10, batch 2000 / 12635, global_step = 39905, learning_rate = 1.000000e-02, loss = 0.384907, l2 = 0.002634, auc = 0.772099
[[34m2023-05-13 14:33:09[0m] elapsed : 0:29:45, ETA : 1:02:08
[[34m2023-05-13 14:33:09[0m] epoch 4 / 10, batch 3000 / 12635, global_step = 40905, learning_rate = 1.000000e-02, loss = 0.384241, l2 = 0.002630, auc = 0.770359
elapsed : 0:29:45, ETA : 1:02:08
epoch 4 / 10, batch 3000 / 12635, global_step = 40905, learning_rate = 1.000000e-02, loss = 0.384241, l2 = 0.002630, auc = 0.770359
[[34m2023-05-13 14:33:44[0m] elapsed : 0:30:19, ETA : 1:01:05
[[34m2023-05-13 14:33:44[0m] epoch 4 / 10, batch 4000 / 12635, global_step = 41905, learning_rate = 1.000000e-02, loss = 0.385463, l2 = 0.002625, auc = 0.770619
elapsed : 0:30:19, ETA : 1:01:05
epoch 4 / 10, batch 4000 / 12635, global_step = 41905, learning_rate = 1.000000e-02, loss = 0.385463, l2 = 0.002625, auc = 0.770619
[[34m2023-05-13 14:34:19[0m] elapsed : 0:30:55, ETA : 1:00:07
[[34m2023-05-13 14:34:19[0m] epoch 4 / 10, batch 5000 / 12635, global_step = 42905, learning_rate = 1.000000e-02, loss = 0.383567, l2 = 0.002623, auc = 0.771537
elapsed : 0:30:55, ETA : 1:00:07
epoch 4 / 10, batch 5000 / 12635, global_step = 42905, learning_rate = 1.000000e-02, loss = 0.383567, l2 = 0.002623, auc = 0.771537
[[34m2023-05-13 14:34:51[0m] elapsed : 0:31:27, ETA : 0:59:03
[[34m2023-05-13 14:34:51[0m] epoch 4 / 10, batch 6000 / 12635, global_step = 43905, learning_rate = 1.000000e-02, loss = 0.382972, l2 = 0.002605, auc = 0.773406
elapsed : 0:31:27, ETA : 0:59:03
epoch 4 / 10, batch 6000 / 12635, global_step = 43905, learning_rate = 1.000000e-02, loss = 0.382972, l2 = 0.002605, auc = 0.773406
[[34m2023-05-13 14:35:26[0m] elapsed : 0:32:02, ETA : 0:58:05
[[34m2023-05-13 14:35:26[0m] epoch 4 / 10, batch 7000 / 12635, global_step = 44905, learning_rate = 1.000000e-02, loss = 0.384026, l2 = 0.002594, auc = 0.771304
elapsed : 0:32:02, ETA : 0:58:05
epoch 4 / 10, batch 7000 / 12635, global_step = 44905, learning_rate = 1.000000e-02, loss = 0.384026, l2 = 0.002594, auc = 0.771304
[[34m2023-05-13 14:36:01[0m] elapsed : 0:32:36, ETA : 0:57:07
[[34m2023-05-13 14:36:01[0m] epoch 4 / 10, batch 8000 / 12635, global_step = 45905, learning_rate = 1.000000e-02, loss = 0.382647, l2 = 0.002591, auc = 0.771529
elapsed : 0:32:36, ETA : 0:57:07
epoch 4 / 10, batch 8000 / 12635, global_step = 45905, learning_rate = 1.000000e-02, loss = 0.382647, l2 = 0.002591, auc = 0.771529
[[34m2023-05-13 14:36:36[0m] elapsed : 0:33:11, ETA : 0:56:12
[[34m2023-05-13 14:36:36[0m] epoch 4 / 10, batch 9000 / 12635, global_step = 46905, learning_rate = 1.000000e-02, loss = 0.383693, l2 = 0.002582, auc = 0.772718
elapsed : 0:33:11, ETA : 0:56:12
epoch 4 / 10, batch 9000 / 12635, global_step = 46905, learning_rate = 1.000000e-02, loss = 0.383693, l2 = 0.002582, auc = 0.772718
[[34m2023-05-13 14:37:10[0m] elapsed : 0:33:46, ETA : 0:55:17
[[34m2023-05-13 14:37:10[0m] epoch 4 / 10, batch 10000 / 12635, global_step = 47905, learning_rate = 1.000000e-02, loss = 0.381391, l2 = 0.002588, auc = 0.772982
elapsed : 0:33:46, ETA : 0:55:17
epoch 4 / 10, batch 10000 / 12635, global_step = 47905, learning_rate = 1.000000e-02, loss = 0.381391, l2 = 0.002588, auc = 0.772982
[[34m2023-05-13 14:37:45[0m] elapsed : 0:34:21, ETA : 0:54:23
[[34m2023-05-13 14:37:45[0m] epoch 4 / 10, batch 11000 / 12635, global_step = 48905, learning_rate = 1.000000e-02, loss = 0.384018, l2 = 0.002579, auc = 0.773046
elapsed : 0:34:21, ETA : 0:54:23
epoch 4 / 10, batch 11000 / 12635, global_step = 48905, learning_rate = 1.000000e-02, loss = 0.384018, l2 = 0.002579, auc = 0.773046
[[34m2023-05-13 14:38:20[0m] elapsed : 0:34:56, ETA : 0:53:30
[[34m2023-05-13 14:38:20[0m] epoch 4 / 10, batch 12000 / 12635, global_step = 49905, learning_rate = 1.000000e-02, loss = 0.383702, l2 = 0.002568, auc = 0.774421
elapsed : 0:34:56, ETA : 0:53:30
epoch 4 / 10, batch 12000 / 12635, global_step = 49905, learning_rate = 1.000000e-02, loss = 0.383702, l2 = 0.002568, auc = 0.774421
[[34m2023-05-13 14:38:42[0m] running test...
on disk...
[[34m2023-05-13 14:38:46[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 14:38:51[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:38:55[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 14:38:59[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:39:03[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 14:39:07[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:39:11[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 14:39:16[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:39:20[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 14:39:24[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:39:28[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 14:39:32[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 14:39:36[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-13 14:39:40[0m] evaluated batches: 14000, 0:00:03
[[34m2023-05-13 14:39:43[0m] evaluated batches: 15000, 0:00:03
[[34m2023-05-13 14:39:48[0m] evaluated batches: 16000, 0:00:04
[[34m2023-05-13 14:39:52[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-13 14:39:56[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-13 14:40:00[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 14:40:04[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-13 14:40:09[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 14:40:13[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 14:40:17[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 14:40:21[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-13 14:40:25[0m] evaluated batches: 25000, 0:00:04
[[34m2023-05-13 14:40:29[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-13 14:40:33[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 14:40:37[0m] evaluated batches: 28000, 0:00:04
[[34m2023-05-13 14:40:42[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 14:40:46[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-13 14:40:50[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 14:40:57[0m] test loss = 0.383032, test auc = 0.774106
[[34m2023-05-13 14:40:57[0m] evaluated time: 0:02:14
[[34m2023-05-13 14:40:57[0m] analyse_structure
[[34m2023-05-13 14:41:31[0m] elapsed : 0:38:07, ETA : 0:55:19
[[34m2023-05-13 14:41:31[0m] epoch 5 / 10, batch 1000 / 12635, global_step = 51540, learning_rate = 1.000000e-02, loss = 0.625859, l2 = 0.004184, auc = 0.773659
elapsed : 0:38:07, ETA : 0:55:19
epoch 5 / 10, batch 1000 / 12635, global_step = 51540, learning_rate = 1.000000e-02, loss = 0.625859, l2 = 0.004184, auc = 0.773659
[[34m2023-05-13 14:42:07[0m] elapsed : 0:38:42, ETA : 0:54:22
[[34m2023-05-13 14:42:07[0m] epoch 5 / 10, batch 2000 / 12635, global_step = 52540, learning_rate = 1.000000e-02, loss = 0.382683, l2 = 0.002549, auc = 0.775433
elapsed : 0:38:42, ETA : 0:54:22
epoch 5 / 10, batch 2000 / 12635, global_step = 52540, learning_rate = 1.000000e-02, loss = 0.382683, l2 = 0.002549, auc = 0.775433
[[34m2023-05-13 14:42:42[0m] elapsed : 0:39:17, ETA : 0:53:25
[[34m2023-05-13 14:42:42[0m] epoch 5 / 10, batch 3000 / 12635, global_step = 53540, learning_rate = 1.000000e-02, loss = 0.382613, l2 = 0.002553, auc = 0.774186
elapsed : 0:39:17, ETA : 0:53:25
epoch 5 / 10, batch 3000 / 12635, global_step = 53540, learning_rate = 1.000000e-02, loss = 0.382613, l2 = 0.002553, auc = 0.774186
[[34m2023-05-13 14:43:17[0m] elapsed : 0:39:53, ETA : 0:52:30
[[34m2023-05-13 14:43:17[0m] epoch 5 / 10, batch 4000 / 12635, global_step = 54540, learning_rate = 1.000000e-02, loss = 0.382601, l2 = 0.002547, auc = 0.772777
elapsed : 0:39:53, ETA : 0:52:30
epoch 5 / 10, batch 4000 / 12635, global_step = 54540, learning_rate = 1.000000e-02, loss = 0.382601, l2 = 0.002547, auc = 0.772777
[[34m2023-05-13 14:43:50[0m] elapsed : 0:40:25, ETA : 0:51:31
[[34m2023-05-13 14:43:50[0m] epoch 5 / 10, batch 5000 / 12635, global_step = 55540, learning_rate = 1.000000e-02, loss = 0.384656, l2 = 0.002541, auc = 0.772785
elapsed : 0:40:25, ETA : 0:51:31
epoch 5 / 10, batch 5000 / 12635, global_step = 55540, learning_rate = 1.000000e-02, loss = 0.384656, l2 = 0.002541, auc = 0.772785
[[34m2023-05-13 14:44:25[0m] elapsed : 0:41:00, ETA : 0:50:37
[[34m2023-05-13 14:44:25[0m] epoch 5 / 10, batch 6000 / 12635, global_step = 56540, learning_rate = 1.000000e-02, loss = 0.382356, l2 = 0.002526, auc = 0.774788
elapsed : 0:41:00, ETA : 0:50:37
epoch 5 / 10, batch 6000 / 12635, global_step = 56540, learning_rate = 1.000000e-02, loss = 0.382356, l2 = 0.002526, auc = 0.774788
[[34m2023-05-13 14:44:57[0m] elapsed : 0:41:33, ETA : 0:49:41
[[34m2023-05-13 14:44:57[0m] epoch 5 / 10, batch 7000 / 12635, global_step = 57540, learning_rate = 1.000000e-02, loss = 0.381531, l2 = 0.002514, auc = 0.774675
elapsed : 0:41:33, ETA : 0:49:41
epoch 5 / 10, batch 7000 / 12635, global_step = 57540, learning_rate = 1.000000e-02, loss = 0.381531, l2 = 0.002514, auc = 0.774675
[[34m2023-05-13 14:45:31[0m] elapsed : 0:42:07, ETA : 0:48:47
[[34m2023-05-13 14:45:31[0m] epoch 5 / 10, batch 8000 / 12635, global_step = 58540, learning_rate = 1.000000e-02, loss = 0.382894, l2 = 0.002506, auc = 0.773827
elapsed : 0:42:07, ETA : 0:48:47
epoch 5 / 10, batch 8000 / 12635, global_step = 58540, learning_rate = 1.000000e-02, loss = 0.382894, l2 = 0.002506, auc = 0.773827
[[34m2023-05-13 14:46:05[0m] elapsed : 0:42:41, ETA : 0:47:53
[[34m2023-05-13 14:46:05[0m] epoch 5 / 10, batch 9000 / 12635, global_step = 59540, learning_rate = 1.000000e-02, loss = 0.383740, l2 = 0.002510, auc = 0.774478
elapsed : 0:42:41, ETA : 0:47:53
epoch 5 / 10, batch 9000 / 12635, global_step = 59540, learning_rate = 1.000000e-02, loss = 0.383740, l2 = 0.002510, auc = 0.774478
[[34m2023-05-13 14:46:40[0m] elapsed : 0:43:16, ETA : 0:47:01
[[34m2023-05-13 14:46:40[0m] epoch 5 / 10, batch 10000 / 12635, global_step = 60540, learning_rate = 1.000000e-02, loss = 0.382041, l2 = 0.002494, auc = 0.775044
elapsed : 0:43:16, ETA : 0:47:01
epoch 5 / 10, batch 10000 / 12635, global_step = 60540, learning_rate = 1.000000e-02, loss = 0.382041, l2 = 0.002494, auc = 0.775044
[[34m2023-05-13 14:47:14[0m] elapsed : 0:43:50, ETA : 0:46:09
[[34m2023-05-13 14:47:14[0m] epoch 5 / 10, batch 11000 / 12635, global_step = 61540, learning_rate = 1.000000e-02, loss = 0.382837, l2 = 0.002491, auc = 0.775222
elapsed : 0:43:50, ETA : 0:46:09
epoch 5 / 10, batch 11000 / 12635, global_step = 61540, learning_rate = 1.000000e-02, loss = 0.382837, l2 = 0.002491, auc = 0.775222
[[34m2023-05-13 14:47:49[0m] elapsed : 0:44:25, ETA : 0:45:19
[[34m2023-05-13 14:47:49[0m] epoch 5 / 10, batch 12000 / 12635, global_step = 62540, learning_rate = 1.000000e-02, loss = 0.383336, l2 = 0.002492, auc = 0.776210
elapsed : 0:44:25, ETA : 0:45:19
epoch 5 / 10, batch 12000 / 12635, global_step = 62540, learning_rate = 1.000000e-02, loss = 0.383336, l2 = 0.002492, auc = 0.776210
[[34m2023-05-13 14:48:11[0m] running test...
on disk...
[[34m2023-05-13 14:48:16[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 14:48:20[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:48:24[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 14:48:28[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:48:32[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 14:48:36[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:48:40[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 14:48:45[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:48:49[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 14:48:53[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:48:57[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 14:49:01[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 14:49:05[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-13 14:49:09[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-13 14:49:13[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-13 14:49:18[0m] evaluated batches: 16000, 0:00:04
[[34m2023-05-13 14:49:22[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-13 14:49:26[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-13 14:49:30[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 14:49:34[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-13 14:49:38[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 14:49:43[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 14:49:47[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 14:49:51[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-13 14:49:55[0m] evaluated batches: 25000, 0:00:04
[[34m2023-05-13 14:49:59[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-13 14:50:03[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 14:50:08[0m] evaluated batches: 28000, 0:00:04
[[34m2023-05-13 14:50:12[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 14:50:16[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-13 14:50:20[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 14:50:27[0m] test loss = 0.381900, test auc = 0.775991
[[34m2023-05-13 14:50:27[0m] evaluated time: 0:02:15
[[34m2023-05-13 14:50:27[0m] analyse_structure
[[34m2023-05-13 14:51:01[0m] elapsed : 0:47:37, ETA : 0:46:07
[[34m2023-05-13 14:51:01[0m] epoch 6 / 10, batch 1000 / 12635, global_step = 64175, learning_rate = 1.000000e-02, loss = 0.624293, l2 = 0.004058, auc = 0.775771
elapsed : 0:47:37, ETA : 0:46:07
epoch 6 / 10, batch 1000 / 12635, global_step = 64175, learning_rate = 1.000000e-02, loss = 0.624293, l2 = 0.004058, auc = 0.775771
[[34m2023-05-13 14:51:36[0m] elapsed : 0:48:12, ETA : 0:45:14
[[34m2023-05-13 14:51:36[0m] epoch 6 / 10, batch 2000 / 12635, global_step = 65175, learning_rate = 1.000000e-02, loss = 0.382109, l2 = 0.002476, auc = 0.775558
elapsed : 0:48:12, ETA : 0:45:14
epoch 6 / 10, batch 2000 / 12635, global_step = 65175, learning_rate = 1.000000e-02, loss = 0.382109, l2 = 0.002476, auc = 0.775558
[[34m2023-05-13 14:52:11[0m] elapsed : 0:48:46, ETA : 0:44:20
[[34m2023-05-13 14:52:11[0m] epoch 6 / 10, batch 3000 / 12635, global_step = 66175, learning_rate = 1.000000e-02, loss = 0.381296, l2 = 0.002478, auc = 0.776614
elapsed : 0:48:46, ETA : 0:44:20
epoch 6 / 10, batch 3000 / 12635, global_step = 66175, learning_rate = 1.000000e-02, loss = 0.381296, l2 = 0.002478, auc = 0.776614
[[34m2023-05-13 14:52:45[0m] elapsed : 0:49:21, ETA : 0:43:28
[[34m2023-05-13 14:52:45[0m] epoch 6 / 10, batch 4000 / 12635, global_step = 67175, learning_rate = 1.000000e-02, loss = 0.384582, l2 = 0.002468, auc = 0.773901
elapsed : 0:49:21, ETA : 0:43:28
epoch 6 / 10, batch 4000 / 12635, global_step = 67175, learning_rate = 1.000000e-02, loss = 0.384582, l2 = 0.002468, auc = 0.773901
[[34m2023-05-13 14:53:19[0m] elapsed : 0:49:55, ETA : 0:42:35
[[34m2023-05-13 14:53:19[0m] epoch 6 / 10, batch 5000 / 12635, global_step = 68175, learning_rate = 1.000000e-02, loss = 0.379672, l2 = 0.002468, auc = 0.778650
elapsed : 0:49:55, ETA : 0:42:35
epoch 6 / 10, batch 5000 / 12635, global_step = 68175, learning_rate = 1.000000e-02, loss = 0.379672, l2 = 0.002468, auc = 0.778650
[[34m2023-05-13 14:53:54[0m] elapsed : 0:50:30, ETA : 0:41:44
[[34m2023-05-13 14:53:54[0m] epoch 6 / 10, batch 6000 / 12635, global_step = 69175, learning_rate = 1.000000e-02, loss = 0.380022, l2 = 0.002457, auc = 0.778232
elapsed : 0:50:30, ETA : 0:41:44
epoch 6 / 10, batch 6000 / 12635, global_step = 69175, learning_rate = 1.000000e-02, loss = 0.380022, l2 = 0.002457, auc = 0.778232
[[34m2023-05-13 14:54:29[0m] elapsed : 0:51:05, ETA : 0:40:53
[[34m2023-05-13 14:54:29[0m] epoch 6 / 10, batch 7000 / 12635, global_step = 70175, learning_rate = 1.000000e-02, loss = 0.381173, l2 = 0.002463, auc = 0.775373
elapsed : 0:51:05, ETA : 0:40:53
epoch 6 / 10, batch 7000 / 12635, global_step = 70175, learning_rate = 1.000000e-02, loss = 0.381173, l2 = 0.002463, auc = 0.775373
[[34m2023-05-13 14:55:01[0m] elapsed : 0:51:37, ETA : 0:40:00
[[34m2023-05-13 14:55:01[0m] epoch 6 / 10, batch 8000 / 12635, global_step = 71175, learning_rate = 1.000000e-02, loss = 0.378655, l2 = 0.002461, auc = 0.777698
elapsed : 0:51:37, ETA : 0:40:00
epoch 6 / 10, batch 8000 / 12635, global_step = 71175, learning_rate = 1.000000e-02, loss = 0.378655, l2 = 0.002461, auc = 0.777698
[[34m2023-05-13 14:55:36[0m] elapsed : 0:52:12, ETA : 0:39:10
[[34m2023-05-13 14:55:36[0m] epoch 6 / 10, batch 9000 / 12635, global_step = 72175, learning_rate = 1.000000e-02, loss = 0.382512, l2 = 0.002455, auc = 0.774858
elapsed : 0:52:12, ETA : 0:39:10
epoch 6 / 10, batch 9000 / 12635, global_step = 72175, learning_rate = 1.000000e-02, loss = 0.382512, l2 = 0.002455, auc = 0.774858
[[34m2023-05-13 14:56:11[0m] elapsed : 0:52:47, ETA : 0:38:21
[[34m2023-05-13 14:56:11[0m] epoch 6 / 10, batch 10000 / 12635, global_step = 73175, learning_rate = 1.000000e-02, loss = 0.380536, l2 = 0.002459, auc = 0.776836
elapsed : 0:52:47, ETA : 0:38:21
epoch 6 / 10, batch 10000 / 12635, global_step = 73175, learning_rate = 1.000000e-02, loss = 0.380536, l2 = 0.002459, auc = 0.776836
[[34m2023-05-13 14:56:46[0m] elapsed : 0:53:21, ETA : 0:37:31
[[34m2023-05-13 14:56:46[0m] epoch 6 / 10, batch 11000 / 12635, global_step = 74175, learning_rate = 1.000000e-02, loss = 0.382242, l2 = 0.002441, auc = 0.776503
elapsed : 0:53:21, ETA : 0:37:31
epoch 6 / 10, batch 11000 / 12635, global_step = 74175, learning_rate = 1.000000e-02, loss = 0.382242, l2 = 0.002441, auc = 0.776503
[[34m2023-05-13 14:57:20[0m] elapsed : 0:53:56, ETA : 0:36:42
[[34m2023-05-13 14:57:20[0m] epoch 6 / 10, batch 12000 / 12635, global_step = 75175, learning_rate = 1.000000e-02, loss = 0.381422, l2 = 0.002434, auc = 0.776963
elapsed : 0:53:56, ETA : 0:36:42
epoch 6 / 10, batch 12000 / 12635, global_step = 75175, learning_rate = 1.000000e-02, loss = 0.381422, l2 = 0.002434, auc = 0.776963
[[34m2023-05-13 14:57:42[0m] running test...
on disk...
[[34m2023-05-13 14:57:46[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 14:57:50[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:57:54[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 14:57:58[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:58:03[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 14:58:07[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:58:11[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 14:58:15[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:58:19[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 14:58:23[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:58:27[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 14:58:31[0m] evaluated batches: 12000, 0:00:03
[[34m2023-05-13 14:58:35[0m] evaluated batches: 13000, 0:00:03
[[34m2023-05-13 14:58:39[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-13 14:58:43[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-13 14:58:47[0m] evaluated batches: 16000, 0:00:04
[[34m2023-05-13 14:58:52[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-13 14:58:56[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-13 14:59:00[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 14:59:04[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-13 14:59:08[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 14:59:12[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 14:59:16[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 14:59:21[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-13 14:59:25[0m] evaluated batches: 25000, 0:00:04
[[34m2023-05-13 14:59:29[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-13 14:59:33[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 14:59:37[0m] evaluated batches: 28000, 0:00:04
[[34m2023-05-13 14:59:41[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 14:59:45[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-13 14:59:49[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 14:59:56[0m] test loss = 0.381245, test auc = 0.777478
[[34m2023-05-13 14:59:56[0m] evaluated time: 0:02:14
[[34m2023-05-13 14:59:56[0m] analyse_structure
[[34m2023-05-13 15:00:31[0m] elapsed : 0:57:06, ETA : 0:36:49
[[34m2023-05-13 15:00:31[0m] epoch 7 / 10, batch 1000 / 12635, global_step = 76810, learning_rate = 1.000000e-02, loss = 0.622982, l2 = 0.003989, auc = 0.777796
elapsed : 0:57:06, ETA : 0:36:49
epoch 7 / 10, batch 1000 / 12635, global_step = 76810, learning_rate = 1.000000e-02, loss = 0.622982, l2 = 0.003989, auc = 0.777796
[[34m2023-05-13 15:01:05[0m] elapsed : 0:57:41, ETA : 0:35:59
[[34m2023-05-13 15:01:05[0m] epoch 7 / 10, batch 2000 / 12635, global_step = 77810, learning_rate = 1.000000e-02, loss = 0.380644, l2 = 0.002446, auc = 0.777296
elapsed : 0:57:41, ETA : 0:35:59
epoch 7 / 10, batch 2000 / 12635, global_step = 77810, learning_rate = 1.000000e-02, loss = 0.380644, l2 = 0.002446, auc = 0.777296
[[34m2023-05-13 15:01:40[0m] elapsed : 0:58:16, ETA : 0:35:08
[[34m2023-05-13 15:01:40[0m] epoch 7 / 10, batch 3000 / 12635, global_step = 78810, learning_rate = 1.000000e-02, loss = 0.379754, l2 = 0.002450, auc = 0.776510
elapsed : 0:58:16, ETA : 0:35:08
epoch 7 / 10, batch 3000 / 12635, global_step = 78810, learning_rate = 1.000000e-02, loss = 0.379754, l2 = 0.002450, auc = 0.776510
[[34m2023-05-13 15:02:15[0m] elapsed : 0:58:51, ETA : 0:34:19
[[34m2023-05-13 15:02:15[0m] epoch 7 / 10, batch 4000 / 12635, global_step = 79810, learning_rate = 1.000000e-02, loss = 0.382107, l2 = 0.002439, auc = 0.777586
elapsed : 0:58:51, ETA : 0:34:19
epoch 7 / 10, batch 4000 / 12635, global_step = 79810, learning_rate = 1.000000e-02, loss = 0.382107, l2 = 0.002439, auc = 0.777586
[[34m2023-05-13 15:02:51[0m] elapsed : 0:59:27, ETA : 0:33:30
[[34m2023-05-13 15:02:51[0m] epoch 7 / 10, batch 5000 / 12635, global_step = 80810, learning_rate = 1.000000e-02, loss = 0.379339, l2 = 0.002434, auc = 0.778708
elapsed : 0:59:27, ETA : 0:33:30
epoch 7 / 10, batch 5000 / 12635, global_step = 80810, learning_rate = 1.000000e-02, loss = 0.379339, l2 = 0.002434, auc = 0.778708
[[34m2023-05-13 15:03:26[0m] elapsed : 1:00:02, ETA : 0:32:41
[[34m2023-05-13 15:03:26[0m] epoch 7 / 10, batch 6000 / 12635, global_step = 81810, learning_rate = 1.000000e-02, loss = 0.379960, l2 = 0.002437, auc = 0.778666
elapsed : 1:00:02, ETA : 0:32:41
epoch 7 / 10, batch 6000 / 12635, global_step = 81810, learning_rate = 1.000000e-02, loss = 0.379960, l2 = 0.002437, auc = 0.778666
[[34m2023-05-13 15:03:58[0m] elapsed : 1:00:34, ETA : 0:31:50
[[34m2023-05-13 15:03:58[0m] epoch 7 / 10, batch 7000 / 12635, global_step = 82810, learning_rate = 1.000000e-02, loss = 0.380533, l2 = 0.002430, auc = 0.778040
elapsed : 1:00:34, ETA : 0:31:50
epoch 7 / 10, batch 7000 / 12635, global_step = 82810, learning_rate = 1.000000e-02, loss = 0.380533, l2 = 0.002430, auc = 0.778040
[[34m2023-05-13 15:04:33[0m] elapsed : 1:01:09, ETA : 0:31:02
[[34m2023-05-13 15:04:33[0m] epoch 7 / 10, batch 8000 / 12635, global_step = 83810, learning_rate = 1.000000e-02, loss = 0.381338, l2 = 0.002426, auc = 0.776967
elapsed : 1:01:09, ETA : 0:31:02
epoch 7 / 10, batch 8000 / 12635, global_step = 83810, learning_rate = 1.000000e-02, loss = 0.381338, l2 = 0.002426, auc = 0.776967
[[34m2023-05-13 15:05:06[0m] elapsed : 1:01:42, ETA : 0:30:13
[[34m2023-05-13 15:05:06[0m] epoch 7 / 10, batch 9000 / 12635, global_step = 84810, learning_rate = 1.000000e-02, loss = 0.380684, l2 = 0.002420, auc = 0.777453
elapsed : 1:01:42, ETA : 0:30:13
epoch 7 / 10, batch 9000 / 12635, global_step = 84810, learning_rate = 1.000000e-02, loss = 0.380684, l2 = 0.002420, auc = 0.777453
[[34m2023-05-13 15:05:41[0m] elapsed : 1:02:16, ETA : 0:29:25
[[34m2023-05-13 15:05:41[0m] epoch 7 / 10, batch 10000 / 12635, global_step = 85810, learning_rate = 1.000000e-02, loss = 0.380245, l2 = 0.002416, auc = 0.777805
elapsed : 1:02:16, ETA : 0:29:25
epoch 7 / 10, batch 10000 / 12635, global_step = 85810, learning_rate = 1.000000e-02, loss = 0.380245, l2 = 0.002416, auc = 0.777805
[[34m2023-05-13 15:06:15[0m] elapsed : 1:02:51, ETA : 0:28:37
[[34m2023-05-13 15:06:15[0m] epoch 7 / 10, batch 11000 / 12635, global_step = 86810, learning_rate = 1.000000e-02, loss = 0.381342, l2 = 0.002403, auc = 0.778857
elapsed : 1:02:51, ETA : 0:28:37
epoch 7 / 10, batch 11000 / 12635, global_step = 86810, learning_rate = 1.000000e-02, loss = 0.381342, l2 = 0.002403, auc = 0.778857
[[34m2023-05-13 15:06:50[0m] elapsed : 1:03:25, ETA : 0:27:50
[[34m2023-05-13 15:06:50[0m] epoch 7 / 10, batch 12000 / 12635, global_step = 87810, learning_rate = 1.000000e-02, loss = 0.378984, l2 = 0.002411, auc = 0.777395
elapsed : 1:03:25, ETA : 0:27:50
epoch 7 / 10, batch 12000 / 12635, global_step = 87810, learning_rate = 1.000000e-02, loss = 0.378984, l2 = 0.002411, auc = 0.777395
[[34m2023-05-13 15:07:11[0m] running test...
on disk...
[[34m2023-05-13 15:07:16[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 15:07:20[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 15:07:24[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 15:07:28[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 15:07:32[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 15:07:36[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 15:07:40[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 15:07:45[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 15:07:49[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 15:07:53[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 15:07:57[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 15:08:01[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 15:08:05[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-13 15:08:09[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-13 15:08:13[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-13 15:08:18[0m] evaluated batches: 16000, 0:00:04
[[34m2023-05-13 15:08:22[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-13 15:08:26[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-13 15:08:30[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 15:08:34[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-13 15:08:38[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 15:08:43[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 15:08:47[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 15:08:51[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-13 15:08:55[0m] evaluated batches: 25000, 0:00:04
[[34m2023-05-13 15:08:59[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-13 15:09:03[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 15:09:07[0m] evaluated batches: 28000, 0:00:04
[[34m2023-05-13 15:09:11[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 15:09:16[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-13 15:09:20[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 15:09:27[0m] test loss = 0.380248, test auc = 0.778847
[[34m2023-05-13 15:09:27[0m] evaluated time: 0:02:15
[[34m2023-05-13 15:09:27[0m] analyse_structure
[[34m2023-05-13 15:10:01[0m] elapsed : 1:06:37, ETA : 0:27:29
[[34m2023-05-13 15:10:01[0m] epoch 8 / 10, batch 1000 / 12635, global_step = 89445, learning_rate = 1.000000e-02, loss = 0.622269, l2 = 0.003942, auc = 0.778581
elapsed : 1:06:37, ETA : 0:27:29
epoch 8 / 10, batch 1000 / 12635, global_step = 89445, learning_rate = 1.000000e-02, loss = 0.622269, l2 = 0.003942, auc = 0.778581
[[34m2023-05-13 15:10:36[0m] elapsed : 1:07:11, ETA : 0:26:40
[[34m2023-05-13 15:10:36[0m] epoch 8 / 10, batch 2000 / 12635, global_step = 90445, learning_rate = 1.000000e-02, loss = 0.377029, l2 = 0.002409, auc = 0.779335
elapsed : 1:07:11, ETA : 0:26:40
epoch 8 / 10, batch 2000 / 12635, global_step = 90445, learning_rate = 1.000000e-02, loss = 0.377029, l2 = 0.002409, auc = 0.779335
[[34m2023-05-13 15:11:10[0m] elapsed : 1:07:46, ETA : 0:25:52
[[34m2023-05-13 15:11:10[0m] epoch 8 / 10, batch 3000 / 12635, global_step = 91445, learning_rate = 1.000000e-02, loss = 0.380261, l2 = 0.002404, auc = 0.778714
elapsed : 1:07:46, ETA : 0:25:52
epoch 8 / 10, batch 3000 / 12635, global_step = 91445, learning_rate = 1.000000e-02, loss = 0.380261, l2 = 0.002404, auc = 0.778714
[[34m2023-05-13 15:11:44[0m] elapsed : 1:08:20, ETA : 0:25:03
[[34m2023-05-13 15:11:44[0m] epoch 8 / 10, batch 4000 / 12635, global_step = 92445, learning_rate = 1.000000e-02, loss = 0.378437, l2 = 0.002406, auc = 0.779977
elapsed : 1:08:20, ETA : 0:25:03
epoch 8 / 10, batch 4000 / 12635, global_step = 92445, learning_rate = 1.000000e-02, loss = 0.378437, l2 = 0.002406, auc = 0.779977
[[34m2023-05-13 15:12:18[0m] elapsed : 1:08:54, ETA : 0:24:15
[[34m2023-05-13 15:12:18[0m] epoch 8 / 10, batch 5000 / 12635, global_step = 93445, learning_rate = 1.000000e-02, loss = 0.381436, l2 = 0.002402, auc = 0.779249
elapsed : 1:08:54, ETA : 0:24:15
epoch 8 / 10, batch 5000 / 12635, global_step = 93445, learning_rate = 1.000000e-02, loss = 0.381436, l2 = 0.002402, auc = 0.779249
[[34m2023-05-13 15:12:54[0m] elapsed : 1:09:29, ETA : 0:23:28
[[34m2023-05-13 15:12:54[0m] epoch 8 / 10, batch 6000 / 12635, global_step = 94445, learning_rate = 1.000000e-02, loss = 0.380307, l2 = 0.002388, auc = 0.779634
elapsed : 1:09:29, ETA : 0:23:28
epoch 8 / 10, batch 6000 / 12635, global_step = 94445, learning_rate = 1.000000e-02, loss = 0.380307, l2 = 0.002388, auc = 0.779634
[[34m2023-05-13 15:13:28[0m] elapsed : 1:10:04, ETA : 0:22:41
[[34m2023-05-13 15:13:28[0m] epoch 8 / 10, batch 7000 / 12635, global_step = 95445, learning_rate = 1.000000e-02, loss = 0.380900, l2 = 0.002392, auc = 0.778133
elapsed : 1:10:04, ETA : 0:22:41
epoch 8 / 10, batch 7000 / 12635, global_step = 95445, learning_rate = 1.000000e-02, loss = 0.380900, l2 = 0.002392, auc = 0.778133
[[34m2023-05-13 15:14:03[0m] elapsed : 1:10:38, ETA : 0:21:54
[[34m2023-05-13 15:14:03[0m] epoch 8 / 10, batch 8000 / 12635, global_step = 96445, learning_rate = 1.000000e-02, loss = 0.379239, l2 = 0.002389, auc = 0.778650
elapsed : 1:10:38, ETA : 0:21:54
epoch 8 / 10, batch 8000 / 12635, global_step = 96445, learning_rate = 1.000000e-02, loss = 0.379239, l2 = 0.002389, auc = 0.778650
[[34m2023-05-13 15:14:37[0m] elapsed : 1:11:13, ETA : 0:21:07
[[34m2023-05-13 15:14:37[0m] epoch 8 / 10, batch 9000 / 12635, global_step = 97445, learning_rate = 1.000000e-02, loss = 0.379949, l2 = 0.002389, auc = 0.779891
elapsed : 1:11:13, ETA : 0:21:07
epoch 8 / 10, batch 9000 / 12635, global_step = 97445, learning_rate = 1.000000e-02, loss = 0.379949, l2 = 0.002389, auc = 0.779891
[[34m2023-05-13 15:15:09[0m] elapsed : 1:11:45, ETA : 0:20:20
[[34m2023-05-13 15:15:09[0m] epoch 8 / 10, batch 10000 / 12635, global_step = 98445, learning_rate = 1.000000e-02, loss = 0.379426, l2 = 0.002386, auc = 0.779334
elapsed : 1:11:45, ETA : 0:20:20
epoch 8 / 10, batch 10000 / 12635, global_step = 98445, learning_rate = 1.000000e-02, loss = 0.379426, l2 = 0.002386, auc = 0.779334
[[34m2023-05-13 15:15:44[0m] elapsed : 1:12:20, ETA : 0:19:34
[[34m2023-05-13 15:15:44[0m] epoch 8 / 10, batch 11000 / 12635, global_step = 99445, learning_rate = 1.000000e-02, loss = 0.379445, l2 = 0.002381, auc = 0.778806
elapsed : 1:12:20, ETA : 0:19:34
epoch 8 / 10, batch 11000 / 12635, global_step = 99445, learning_rate = 1.000000e-02, loss = 0.379445, l2 = 0.002381, auc = 0.778806
[[34m2023-05-13 15:16:18[0m] elapsed : 1:12:54, ETA : 0:18:48
[[34m2023-05-13 15:16:18[0m] epoch 8 / 10, batch 12000 / 12635, global_step = 100445, learning_rate = 1.000000e-02, loss = 0.382541, l2 = 0.002378, auc = 0.777829
elapsed : 1:12:54, ETA : 0:18:48
epoch 8 / 10, batch 12000 / 12635, global_step = 100445, learning_rate = 1.000000e-02, loss = 0.382541, l2 = 0.002378, auc = 0.777829
[[34m2023-05-13 15:16:40[0m] running test...
on disk...
[[34m2023-05-13 15:16:44[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 15:16:49[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 15:16:53[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 15:16:57[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 15:17:01[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 15:17:05[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 15:17:09[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 15:17:13[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 15:17:17[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 15:17:21[0m] evaluated batches: 10000, 0:00:03
[[34m2023-05-13 15:17:25[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 15:17:29[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 15:17:33[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-13 15:17:37[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-13 15:17:42[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-13 15:17:46[0m] evaluated batches: 16000, 0:00:04
[[34m2023-05-13 15:17:50[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-13 15:17:54[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-13 15:17:58[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 15:18:02[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-13 15:18:07[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 15:18:11[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 15:18:15[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 15:18:19[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-13 15:18:23[0m] evaluated batches: 25000, 0:00:04
[[34m2023-05-13 15:18:28[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-13 15:18:32[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 15:18:36[0m] evaluated batches: 28000, 0:00:04
[[34m2023-05-13 15:18:40[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 15:18:44[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-13 15:18:48[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 15:18:55[0m] test loss = 0.379552, test auc = 0.780008
[[34m2023-05-13 15:18:55[0m] evaluated time: 0:02:14
[[34m2023-05-13 15:18:55[0m] analyse_structure
[[34m2023-05-13 15:19:30[0m] elapsed : 1:16:05, ETA : 0:18:05
[[34m2023-05-13 15:19:30[0m] epoch 9 / 10, batch 1000 / 12635, global_step = 102080, learning_rate = 1.000000e-02, loss = 0.621469, l2 = 0.003881, auc = 0.779440
elapsed : 1:16:05, ETA : 0:18:05
epoch 9 / 10, batch 1000 / 12635, global_step = 102080, learning_rate = 1.000000e-02, loss = 0.621469, l2 = 0.003881, auc = 0.779440
[[34m2023-05-13 15:20:04[0m] elapsed : 1:16:40, ETA : 0:17:18
[[34m2023-05-13 15:20:04[0m] epoch 9 / 10, batch 2000 / 12635, global_step = 103080, learning_rate = 1.000000e-02, loss = 0.380561, l2 = 0.002372, auc = 0.778892
elapsed : 1:16:40, ETA : 0:17:18
epoch 9 / 10, batch 2000 / 12635, global_step = 103080, learning_rate = 1.000000e-02, loss = 0.380561, l2 = 0.002372, auc = 0.778892
[[34m2023-05-13 15:20:38[0m] elapsed : 1:17:14, ETA : 0:16:31
[[34m2023-05-13 15:20:38[0m] epoch 9 / 10, batch 3000 / 12635, global_step = 104080, learning_rate = 1.000000e-02, loss = 0.378478, l2 = 0.002365, auc = 0.779240
elapsed : 1:17:14, ETA : 0:16:31
epoch 9 / 10, batch 3000 / 12635, global_step = 104080, learning_rate = 1.000000e-02, loss = 0.378478, l2 = 0.002365, auc = 0.779240
[[34m2023-05-13 15:21:13[0m] elapsed : 1:17:48, ETA : 0:15:44
[[34m2023-05-13 15:21:13[0m] epoch 9 / 10, batch 4000 / 12635, global_step = 105080, learning_rate = 1.000000e-02, loss = 0.378308, l2 = 0.002364, auc = 0.781187
elapsed : 1:17:48, ETA : 0:15:44
epoch 9 / 10, batch 4000 / 12635, global_step = 105080, learning_rate = 1.000000e-02, loss = 0.378308, l2 = 0.002364, auc = 0.781187
[[34m2023-05-13 15:21:48[0m] elapsed : 1:18:24, ETA : 0:14:58
[[34m2023-05-13 15:21:48[0m] epoch 9 / 10, batch 5000 / 12635, global_step = 106080, learning_rate = 1.000000e-02, loss = 0.379936, l2 = 0.002364, auc = 0.779893
elapsed : 1:18:24, ETA : 0:14:58
epoch 9 / 10, batch 5000 / 12635, global_step = 106080, learning_rate = 1.000000e-02, loss = 0.379936, l2 = 0.002364, auc = 0.779893
[[34m2023-05-13 15:22:23[0m] elapsed : 1:18:59, ETA : 0:14:12
[[34m2023-05-13 15:22:23[0m] epoch 9 / 10, batch 6000 / 12635, global_step = 107080, learning_rate = 1.000000e-02, loss = 0.378524, l2 = 0.002357, auc = 0.779756
elapsed : 1:18:59, ETA : 0:14:12
epoch 9 / 10, batch 6000 / 12635, global_step = 107080, learning_rate = 1.000000e-02, loss = 0.378524, l2 = 0.002357, auc = 0.779756
[[34m2023-05-13 15:22:58[0m] elapsed : 1:19:34, ETA : 0:13:27
[[34m2023-05-13 15:22:58[0m] epoch 9 / 10, batch 7000 / 12635, global_step = 108080, learning_rate = 1.000000e-02, loss = 0.379640, l2 = 0.002354, auc = 0.779965
elapsed : 1:19:34, ETA : 0:13:27
epoch 9 / 10, batch 7000 / 12635, global_step = 108080, learning_rate = 1.000000e-02, loss = 0.379640, l2 = 0.002354, auc = 0.779965
[[34m2023-05-13 15:23:33[0m] elapsed : 1:20:08, ETA : 0:12:41
[[34m2023-05-13 15:23:33[0m] epoch 9 / 10, batch 8000 / 12635, global_step = 109080, learning_rate = 1.000000e-02, loss = 0.376039, l2 = 0.002351, auc = 0.781361
elapsed : 1:20:08, ETA : 0:12:41
epoch 9 / 10, batch 8000 / 12635, global_step = 109080, learning_rate = 1.000000e-02, loss = 0.376039, l2 = 0.002351, auc = 0.781361
[[34m2023-05-13 15:24:06[0m] elapsed : 1:20:42, ETA : 0:11:55
[[34m2023-05-13 15:24:06[0m] epoch 9 / 10, batch 9000 / 12635, global_step = 110080, learning_rate = 1.000000e-02, loss = 0.381022, l2 = 0.002356, auc = 0.778564
elapsed : 1:20:42, ETA : 0:11:55
epoch 9 / 10, batch 9000 / 12635, global_step = 110080, learning_rate = 1.000000e-02, loss = 0.381022, l2 = 0.002356, auc = 0.778564
[[34m2023-05-13 15:24:41[0m] elapsed : 1:21:17, ETA : 0:11:10
[[34m2023-05-13 15:24:41[0m] epoch 9 / 10, batch 10000 / 12635, global_step = 111080, learning_rate = 1.000000e-02, loss = 0.379208, l2 = 0.002356, auc = 0.780302
elapsed : 1:21:17, ETA : 0:11:10
epoch 9 / 10, batch 10000 / 12635, global_step = 111080, learning_rate = 1.000000e-02, loss = 0.379208, l2 = 0.002356, auc = 0.780302
[[34m2023-05-13 15:25:15[0m] elapsed : 1:21:50, ETA : 0:10:25
[[34m2023-05-13 15:25:15[0m] epoch 9 / 10, batch 11000 / 12635, global_step = 112080, learning_rate = 1.000000e-02, loss = 0.379280, l2 = 0.002348, auc = 0.779703
elapsed : 1:21:50, ETA : 0:10:25
epoch 9 / 10, batch 11000 / 12635, global_step = 112080, learning_rate = 1.000000e-02, loss = 0.379280, l2 = 0.002348, auc = 0.779703
[[34m2023-05-13 15:25:48[0m] elapsed : 1:22:24, ETA : 0:09:40
[[34m2023-05-13 15:25:48[0m] epoch 9 / 10, batch 12000 / 12635, global_step = 113080, learning_rate = 1.000000e-02, loss = 0.379662, l2 = 0.002344, auc = 0.779725
elapsed : 1:22:24, ETA : 0:09:40
epoch 9 / 10, batch 12000 / 12635, global_step = 113080, learning_rate = 1.000000e-02, loss = 0.379662, l2 = 0.002344, auc = 0.779725
[[34m2023-05-13 15:26:10[0m] running test...
on disk...
[[34m2023-05-13 15:26:15[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 15:26:19[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 15:26:23[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 15:26:27[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 15:26:31[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 15:26:35[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 15:26:39[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 15:26:44[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 15:26:48[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 15:26:52[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 15:26:56[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 15:27:00[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 15:27:04[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-13 15:27:09[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-13 15:27:13[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-13 15:27:17[0m] evaluated batches: 16000, 0:00:04
[[34m2023-05-13 15:27:21[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-13 15:27:25[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-13 15:27:29[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 15:27:34[0m] evaluated batches: 20000, 0:00:04
[[34m2023-05-13 15:27:38[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 15:27:42[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 15:27:46[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 15:27:50[0m] evaluated batches: 24000, 0:00:04
[[34m2023-05-13 15:27:54[0m] evaluated batches: 25000, 0:00:03
[[34m2023-05-13 15:27:58[0m] evaluated batches: 26000, 0:00:04
[[34m2023-05-13 15:28:03[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 15:28:07[0m] evaluated batches: 28000, 0:00:04
[[34m2023-05-13 15:28:11[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 15:28:15[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-13 15:28:19[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 15:28:26[0m] test loss = 0.379100, test auc = 0.781065
[[34m2023-05-13 15:28:26[0m] evaluated time: 0:02:15
[[34m2023-05-13 15:28:26[0m] analyse_structure
[[34m2023-05-13 15:29:01[0m] elapsed : 1:25:37, ETA : 0:08:41
[[34m2023-05-13 15:29:01[0m] epoch 10 / 10, batch 1000 / 12635, global_step = 114715, learning_rate = 1.000000e-02, loss = 0.618500, l2 = 0.003846, auc = 0.781917
elapsed : 1:25:37, ETA : 0:08:41
epoch 10 / 10, batch 1000 / 12635, global_step = 114715, learning_rate = 1.000000e-02, loss = 0.618500, l2 = 0.003846, auc = 0.781917
[[34m2023-05-13 15:29:35[0m] elapsed : 1:26:11, ETA : 0:07:55
[[34m2023-05-13 15:29:35[0m] epoch 10 / 10, batch 2000 / 12635, global_step = 115715, learning_rate = 1.000000e-02, loss = 0.377230, l2 = 0.002348, auc = 0.782211
elapsed : 1:26:11, ETA : 0:07:55
epoch 10 / 10, batch 2000 / 12635, global_step = 115715, learning_rate = 1.000000e-02, loss = 0.377230, l2 = 0.002348, auc = 0.782211
[[34m2023-05-13 15:30:09[0m] elapsed : 1:26:45, ETA : 0:07:09
[[34m2023-05-13 15:30:09[0m] epoch 10 / 10, batch 3000 / 12635, global_step = 116715, learning_rate = 1.000000e-02, loss = 0.379333, l2 = 0.002347, auc = 0.781012
elapsed : 1:26:45, ETA : 0:07:09
epoch 10 / 10, batch 3000 / 12635, global_step = 116715, learning_rate = 1.000000e-02, loss = 0.379333, l2 = 0.002347, auc = 0.781012
[[34m2023-05-13 15:30:50[0m] elapsed : 1:27:26, ETA : 0:06:24
[[34m2023-05-13 15:30:50[0m] epoch 10 / 10, batch 4000 / 12635, global_step = 117715, learning_rate = 1.000000e-02, loss = 0.380592, l2 = 0.002339, auc = 0.780003
elapsed : 1:27:26, ETA : 0:06:24
epoch 10 / 10, batch 4000 / 12635, global_step = 117715, learning_rate = 1.000000e-02, loss = 0.380592, l2 = 0.002339, auc = 0.780003
[[34m2023-05-13 15:31:35[0m] elapsed : 1:28:11, ETA : 0:05:40
[[34m2023-05-13 15:31:35[0m] epoch 10 / 10, batch 5000 / 12635, global_step = 118715, learning_rate = 1.000000e-02, loss = 0.379829, l2 = 0.002343, auc = 0.781572
elapsed : 1:28:11, ETA : 0:05:40
epoch 10 / 10, batch 5000 / 12635, global_step = 118715, learning_rate = 1.000000e-02, loss = 0.379829, l2 = 0.002343, auc = 0.781572
[[34m2023-05-13 15:32:20[0m] elapsed : 1:28:56, ETA : 0:04:55
[[34m2023-05-13 15:32:20[0m] epoch 10 / 10, batch 6000 / 12635, global_step = 119715, learning_rate = 1.000000e-02, loss = 0.380514, l2 = 0.002340, auc = 0.781254
elapsed : 1:28:56, ETA : 0:04:55
epoch 10 / 10, batch 6000 / 12635, global_step = 119715, learning_rate = 1.000000e-02, loss = 0.380514, l2 = 0.002340, auc = 0.781254
[[34m2023-05-13 15:33:06[0m] elapsed : 1:29:42, ETA : 0:04:11
[[34m2023-05-13 15:33:06[0m] epoch 10 / 10, batch 7000 / 12635, global_step = 120715, learning_rate = 1.000000e-02, loss = 0.380293, l2 = 0.002343, auc = 0.780018
elapsed : 1:29:42, ETA : 0:04:11
epoch 10 / 10, batch 7000 / 12635, global_step = 120715, learning_rate = 1.000000e-02, loss = 0.380293, l2 = 0.002343, auc = 0.780018
[[34m2023-05-13 15:33:52[0m] elapsed : 1:30:28, ETA : 0:03:26
[[34m2023-05-13 15:33:52[0m] epoch 10 / 10, batch 8000 / 12635, global_step = 121715, learning_rate = 1.000000e-02, loss = 0.379863, l2 = 0.002339, auc = 0.779667
elapsed : 1:30:28, ETA : 0:03:26
epoch 10 / 10, batch 8000 / 12635, global_step = 121715, learning_rate = 1.000000e-02, loss = 0.379863, l2 = 0.002339, auc = 0.779667
[[34m2023-05-13 15:34:38[0m] elapsed : 1:31:14, ETA : 0:02:42
[[34m2023-05-13 15:34:38[0m] epoch 10 / 10, batch 9000 / 12635, global_step = 122715, learning_rate = 1.000000e-02, loss = 0.378274, l2 = 0.002332, auc = 0.780754
elapsed : 1:31:14, ETA : 0:02:42
epoch 10 / 10, batch 9000 / 12635, global_step = 122715, learning_rate = 1.000000e-02, loss = 0.378274, l2 = 0.002332, auc = 0.780754
[[34m2023-05-13 15:35:24[0m] elapsed : 1:32:00, ETA : 0:01:57
[[34m2023-05-13 15:35:24[0m] epoch 10 / 10, batch 10000 / 12635, global_step = 123715, learning_rate = 1.000000e-02, loss = 0.378952, l2 = 0.002341, auc = 0.780594
elapsed : 1:32:00, ETA : 0:01:57
epoch 10 / 10, batch 10000 / 12635, global_step = 123715, learning_rate = 1.000000e-02, loss = 0.378952, l2 = 0.002341, auc = 0.780594
[[34m2023-05-13 15:36:09[0m] elapsed : 1:32:45, ETA : 0:01:12
[[34m2023-05-13 15:36:09[0m] epoch 10 / 10, batch 11000 / 12635, global_step = 124715, learning_rate = 1.000000e-02, loss = 0.378907, l2 = 0.002327, auc = 0.783612
elapsed : 1:32:45, ETA : 0:01:12
epoch 10 / 10, batch 11000 / 12635, global_step = 124715, learning_rate = 1.000000e-02, loss = 0.378907, l2 = 0.002327, auc = 0.783612
[[34m2023-05-13 15:36:55[0m] elapsed : 1:33:30, ETA : 0:00:28
[[34m2023-05-13 15:36:55[0m] epoch 10 / 10, batch 12000 / 12635, global_step = 125715, learning_rate = 1.000000e-02, loss = 0.378461, l2 = 0.002328, auc = 0.782331
elapsed : 1:33:30, ETA : 0:00:28
epoch 10 / 10, batch 12000 / 12635, global_step = 125715, learning_rate = 1.000000e-02, loss = 0.378461, l2 = 0.002328, auc = 0.782331
[[34m2023-05-13 15:37:24[0m] new iteration
new iteration
on disk...
[[34m2023-05-13 15:37:24[0m] running test...
on disk...
[[34m2023-05-13 15:37:29[0m] evaluated batches: 1000, 0:00:04
[[34m2023-05-13 15:37:34[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 15:37:39[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 15:37:44[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 15:37:49[0m] evaluated batches: 5000, 0:00:04
[[34m2023-05-13 15:37:54[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 15:37:58[0m] evaluated batches: 7000, 0:00:04
[[34m2023-05-13 15:38:04[0m] evaluated batches: 8000, 0:00:05
[[34m2023-05-13 15:38:09[0m] evaluated batches: 9000, 0:00:04
[[34m2023-05-13 15:38:13[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 15:38:18[0m] evaluated batches: 11000, 0:00:04
[[34m2023-05-13 15:38:23[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 15:38:28[0m] evaluated batches: 13000, 0:00:04
[[34m2023-05-13 15:38:32[0m] evaluated batches: 14000, 0:00:04
[[34m2023-05-13 15:38:37[0m] evaluated batches: 15000, 0:00:04
[[34m2023-05-13 15:38:43[0m] evaluated batches: 16000, 0:00:05
[[34m2023-05-13 15:38:48[0m] evaluated batches: 17000, 0:00:04
[[34m2023-05-13 15:38:53[0m] evaluated batches: 18000, 0:00:04
[[34m2023-05-13 15:38:58[0m] evaluated batches: 19000, 0:00:04
[[34m2023-05-13 15:39:03[0m] evaluated batches: 20000, 0:00:05
[[34m2023-05-13 15:39:08[0m] evaluated batches: 21000, 0:00:04
[[34m2023-05-13 15:39:13[0m] evaluated batches: 22000, 0:00:04
[[34m2023-05-13 15:39:17[0m] evaluated batches: 23000, 0:00:04
[[34m2023-05-13 15:39:23[0m] evaluated batches: 24000, 0:00:05
[[34m2023-05-13 15:39:28[0m] evaluated batches: 25000, 0:00:05
[[34m2023-05-13 15:39:33[0m] evaluated batches: 26000, 0:00:05
[[34m2023-05-13 15:39:38[0m] evaluated batches: 27000, 0:00:04
[[34m2023-05-13 15:39:43[0m] evaluated batches: 28000, 0:00:05
[[34m2023-05-13 15:39:48[0m] evaluated batches: 29000, 0:00:04
[[34m2023-05-13 15:39:53[0m] evaluated batches: 30000, 0:00:04
[[34m2023-05-13 15:39:58[0m] evaluated batches: 31000, 0:00:04
[[34m2023-05-13 15:40:05[0m] test loss = 0.378354, test auc = 0.781875
[[34m2023-05-13 15:40:05[0m] evaluated time: 0:02:40
[[34m2023-05-13 15:40:05[0m] analyse_structure
[[34m2023-05-13 15:40:05[0m] Done!
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:       batch_size ▁
wandb:               lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         test_auc ▁▃▄▅▆▇▇▇██
wandb:    test_log_loss █▆▅▄▃▃▂▂▁▁
wandb:    train_l2_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss ▃▂▁▁█▁▁▁█▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_moving_auc ▁▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████████
wandb: 
wandb: Run summary:
wandb: batch_size 256
wandb:         lr 0.01
wandb: 
wandb: 🚀 View run avazu-BS-256-001-retrain_irazor-2023-05-13 14:03:15 at: https://wandb.ai/yao-yao/irazor/runs/gtew4j0b
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20230513_140316-gtew4j0b/logs
