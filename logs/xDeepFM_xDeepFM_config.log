nohup: ignoring input
2023-05-14 14:49:08.134080: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-14 14:49:09.098798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Got hdf Avazu data set, getting metadata...
Initialization finished!
****************************************************************************************************
config-xDeepFM_config, fileds-23, parms-353372, dim-118.
****************************************************************************************************
current config:  [6, 4, 10, 3, 3, 22, 3, 5, 2, 0, 2, 4, 3, 2, 3, 6, 3, 4, 2, 6, 2, 16, 2, 5]
full_emb_for_0_emb_size_6 initialized from: -0.6793662204867574 0.6793662204867574
full_emb_for_1_emb_size_4 initialized from: -0.7385489458759964 0.7385489458759964
full_emb_for_2_emb_size_10 initialized from: -0.04367825709760247 0.04367825709760247
full_emb_for_3_emb_size_3 initialized from: -0.04146320908632603 0.04146320908632603
full_emb_for_4_emb_size_3 initialized from: -0.4714045207910317 0.4714045207910317
full_emb_for_5_emb_size_22 initialized from: -0.0386141642138271 0.0386141642138271
full_emb_for_6_emb_size_3 initialized from: -0.1533929977694741 0.1533929977694741
full_emb_for_7_emb_size_5 initialized from: -0.4264014327112209 0.4264014327112209
full_emb_for_8_emb_size_2 initialized from: -0.007690373965638974 0.007690373965638974
full_emb_for_10_emb_size_2 initialized from: -0.03181692187549228 0.03181692187549228
full_emb_for_11_emb_size_4 initialized from: -0.816496580927726 0.816496580927726
full_emb_for_12_emb_size_3 initialized from: -0.9258200997725514 0.9258200997725514
full_emb_for_13_emb_size_2 initialized from: -0.04980325076426954 0.04980325076426954
full_emb_for_14_emb_size_3 initialized from: -0.7385489458759964 0.7385489458759964
full_emb_for_15_emb_size_6 initialized from: -0.6324555320336759 0.6324555320336759
full_emb_for_16_emb_size_3 initialized from: -0.11826247919781652 0.11826247919781652
full_emb_for_17_emb_size_4 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_18_emb_size_2 initialized from: -0.29488391230979427 0.29488391230979427
full_emb_for_19_emb_size_6 initialized from: -0.1867718419094071 0.1867718419094071
full_emb_for_20_emb_size_2 initialized from: -0.3110855084191276 0.3110855084191276
full_emb_for_21_emb_size_16 initialized from: -0.4803844614152614 0.4803844614152614
full_emb_for_22_emb_size_2 initialized from: -0.4803844614152614 0.4803844614152614
full_emb_for_23_emb_size_5 initialized from: -0.7071067811865476 0.7071067811865476
0_transform_6_to_22 initialized from: -0.4629100498862757 0.4629100498862757
0_transform_6_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
1_transform_4_to_22 initialized from: -0.4803844614152614 0.4803844614152614
1_transform_4_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
2_transform_10_to_22 initialized from: -0.4330127018922193 0.4330127018922193
2_transform_10_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
3_transform_3_to_22 initialized from: -0.4898979485566356 0.4898979485566356
3_transform_3_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
4_transform_3_to_22 initialized from: -0.4898979485566356 0.4898979485566356
4_transform_3_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
6_transform_3_to_22 initialized from: -0.4898979485566356 0.4898979485566356
6_transform_3_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
7_transform_5_to_22 initialized from: -0.4714045207910317 0.4714045207910317
7_transform_5_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
8_transform_2_to_22 initialized from: -0.5 0.5
8_transform_2_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
9_transform_2_to_22 initialized from: -0.5 0.5
9_transform_2_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
10_transform_4_to_22 initialized from: -0.4803844614152614 0.4803844614152614
10_transform_4_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
11_transform_3_to_22 initialized from: -0.4898979485566356 0.4898979485566356
11_transform_3_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
12_transform_2_to_22 initialized from: -0.5 0.5
12_transform_2_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
13_transform_3_to_22 initialized from: -0.4898979485566356 0.4898979485566356
13_transform_3_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
14_transform_6_to_22 initialized from: -0.4629100498862757 0.4629100498862757
14_transform_6_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
15_transform_3_to_22 initialized from: -0.4898979485566356 0.4898979485566356
15_transform_3_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
16_transform_4_to_22 initialized from: -0.4803844614152614 0.4803844614152614
16_transform_4_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
17_transform_2_to_22 initialized from: -0.5 0.5
17_transform_2_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
18_transform_6_to_22 initialized from: -0.4629100498862757 0.4629100498862757
18_transform_6_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
19_transform_2_to_22 initialized from: -0.5 0.5
19_transform_2_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
20_transform_16_to_22 initialized from: -0.39735970711951313 0.39735970711951313
20_transform_16_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
21_transform_2_to_22 initialized from: -0.5 0.5
21_transform_2_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
22_transform_5_to_22 initialized from: -0.4714045207910317 0.4714045207910317
22_transform_5_to_22_wt initialized from: -0.5107539184552492 0.5107539184552492
current config:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
full_emb_for_0_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_1_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_2_emb_size_1 initialized from: -0.043740888263985325 0.043740888263985325
full_emb_for_3_emb_size_1 initialized from: -0.04147509477069983 0.04147509477069983
full_emb_for_4_emb_size_1 initialized from: -0.4898979485566356 0.4898979485566356
full_emb_for_5_emb_size_1 initialized from: -0.038715317938997504 0.038715317938997504
full_emb_for_6_emb_size_1 initialized from: -0.15399810070180361 0.15399810070180361
full_emb_for_7_emb_size_1 initialized from: -0.454858826147342 0.454858826147342
full_emb_for_8_emb_size_1 initialized from: -0.007690411867832244 0.007690411867832244
full_emb_for_9_emb_size_1 initialized from: -0.003384897591352657 0.003384897591352657
full_emb_for_10_emb_size_1 initialized from: -0.031819606281476856 0.031819606281476856
full_emb_for_11_emb_size_1 initialized from: -1.0 1.0
full_emb_for_12_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_13_emb_size_1 initialized from: -0.04981354813867179 0.04981354813867179
full_emb_for_14_emb_size_1 initialized from: -0.816496580927726 0.816496580927726
full_emb_for_15_emb_size_1 initialized from: -0.7745966692414834 0.7745966692414834
full_emb_for_16_emb_size_1 initialized from: -0.11853911695403994 0.11853911695403994
full_emb_for_17_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_18_emb_size_1 initialized from: -0.2970442628930023 0.2970442628930023
full_emb_for_19_emb_size_1 initialized from: -0.18954720708196904 0.18954720708196904
full_emb_for_20_emb_size_1 initialized from: -0.31362502409359 0.31362502409359
full_emb_for_21_emb_size_1 initialized from: -0.7385489458759964 0.7385489458759964
full_emb_for_22_emb_size_1 initialized from: -0.4898979485566356 0.4898979485566356
full_emb_for_23_emb_size_1 initialized from: -0.8660254037844386 0.8660254037844386
all_bias [<tf.Tensor 'GatherV2_23:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_24:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_25:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_26:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_27:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_28:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_29:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_30:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_31:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_32:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_33:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_34:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_35:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_36:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_37:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_38:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_39:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_40:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_41:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_42:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_43:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_44:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_45:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'GatherV2_46:0' shape=(?, 1) dtype=float32>]
cin_0_wt_23_new_23 initialized from: -0.10425720702853739 0.10425720702853739
cin_0_bias_23_new_23 initialized from: -0.4898979485566356 0.4898979485566356
cin_1_wt_23_new_23 initialized from: -0.10425720702853739 0.10425720702853739
cin_1_bias_23_new_23 initialized from: -0.4898979485566356 0.4898979485566356
cin_2_wt_23_new_23 initialized from: -0.10425720702853739 0.10425720702853739
cin_2_bias_23_new_23 initialized from: -0.4898979485566356 0.4898979485566356
cin_pooling_in_69_out1 initialized from: -0.29277002188455997 0.29277002188455997
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
w_0 initialized from: -0.08564440043098774 0.08564440043098774
(118, 700) (700,)
relu
w_1 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_2 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_3 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_4 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_5 initialized from: -0.0925159507394634 0.0925159507394634
(700, 1) (1,)
none
mlp output Tensor("hidden_5/add:0", shape=(?, 1), dtype=float32) bias_sum Tensor("Sum_3:0", shape=(?, 1), dtype=float32)
lgits Tensor("Sum_4:0", shape=(?,), dtype=float32)
[[34m2023-05-14 14:49:11[0m] Experiment directory created at /home/ubuntu/results/xDeepFM_Retrain/avazu/000-[700, 700, 700, 700, 700, 1]-bs-128
[[34m2023-05-14 14:49:11[0m] Batchsize: 128
wandb: Currently logged in as: yao-yao. Use `wandb login --relogin` to force relogin
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /tmp/wandb/run-20230514_144912-eogrz2xo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avazu-BS-128-000-xDeepFM_Retrain-2023-05-14 14:49:11
wandb: â­ï¸ View project at https://wandb.ai/yao-yao/irazor
wandb: ðŸš€ View run at https://wandb.ai/yao-yao/irazor/runs/eogrz2xo
2023-05-14 14:49:17.529965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:49:17.531690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:49:17.532548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:49:19.130479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:49:19.131492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:49:19.132289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-14 14:49:19.133133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 36113 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
[[34m2023-05-14 14:49:19[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
add l2 0.001 Tensor("concat:0", shape=(?, 118), dtype=float32)
add l2 0.001 Tensor("concat_1:0", shape=(?, 24), dtype=float32)
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
[[34m2023-05-14 14:49:19[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2023-05-14 14:49:20.657436: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
[[34m2023-05-14 14:49:21[0m] total batches: 505370	batch per epoch: 50537
[[34m2023-05-14 14:49:21[0m] new iteration
new iteration
on disk...
2023-05-14 14:49:22.911495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
[[34m2023-05-14 14:49:42[0m] elapsed : 0:00:21, ETA : 2:56:31
[[34m2023-05-14 14:49:42[0m] epoch 1 / 10, batch 1000 / 50537, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.510215, l2 = 0.433153, auc = 0.690619
elapsed : 0:00:21, ETA : 2:56:31
epoch 1 / 10, batch 1000 / 50537, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.510215, l2 = 0.433153, auc = 0.690619
[[34m2023-05-14 14:50:01[0m] elapsed : 0:00:40, ETA : 2:47:47
[[34m2023-05-14 14:50:01[0m] epoch 1 / 10, batch 2000 / 50537, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.405062, l2 = 0.211218, auc = 0.726833
elapsed : 0:00:40, ETA : 2:47:47
epoch 1 / 10, batch 2000 / 50537, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.405062, l2 = 0.211218, auc = 0.726833
[[34m2023-05-14 14:50:29[0m] elapsed : 0:01:08, ETA : 3:09:47
[[34m2023-05-14 14:50:29[0m] epoch 1 / 10, batch 3000 / 50537, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.404539, l2 = 0.117825, auc = 0.734528
elapsed : 0:01:08, ETA : 3:09:47
epoch 1 / 10, batch 3000 / 50537, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.404539, l2 = 0.117825, auc = 0.734528
[[34m2023-05-14 14:51:03[0m] elapsed : 0:01:42, ETA : 3:33:04
[[34m2023-05-14 14:51:03[0m] epoch 1 / 10, batch 4000 / 50537, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.402817, l2 = 0.070444, auc = 0.736624
elapsed : 0:01:42, ETA : 3:33:04
epoch 1 / 10, batch 4000 / 50537, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.402817, l2 = 0.070444, auc = 0.736624
[[34m2023-05-14 14:51:37[0m] elapsed : 0:02:16, ETA : 3:46:50
[[34m2023-05-14 14:51:37[0m] epoch 1 / 10, batch 5000 / 50537, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.401755, l2 = 0.044437, auc = 0.739061
elapsed : 0:02:16, ETA : 3:46:50
epoch 1 / 10, batch 5000 / 50537, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.401755, l2 = 0.044437, auc = 0.739061
[[34m2023-05-14 14:52:10[0m] elapsed : 0:02:49, ETA : 3:54:25
[[34m2023-05-14 14:52:10[0m] epoch 1 / 10, batch 6000 / 50537, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.399556, l2 = 0.029416, auc = 0.741852
elapsed : 0:02:49, ETA : 3:54:25
epoch 1 / 10, batch 6000 / 50537, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.399556, l2 = 0.029416, auc = 0.741852
[[34m2023-05-14 14:52:44[0m] elapsed : 0:03:23, ETA : 4:00:52
[[34m2023-05-14 14:52:44[0m] epoch 1 / 10, batch 7000 / 50537, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.397563, l2 = 0.020308, auc = 0.744958
elapsed : 0:03:23, ETA : 4:00:52
epoch 1 / 10, batch 7000 / 50537, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.397563, l2 = 0.020308, auc = 0.744958
[[34m2023-05-14 14:53:18[0m] elapsed : 0:03:57, ETA : 4:05:34
[[34m2023-05-14 14:53:18[0m] epoch 1 / 10, batch 8000 / 50537, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.398958, l2 = 0.014640, auc = 0.744217
elapsed : 0:03:57, ETA : 4:05:34
epoch 1 / 10, batch 8000 / 50537, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.398958, l2 = 0.014640, auc = 0.744217
[[34m2023-05-14 14:53:52[0m] elapsed : 0:04:31, ETA : 4:09:06
[[34m2023-05-14 14:53:52[0m] epoch 1 / 10, batch 9000 / 50537, global_step = 9000, learning_rate = 1.000000e-02, loss = 0.398421, l2 = 0.011045, auc = 0.742637
elapsed : 0:04:31, ETA : 4:09:06
epoch 1 / 10, batch 9000 / 50537, global_step = 9000, learning_rate = 1.000000e-02, loss = 0.398421, l2 = 0.011045, auc = 0.742637
[[34m2023-05-14 14:54:26[0m] elapsed : 0:05:05, ETA : 4:11:48
[[34m2023-05-14 14:54:26[0m] epoch 1 / 10, batch 10000 / 50537, global_step = 10000, learning_rate = 1.000000e-02, loss = 0.397904, l2 = 0.008688, auc = 0.745058
elapsed : 0:05:05, ETA : 4:11:48
epoch 1 / 10, batch 10000 / 50537, global_step = 10000, learning_rate = 1.000000e-02, loss = 0.397904, l2 = 0.008688, auc = 0.745058
[[34m2023-05-14 14:55:00[0m] elapsed : 0:05:39, ETA : 4:13:55
[[34m2023-05-14 14:55:00[0m] epoch 1 / 10, batch 11000 / 50537, global_step = 11000, learning_rate = 1.000000e-02, loss = 0.397160, l2 = 0.007100, auc = 0.745332
elapsed : 0:05:39, ETA : 4:13:55
epoch 1 / 10, batch 11000 / 50537, global_step = 11000, learning_rate = 1.000000e-02, loss = 0.397160, l2 = 0.007100, auc = 0.745332
[[34m2023-05-14 14:55:34[0m] elapsed : 0:06:13, ETA : 4:15:35
[[34m2023-05-14 14:55:34[0m] epoch 1 / 10, batch 12000 / 50537, global_step = 12000, learning_rate = 1.000000e-02, loss = 0.398488, l2 = 0.006017, auc = 0.747157
elapsed : 0:06:13, ETA : 4:15:35
epoch 1 / 10, batch 12000 / 50537, global_step = 12000, learning_rate = 1.000000e-02, loss = 0.398488, l2 = 0.006017, auc = 0.747157
[[34m2023-05-14 14:56:08[0m] elapsed : 0:06:46, ETA : 4:16:17
[[34m2023-05-14 14:56:08[0m] epoch 1 / 10, batch 13000 / 50537, global_step = 13000, learning_rate = 1.000000e-02, loss = 0.398497, l2 = 0.005202, auc = 0.743555
elapsed : 0:06:46, ETA : 4:16:17
epoch 1 / 10, batch 13000 / 50537, global_step = 13000, learning_rate = 1.000000e-02, loss = 0.398497, l2 = 0.005202, auc = 0.743555
[[34m2023-05-14 14:56:41[0m] elapsed : 0:07:20, ETA : 4:17:23
[[34m2023-05-14 14:56:41[0m] epoch 1 / 10, batch 14000 / 50537, global_step = 14000, learning_rate = 1.000000e-02, loss = 0.395586, l2 = 0.004663, auc = 0.749041
elapsed : 0:07:20, ETA : 4:17:23
epoch 1 / 10, batch 14000 / 50537, global_step = 14000, learning_rate = 1.000000e-02, loss = 0.395586, l2 = 0.004663, auc = 0.749041
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [[34m2023-05-14 15:04:00[0m] elapsed : 0:14:39, ETA : 4:19:33
[[34m2023-05-14 15:04:00[0m] epoch 1 / 10, batch 27000 / 50537, global_step = 27000, learning_rate = 1.000000e-02, loss = 0.392681, l2 = 0.002655, auc = 0.756029
elapsed : 0:14:39, ETA : 4:19:33
epoch 1 / 10, batch 27000 / 50537, global_step = 27000, learning_rate = 1.000000e-02, loss = 0.392681, l2 = 0.002655, auc = 0.756029
[[34m2023-05-14 15:04:34[0m] elapsed : 0:15:13, ETA : 4:19:25
[[34m2023-05-14 15:04:34[0m] epoch 1 / 10, batch 28000 / 50537, global_step = 28000, learning_rate = 1.000000e-02, loss = 0.391379, l2 = 0.002625, auc = 0.759538
elapsed : 0:15:13, ETA : 4:19:25
epoch 1 / 10, batch 28000 / 50537, global_step = 28000, learning_rate = 1.000000e-02, loss = 0.391379, l2 = 0.002625, auc = 0.759538
[[34m2023-05-14 15:05:07[0m] elapsed : 0:15:46, ETA : 4:18:59
[[34m2023-05-14 15:05:07[0m] epoch 1 / 10, batch 29000 / 50537, global_step = 29000, learning_rate = 1.000000e-02, loss = 0.391688, l2 = 0.002586, auc = 0.758008
elapsed : 0:15:46, ETA : 4:18:59
epoch 1 / 10, batch 29000 / 50537, global_step = 29000, learning_rate = 1.000000e-02, loss = 0.391688, l2 = 0.002586, auc = 0.758008
[[34m2023-05-14 15:05:41[0m] elapsed : 0:16:20, ETA : 4:18:48
[[34m2023-05-14 15:05:41[0m] epoch 1 / 10, batch 30000 / 50537, global_step = 30000, learning_rate = 1.000000e-02, loss = 0.392807, l2 = 0.002539, auc = 0.756685
elapsed : 0:16:20, ETA : 4:18:48
epoch 1 / 10, batch 30000 / 50537, global_step = 30000, learning_rate = 1.000000e-02, loss = 0.392807, l2 = 0.002539, auc = 0.756685
[[34m2023-05-14 15:06:15[0m] elapsed : 0:16:54, ETA : 4:18:36
[[34m2023-05-14 15:06:15[0m] epoch 1 / 10, batch 31000 / 50537, global_step = 31000, learning_rate = 1.000000e-02, loss = 0.389101, l2 = 0.002506, auc = 0.756326
elapsed : 0:16:54, ETA : 4:18:36
epoch 1 / 10, batch 31000 / 50537, global_step = 31000, learning_rate = 1.000000e-02, loss = 0.389101, l2 = 0.002506, auc = 0.756326
[[34m2023-05-14 15:06:49[0m] elapsed : 0:17:28, ETA : 4:18:22
[[34m2023-05-14 15:06:49[0m] epoch 1 / 10, batch 32000 / 50537, global_step = 32000, learning_rate = 1.000000e-02, loss = 0.394352, l2 = 0.002452, auc = 0.756684
elapsed : 0:17:28, ETA : 4:18:22
epoch 1 / 10, batch 32000 / 50537, global_step = 32000, learning_rate = 1.000000e-02, loss = 0.394352, l2 = 0.002452, auc = 0.756684
[[34m2023-05-14 15:07:21[0m] elapsed : 0:18:00, ETA : 4:17:39
[[34m2023-05-14 15:07:21[0m] epoch 1 / 10, batch 33000 / 50537, global_step = 33000, learning_rate = 1.000000e-02, loss = 0.390659, l2 = 0.002442, auc = 0.756939
elapsed : 0:18:00, ETA : 4:17:39
epoch 1 / 10, batch 33000 / 50537, global_step = 33000, learning_rate = 1.000000e-02, loss = 0.390659, l2 = 0.002442, auc = 0.756939
[[34m2023-05-14 15:07:54[0m] elapsed : 0:18:33, ETA : 4:17:10
[[34m2023-05-14 15:07:54[0m] epoch 1 / 10, batch 34000 / 50537, global_step = 34000, learning_rate = 1.000000e-02, loss = 0.391995, l2 = 0.002408, auc = 0.756256
elapsed : 0:18:33, ETA : 4:17:10
epoch 1 / 10, batch 34000 / 50537, global_step = 34000, learning_rate = 1.000000e-02, loss = 0.391995, l2 = 0.002408, auc = 0.756256
[[34m2023-05-14 15:08:26[0m] elapsed : 0:19:05, ETA : 4:16:27
[[34m2023-05-14 15:08:26[0m] epoch 1 / 10, batch 35000 / 50537, global_step = 35000, learning_rate = 1.000000e-02, loss = 0.392973, l2 = 0.002383, auc = 0.756180
elapsed : 0:19:05, ETA : 4:16:27
epoch 1 / 10, batch 35000 / 50537, global_step = 35000, learning_rate = 1.000000e-02, loss = 0.392973, l2 = 0.002383, auc = 0.756180
[[34m2023-05-14 15:08:59[0m] elapsed : 0:19:38, ETA : 4:15:58
[[34m2023-05-14 15:08:59[0m] epoch 1 / 10, batch 36000 / 50537, global_step = 36000, learning_rate = 1.000000e-02, loss = 0.394417, l2 = 0.002362, auc = 0.756142
elapsed : 0:19:38, ETA : 4:15:58
epoch 1 / 10, batch 36000 / 50537, global_step = 36000, learning_rate = 1.000000e-02, loss = 0.394417, l2 = 0.002362, auc = 0.756142
[[34m2023-05-14 15:09:31[0m] elapsed : 0:20:10, ETA : 4:15:16
[[34m2023-05-14 15:09:31[0m] epoch 1 / 10, batch 37000 / 50537, global_step = 37000, learning_rate = 1.000000e-02, loss = 0.392843, l2 = 0.002334, auc = 0.757205
elapsed : 0:20:10, ETA : 4:15:16
epoch 1 / 10, batch 37000 / 50537, global_step = 37000, learning_rate = 1.000000e-02, loss = 0.392843, l2 = 0.002334, auc = 0.757205
[[34m2023-05-14 15:10:04[0m] elapsed : 0:20:43, ETA : 4:14:47
[[34m2023-05-14 15:10:04[0m] epoch 1 / 10, batch 38000 / 50537, global_step = 38000, learning_rate = 1.000000e-02, loss = 0.390820, l2 = 0.002292, auc = 0.758534
elapsed : 0:20:43, ETA : 4:14:47
epoch 1 / 10, batch 38000 / 50537, global_step = 38000, learning_rate = 1.000000e-02, loss = 0.390820, l2 = 0.002292, auc = 0.758534
[[34m2023-05-14 15:10:36[0m] elapsed : 0:21:15, ETA : 4:14:06
[[34m2023-05-14 15:10:36[0m] epoch 1 / 10, batch 39000 / 50537, global_step = 39000, learning_rate = 1.000000e-02, loss = 0.391679, l2 = 0.002277, auc = 0.759237
elapsed : 0:21:15, ETA : 4:14:06
epoch 1 / 10, batch 39000 / 50537, global_step = 39000, learning_rate = 1.000000e-02, loss = 0.391679, l2 = 0.002277, auc = 0.759237
[[34m2023-05-14 15:11:09[0m] elapsed : 0:21:48, ETA : 4:13:37
[[34m2023-05-14 15:11:09[0m] epoch 1 / 10, batch 40000 / 50537, global_step = 40000, learning_rate = 1.000000e-02, loss = 0.393590, l2 = 0.002258, auc = 0.757044
elapsed : 0:21:48, ETA : 4:13:37
epoch 1 / 10, batch 40000 / 50537, global_step = 40000, learning_rate = 1.000000e-02, loss = 0.393590, l2 = 0.002258, auc = 0.757044
[[34m2023-05-14 15:11:41[0m] elapsed : 0:22:20, ETA : 4:12:56
[[34m2023-05-14 15:11:41[0m] epoch 1 / 10, batch 41000 / 50537, global_step = 41000, learning_rate = 1.000000e-02, loss = 0.393556, l2 = 0.002236, auc = 0.758226
elapsed : 0:22:20, ETA : 4:12:56
epoch 1 / 10, batch 41000 / 50537, global_step = 41000, learning_rate = 1.000000e-02, loss = 0.393556, l2 = 0.002236, auc = 0.758226
[[34m2023-05-14 15:12:11[0m] elapsed : 0:22:50, ETA : 4:11:54
[[34m2023-05-14 15:12:11[0m] epoch 1 / 10, batch 42000 / 50537, global_step = 42000, learning_rate = 1.000000e-02, loss = 0.389623, l2 = 0.002203, auc = 0.757705
elapsed : 0:22:50, ETA : 4:11:54
epoch 1 / 10, batch 42000 / 50537, global_step = 42000, learning_rate = 1.000000e-02, loss = 0.389623, l2 = 0.002203, auc = 0.757705
[[34m2023-05-14 15:12:45[0m] elapsed : 0:23:24, ETA : 4:11:36
[[34m2023-05-14 15:12:45[0m] epoch 1 / 10, batch 43000 / 50537, global_step = 43000, learning_rate = 1.000000e-02, loss = 0.393979, l2 = 0.002180, auc = 0.756021
elapsed : 0:23:24, ETA : 4:11:36
epoch 1 / 10, batch 43000 / 50537, global_step = 43000, learning_rate = 1.000000e-02, loss = 0.393979, l2 = 0.002180, auc = 0.756021
[[34m2023-05-14 15:13:19[0m] elapsed : 0:23:58, ETA : 4:11:18
[[34m2023-05-14 15:13:19[0m] epoch 1 / 10, batch 44000 / 50537, global_step = 44000, learning_rate = 1.000000e-02, loss = 0.391548, l2 = 0.002170, auc = 0.760119
elapsed : 0:23:58, ETA : 4:11:18
epoch 1 / 10, batch 44000 / 50537, global_step = 44000, learning_rate = 1.000000e-02, loss = 0.391548, l2 = 0.002170, auc = 0.760119
[[34m2023-05-14 15:13:53[0m] elapsed : 0:24:32, ETA : 4:10:59
[[34m2023-05-14 15:13:53[0m] epoch 1 / 10, batch 45000 / 50537, global_step = 45000, learning_rate = 1.000000e-02, loss = 0.388375, l2 = 0.002148, auc = 0.760834
elapsed : 0:24:32, ETA : 4:10:59
epoch 1 / 10, batch 45000 / 50537, global_step = 45000, learning_rate = 1.000000e-02, loss = 0.388375, l2 = 0.002148, auc = 0.760834
[[34m2023-05-14 15:14:39[0m] elapsed : 0:25:18, ETA : 4:12:39
[[34m2023-05-14 15:14:39[0m] epoch 1 / 10, batch 46000 / 50537, global_step = 46000, learning_rate = 1.000000e-02, loss = 0.388767, l2 = 0.002159, auc = 0.757338
elapsed : 0:25:18, ETA : 4:12:39
epoch 1 / 10, batch 46000 / 50537, global_step = 46000, learning_rate = 1.000000e-02, loss = 0.388767, l2 = 0.002159, auc = 0.757338
[[34m2023-05-14 15:15:27[0m] elapsed : 0:26:06, ETA : 4:14:32
[[34m2023-05-14 15:15:27[0m] epoch 1 / 10, batch 47000 / 50537, global_step = 47000, learning_rate = 1.000000e-02, loss = 0.391682, l2 = 0.002120, auc = 0.758159
elapsed : 0:26:06, ETA : 4:14:32
epoch 1 / 10, batch 47000 / 50537, global_step = 47000, learning_rate = 1.000000e-02, loss = 0.391682, l2 = 0.002120, auc = 0.758159
[[34m2023-05-14 15:16:15[0m] elapsed : 0:26:54, ETA : 4:16:19
[[34m2023-05-14 15:16:15[0m] epoch 1 / 10, batch 48000 / 50537, global_step = 48000, learning_rate = 1.000000e-02, loss = 0.391427, l2 = 0.002116, auc = 0.760787
elapsed : 0:26:54, ETA : 4:16:19
epoch 1 / 10, batch 48000 / 50537, global_step = 48000, learning_rate = 1.000000e-02, loss = 0.391427, l2 = 0.002116, auc = 0.760787
[[34m2023-05-14 15:17:03[0m] elapsed : 0:27:42, ETA : 4:17:59
[[34m2023-05-14 15:17:03[0m] epoch 1 / 10, batch 49000 / 50537, global_step = 49000, learning_rate = 1.000000e-02, loss = 0.387751, l2 = 0.002102, auc = 0.759308
elapsed : 0:27:42, ETA : 4:17:59
epoch 1 / 10, batch 49000 / 50537, global_step = 49000, learning_rate = 1.000000e-02, loss = 0.387751, l2 = 0.002102, auc = 0.759308
[[34m2023-05-14 15:17:51[0m] elapsed : 0:28:30, ETA : 4:19:33
[[34m2023-05-14 15:17:51[0m] epoch 1 / 10, batch 50000 / 50537, global_step = 50000, learning_rate = 1.000000e-02, loss = 0.392735, l2 = 0.002081, auc = 0.756711
elapsed : 0:28:30, ETA : 4:19:33
epoch 1 / 10, batch 50000 / 50537, global_step = 50000, learning_rate = 1.000000e-02, loss = 0.392735, l2 = 0.002081, auc = 0.756711
[[34m2023-05-14 15:18:16[0m] running test...
on disk...
[[34m2023-05-14 15:18:23[0m] evaluated batches: 1000, 0:00:07
[[34m2023-05-14 15:18:30[0m] evaluated batches: 2000, 0:00:06
[[34m2023-05-14 15:18:36[0m] evaluated batches: 3000, 0:00:06
[[34m2023-05-14 15:18:43[0m] evaluated batches: 4000, 0:00:06
[[34m2023-05-14 15:18:49[0m] evaluated batches: 5000, 0:00:06
[[34m2023-05-14 15:18:56[0m] evaluated batches: 6000, 0:00:06
[[34m2023-05-14 15:19:02[0m] evaluated batches: 7000, 0:00:06
[[34m2023-05-14 15:19:09[0m] evaluated batches: 8000, 0:00:06
[[34m2023-05-14 15:19:15[0m] evaluated batches: 9000, 0:00:06
[[34m2023-05-14 15:19:22[0m] evaluated batches: 10000, 0:00:06
[[34m2023-05-14 15:19:29[0m] evaluated batches: 11000, 0:00:06
[[34m2023-05-14 15:19:35[0m] evaluated batches: 12000, 0:00:06
[[34m2023-05-14 15:19:42[0m] evaluated batches: 13000, 0:00:06
[[34m2023-05-14 15:19:49[0m] evaluated batches: 14000, 0:00:06
[[34m2023-05-14 15:19:55[0m] evaluated batches: 15000, 0:00:06
[[34m2023-05-14 15:20:02[0m] evaluated batches: 16000, 0:00:06
[[34m2023-05-14 15:20:09[0m] evaluated batches: 17000, 0:00:06
[[34m2023-05-14 15:20:15[0m] evaluated batches: 18000, 0:00:06
[[34m2023-05-14 15:20:22[0m] evaluated batches: 19000, 0:00:06
[[34m2023-05-14 15:20:28[0m] evaluated batches: 20000, 0:00:06
[[34m2023-05-14 15:20:35[0m] evaluated batches: 21000, 0:00:06
[[34m2023-05-14 15:20:42[0m] evaluated batches: 22000, 0:00:06
[[34m2023-05-14 15:20:48[0m] evaluated batches: 23000, 0:00:06
[[34m2023-05-14 15:20:55[0m] evaluated batches: 24000, 0:00:06
[[34m2023-05-14 15:21:02[0m] evaluated batches: 25000, 0:00:06
[[34m2023-05-14 15:21:08[0m] evaluated batches: 26000, 0:00:06
[[34m2023-05-14 15:21:15[0m] evaluated batches: 27000, 0:00:06
[[34m2023-05-14 15:21:21[0m] evaluated batches: 28000, 0:00:06
[[34m2023-05-14 15:21:28[0m] evaluated batches: 29000, 0:00:06
[[34m2023-05-14 15:21:35[0m] evaluated batches: 30000, 0:00:06
[[34m2023-05-14 15:21:41[0m] evaluated batches: 31000, 0:00:06
[[34m2023-05-14 15:21:48[0m] evaluated batches: 32000, 0:00:06
[[34m2023-05-14 15:21:55[0m] evaluated batches: 33000, 0:00:06
[[34m2023-05-14 15:22:01[0m] evaluated batches: 34000, 0:00:06
[[34m2023-05-14 15:22:08[0m] evaluated batches: 35000, 0:00:06
[[34m2023-05-14 15:22:15[0m] evaluated batches: 36000, 0:00:06
[[34m2023-05-14 15:22:21[0m] evaluated batches: 37000, 0:00:06
[[34m2023-05-14 15:22:28[0m] evaluated batches: 38000, 0:00:06
[[34m2023-05-14 15:22:34[0m] evaluated batches: 39000, 0:00:06
[[34m2023-05-14 15:22:41[0m] evaluated batches: 40000, 0:00:06
[[34m2023-05-14 15:22:48[0m] evaluated batches: 41000, 0:00:06
[[34m2023-05-14 15:22:54[0m] evaluated batches: 42000, 0:00:06
[[34m2023-05-14 15:23:01[0m] evaluated batches: 43000, 0:00:06
[[34m2023-05-14 15:23:08[0m] evaluated batches: 44000, 0:00:06
[[34m2023-05-14 15:23:14[0m] evaluated batches: 45000, 0:00:06
[[34m2023-05-14 15:23:21[0m] evaluated batches: 46000, 0:00:06
[[34m2023-05-14 15:23:28[0m] evaluated batches: 47000, 0:00:06
[[34m2023-05-14 15:23:34[0m] evaluated batches: 48000, 0:00:06
[[34m2023-05-14 15:23:41[0m] evaluated batches: 49000, 0:00:06
[[34m2023-05-14 15:23:48[0m] evaluated batches: 50000, 0:00:06
[[34m2023-05-14 15:23:54[0m] evaluated batches: 51000, 0:00:06
[[34m2023-05-14 15:24:01[0m] evaluated batches: 52000, 0:00:06
[[34m2023-05-14 15:24:07[0m] evaluated batches: 53000, 0:00:06
[[34m2023-05-14 15:24:14[0m] evaluated batches: 54000, 0:00:06
[[34m2023-05-14 15:24:21[0m] evaluated batches: 55000, 0:00:06
[[34m2023-05-14 15:24:27[0m] evaluated batches: 56000, 0:00:06
[[34m2023-05-14 15:24:34[0m] evaluated batches: 57000, 0:00:06
[[34m2023-05-14 15:24:40[0m] evaluated batches: 58000, 0:00:06
[[34m2023-05-14 15:24:47[0m] evaluated batches: 59000, 0:00:06
[[34m2023-05-14 15:24:54[0m] evaluated batches: 60000, 0:00:06
[[34m2023-05-14 15:25:00[0m] evaluated batches: 61000, 0:00:06
[[34m2023-05-14 15:25:06[0m] evaluated batches: 62000, 0:00:06
[[34m2023-05-14 15:25:13[0m] evaluated batches: 63000, 0:00:06
[[34m2023-05-14 15:25:18[0m] test loss = 0.393184, test auc = 0.759870
[[34m2023-05-14 15:25:18[0m] evaluated time: 0:07:02
[[34m2023-05-14 15:25:18[0m] analyse_structure
[[34m2023-05-14 15:26:06[0m] elapsed : 0:36:45, ETA : 5:23:37
[[34m2023-05-14 15:26:06[0m] epoch 2 / 10, batch 1000 / 50537, global_step = 51537, learning_rate = 1.000000e-02, loss = 0.600695, l2 = 0.003166, auc = 0.757784
elapsed : 0:36:45, ETA : 5:23:37
epoch 2 / 10, batch 1000 / 50537, global_step = 51537, learning_rate = 1.000000e-02, loss = 0.600695, l2 = 0.003166, auc = 0.757784
[[34m2023-05-14 15:26:54[0m] elapsed : 0:37:33, ETA : 5:23:39
[[34m2023-05-14 15:26:54[0m] epoch 2 / 10, batch 2000 / 50537, global_step = 52537, learning_rate = 1.000000e-02, loss = 0.390525, l2 = 0.002057, auc = 0.758344
elapsed : 0:37:33, ETA : 5:23:39
epoch 2 / 10, batch 2000 / 50537, global_step = 52537, learning_rate = 1.000000e-02, loss = 0.390525, l2 = 0.002057, auc = 0.758344
[[34m2023-05-14 15:27:42[0m] elapsed : 0:38:21, ETA : 5:23:39
[[34m2023-05-14 15:27:42[0m] epoch 2 / 10, batch 3000 / 50537, global_step = 53537, learning_rate = 1.000000e-02, loss = 0.392289, l2 = 0.002042, auc = 0.760999
elapsed : 0:38:21, ETA : 5:23:39
epoch 2 / 10, batch 3000 / 50537, global_step = 53537, learning_rate = 1.000000e-02, loss = 0.392289, l2 = 0.002042, auc = 0.760999
[[34m2023-05-14 15:28:29[0m] elapsed : 0:39:08, ETA : 5:23:29
[[34m2023-05-14 15:28:29[0m] epoch 2 / 10, batch 4000 / 50537, global_step = 54537, learning_rate = 1.000000e-02, loss = 0.392777, l2 = 0.002013, auc = 0.758970
elapsed : 0:39:08, ETA : 5:23:29
epoch 2 / 10, batch 4000 / 50537, global_step = 54537, learning_rate = 1.000000e-02, loss = 0.392777, l2 = 0.002013, auc = 0.758970
[[34m2023-05-14 15:29:17[0m] elapsed : 0:39:56, ETA : 5:23:26
[[34m2023-05-14 15:29:17[0m] epoch 2 / 10, batch 5000 / 50537, global_step = 55537, learning_rate = 1.000000e-02, loss = 0.391308, l2 = 0.001992, auc = 0.759837
elapsed : 0:39:56, ETA : 5:23:26
epoch 2 / 10, batch 5000 / 50537, global_step = 55537, learning_rate = 1.000000e-02, loss = 0.391308, l2 = 0.001992, auc = 0.759837
[[34m2023-05-14 15:30:05[0m] elapsed : 0:40:44, ETA : 5:23:22
[[34m2023-05-14 15:30:05[0m] epoch 2 / 10, batch 6000 / 50537, global_step = 56537, learning_rate = 1.000000e-02, loss = 0.390940, l2 = 0.002019, auc = 0.759665
elapsed : 0:40:44, ETA : 5:23:22
epoch 2 / 10, batch 6000 / 50537, global_step = 56537, learning_rate = 1.000000e-02, loss = 0.390940, l2 = 0.002019, auc = 0.759665
[[34m2023-05-14 15:30:53[0m] elapsed : 0:41:32, ETA : 5:23:16
[[34m2023-05-14 15:30:53[0m] epoch 2 / 10, batch 7000 / 50537, global_step = 57537, learning_rate = 1.000000e-02, loss = 0.388467, l2 = 0.001996, auc = 0.757820
elapsed : 0:41:32, ETA : 5:23:16
epoch 2 / 10, batch 7000 / 50537, global_step = 57537, learning_rate = 1.000000e-02, loss = 0.388467, l2 = 0.001996, auc = 0.757820
[[34m2023-05-14 15:31:41[0m] elapsed : 0:42:20, ETA : 5:23:08
[[34m2023-05-14 15:31:41[0m] epoch 2 / 10, batch 8000 / 50537, global_step = 58537, learning_rate = 1.000000e-02, loss = 0.391096, l2 = 0.001993, auc = 0.759840
elapsed : 0:42:20, ETA : 5:23:08
epoch 2 / 10, batch 8000 / 50537, global_step = 58537, learning_rate = 1.000000e-02, loss = 0.391096, l2 = 0.001993, auc = 0.759840
[[34m2023-05-14 15:32:31[0m] elapsed : 0:43:10, ETA : 5:23:14
[[34m2023-05-14 15:32:31[0m] epoch 2 / 10, batch 9000 / 50537, global_step = 59537, learning_rate = 1.000000e-02, loss = 0.393450, l2 = 0.001976, auc = 0.758293
elapsed : 0:43:10, ETA : 5:23:14
epoch 2 / 10, batch 9000 / 50537, global_step = 59537, learning_rate = 1.000000e-02, loss = 0.393450, l2 = 0.001976, auc = 0.758293
[[34m2023-05-14 15:33:33[0m] elapsed : 0:44:12, ETA : 5:24:47
[[34m2023-05-14 15:33:33[0m] epoch 2 / 10, batch 10000 / 50537, global_step = 60537, learning_rate = 1.000000e-02, loss = 0.390902, l2 = 0.001969, auc = 0.759500
elapsed : 0:44:12, ETA : 5:24:47
epoch 2 / 10, batch 10000 / 50537, global_step = 60537, learning_rate = 1.000000e-02, loss = 0.390902, l2 = 0.001969, auc = 0.759500
[[34m2023-05-14 15:34:37[0m] elapsed : 0:45:15, ETA : 5:26:21
[[34m2023-05-14 15:34:37[0m] epoch 2 / 10, batch 11000 / 50537, global_step = 61537, learning_rate = 1.000000e-02, loss = 0.392591, l2 = 0.001961, auc = 0.759927
elapsed : 0:45:15, ETA : 5:26:21
epoch 2 / 10, batch 11000 / 50537, global_step = 61537, learning_rate = 1.000000e-02, loss = 0.392591, l2 = 0.001961, auc = 0.759927
