nohup: ignoring input
2023-05-13 14:03:42.215506: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-13 14:03:43.193642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Got hdf Criteo-8d data set, getting metadata...
Initialization finished!
39
field-1: dim-15
field-2: dim-12
field-3: dim-4
field-4: dim-17
field-5: dim-4
field-6: dim-4
field-7: dim-4
field-8: dim-17
field-9: dim-2
field-10: dim-4
field-11: dim-12
field-12: dim-24
field-13: dim-13
field-14: dim-8
field-15: dim-24
field-16: dim-25
field-19: dim-4
field-20: dim-24
field-21: dim-16
field-22: dim-16
field-23: dim-17
field-24: dim-25
field-25: dim-21
field-26: dim-17
field-27: dim-8
field-28: dim-23
field-29: dim-4
field-30: dim-8
field-32: dim-16
field-33: dim-4
field-34: dim-21
field-35: dim-8
field-36: dim-23
field-37: dim-23
field-38: dim-17
field-39: dim-23
full_emb_for_0_emb_size_15 initialized from: -0.03691067352627811 0.03691067352627811
full_emb_for_1_emb_size_12 initialized from: -0.02736561135755131 0.02736561135755131
full_emb_for_2_emb_size_4 initialized from: -0.13423121104280486 0.13423121104280486
full_emb_for_3_emb_size_17 initialized from: -0.028380931014817347 0.028380931014817347
full_emb_for_4_emb_size_4 initialized from: -0.04758309514308865 0.04758309514308865
full_emb_for_5_emb_size_4 initialized from: -0.11785113019775792 0.11785113019775792
full_emb_for_6_emb_size_4 initialized from: -0.15911145683514602 0.15911145683514602
full_emb_for_7_emb_size_17 initialized from: -0.030816677568068284 0.030816677568068284
full_emb_for_8_emb_size_2 initialized from: -0.1421338109037403 0.1421338109037403
full_emb_for_9_emb_size_4 initialized from: -0.6324555320336759 0.6324555320336759
full_emb_for_10_emb_size_12 initialized from: -0.18009006755629928 0.18009006755629928
full_emb_for_11_emb_size_24 initialized from: -0.0058277261698637135 0.0058277261698637135
full_emb_for_12_emb_size_13 initialized from: -0.10016708449412667 0.10016708449412667
full_emb_for_13_emb_size_8 initialized from: -0.0063860510691885145 0.0063860510691885145
full_emb_for_14_emb_size_24 initialized from: -0.01737751292933602 0.01737751292933602
full_emb_for_15_emb_size_25 initialized from: -0.020097373193773395 0.020097373193773395
full_emb_for_18_emb_size_4 initialized from: -0.8660254037844386 0.8660254037844386
full_emb_for_19_emb_size_24 initialized from: -0.029992502811328637 0.029992502811328637
full_emb_for_20_emb_size_16 initialized from: -0.0682523632789935 0.0682523632789935
full_emb_for_21_emb_size_16 initialized from: -0.3110855084191276 0.3110855084191276
full_emb_for_22_emb_size_17 initialized from: -0.0065209225253894925 0.0065209225253894925
full_emb_for_23_emb_size_25 initialized from: -0.009651892169482854 0.009651892169482854
full_emb_for_24_emb_size_21 initialized from: -0.009704241543027231 0.009704241543027231
full_emb_for_25_emb_size_17 initialized from: -0.4629100498862757 0.4629100498862757
full_emb_for_26_emb_size_8 initialized from: -0.052655894762455135 0.052655894762455135
full_emb_for_27_emb_size_23 initialized from: -0.027683594464555476 0.027683594464555476
full_emb_for_28_emb_size_4 initialized from: -0.3038218101251 0.3038218101251
full_emb_for_29_emb_size_8 initialized from: -0.6793662204867574 0.6793662204867574
full_emb_for_31_emb_size_16 initialized from: -0.43994134506405985 0.43994134506405985
full_emb_for_32_emb_size_4 initialized from: -0.006380285938708977 0.006380285938708977
full_emb_for_33_emb_size_21 initialized from: -0.007181062370267826 0.007181062370267826
full_emb_for_34_emb_size_8 initialized from: -0.006418481711859752 0.006418481711859752
full_emb_for_35_emb_size_23 initialized from: -0.010241025486591067 0.010241025486591067
full_emb_for_36_emb_size_23 initialized from: -0.02535915646704869 0.02535915646704869
full_emb_for_37_emb_size_17 initialized from: -0.27386127875258304 0.27386127875258304
full_emb_for_38_emb_size_23 initialized from: -0.3244428422615251 0.3244428422615251
full_emb_for_0_emb_size_1 initialized from: -0.03696948196568264 0.03696948196568264
full_emb_for_1_emb_size_1 initialized from: -0.02738441640271498 0.02738441640271498
full_emb_for_2_emb_size_1 initialized from: -0.13483997249264842 0.13483997249264842
full_emb_for_3_emb_size_1 initialized from: -0.02841146046402596 0.02841146046402596
full_emb_for_4_emb_size_1 initialized from: -0.04761005186046748 0.04761005186046748
full_emb_for_5_emb_size_1 initialized from: -0.11826247919781652 0.11826247919781652
full_emb_for_6_emb_size_1 initialized from: -0.16012815380508713 0.16012815380508713
full_emb_for_7_emb_size_1 initialized from: -0.03085577263937756 0.03085577263937756
full_emb_for_8_emb_size_1 initialized from: -0.14237369936287486 0.14237369936287486
full_emb_for_9_emb_size_1 initialized from: -0.7071067811865476 0.7071067811865476
full_emb_for_10_emb_size_1 initialized from: -0.18569533817705186 0.18569533817705186
full_emb_for_11_emb_size_1 initialized from: -0.005828105560326563 0.005828105560326563
full_emb_for_12_emb_size_1 initialized from: -0.10118748860323272 0.10118748860323272
full_emb_for_13_emb_size_1 initialized from: -0.0063862029942614255 0.0063862029942614255
full_emb_for_14_emb_size_1 initialized from: -0.017387579619448895 0.017387579619448895
full_emb_for_15_emb_size_1 initialized from: -0.020113627727557096 0.020113627727557096
full_emb_for_16_emb_size_1 initialized from: -0.02945213193905299 0.02945213193905299
full_emb_for_17_emb_size_1 initialized from: -0.01791819309628112 0.01791819309628112
full_emb_for_18_emb_size_1 initialized from: -1.0954451150103321 1.0954451150103321
full_emb_for_19_emb_size_1 initialized from: -0.03004434814442642 0.03004434814442642
full_emb_for_20_emb_size_1 initialized from: -0.06865330091576084 0.06865330091576084
full_emb_for_21_emb_size_1 initialized from: -0.3572948005052482 0.3572948005052482
full_emb_for_22_emb_size_1 initialized from: -0.0065212922708019596 0.0065212922708019596
full_emb_for_23_emb_size_1 initialized from: -0.009653690993907272 0.009653690993907272
full_emb_for_24_emb_size_1 initialized from: -0.00970576501968314 0.00970576501968314
full_emb_for_25_emb_size_1 initialized from: -0.7071067811865476 0.7071067811865476
full_emb_for_26_emb_size_1 initialized from: -0.052741266274988985 0.052741266274988985
full_emb_for_27_emb_size_1 initialized from: -0.027722572984866366 0.027722572984866366
full_emb_for_28_emb_size_1 initialized from: -0.3110855084191276 0.3110855084191276
full_emb_for_29_emb_size_1 initialized from: -1.0 1.0
full_emb_for_30_emb_size_1 initialized from: -0.0803651515820456 0.0803651515820456
full_emb_for_31_emb_size_1 initialized from: -0.6123724356957945 0.6123724356957945
full_emb_for_32_emb_size_1 initialized from: -0.006380350871947833 0.006380350871947833
full_emb_for_33_emb_size_1 initialized from: -0.007181679634111962 0.007181679634111962
full_emb_for_34_emb_size_1 initialized from: -0.00641863596335187 0.00641863596335187
full_emb_for_35_emb_size_1 initialized from: -0.010242995172850529 0.010242995172850529
full_emb_for_36_emb_size_1 initialized from: -0.025389107701102583 0.025389107701102583
full_emb_for_37_emb_size_1 initialized from: -0.30618621784789724 0.30618621784789724
full_emb_for_38_emb_size_1 initialized from: -0.4140393356054125 0.4140393356054125
w_0 initialized from: -0.07050533657462563 0.07050533657462563
(507, 700) (700,)
relu
w_1 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_2 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_3 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_4 initialized from: -0.06546536707079771 0.06546536707079771
(700, 700) (700,)
relu
w_5 initialized from: -0.0925159507394634 0.0925159507394634
(700, 1) (1,)
none
[[34m2023-05-13 14:03:46[0m] Experiment directory created at /home/ubuntu/results/retrain_irazor/criteo/001-retrain_irazor
[[34m2023-05-13 14:03:46[0m] Batchsize: 1000
wandb: Currently logged in as: yao-yao. Use `wandb login --relogin` to force relogin
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path /workspace/wandb/wandb/ wasn't writable, using system temp directory
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /tmp/wandb/run-20230513_140347-b3ul517s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run criteo-BS-1000-001-retrain_irazor-2023-05-13 14:03:46
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yao-yao/irazor
wandb: üöÄ View run at https://wandb.ai/yao-yao/irazor/runs/b3ul517s
2023-05-13 14:03:52.817523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:52.820090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:52.821944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:54.927941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:54.929040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:54.929881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-13 14:03:54.930702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 32503 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:06:00.0, compute capability: 8.0
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
[[34m2023-05-13 14:03:54[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
add l2 0.001 Tensor("concat:0", shape=(?, 507), dtype=float32)
add l2 0.001 Tensor("concat_1:0", shape=(?, 39), dtype=float32)
WARNING:tensorflow:From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
[[34m2023-05-13 14:03:55[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2023-05-13 14:03:56.402034: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
[[34m2023-05-13 14:03:57[0m] total batches: 86890	batch per epoch: 8689
[[34m2023-05-13 14:03:57[0m] new iteration
new iteration
on disk...
2023-05-13 14:03:59.447596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
[[34m2023-05-13 14:05:00[0m] elapsed : 0:01:03, ETA : 1:30:11
[[34m2023-05-13 14:05:00[0m] epoch 1 / 10, batch 1000 / 8689, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.621023, l2 = 0.774117, auc = 0.748385
elapsed : 0:01:03, ETA : 1:30:11
epoch 1 / 10, batch 1000 / 8689, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.621023, l2 = 0.774117, auc = 0.748385
[[34m2023-05-13 14:06:02[0m] elapsed : 0:02:04, ETA : 1:27:43
[[34m2023-05-13 14:06:02[0m] epoch 1 / 10, batch 2000 / 8689, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.574789, l2 = 0.097250, auc = 0.767867
elapsed : 0:02:04, ETA : 1:27:43
epoch 1 / 10, batch 2000 / 8689, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.574789, l2 = 0.097250, auc = 0.767867
[[34m2023-05-13 14:07:03[0m] elapsed : 0:03:06, ETA : 1:26:41
[[34m2023-05-13 14:07:03[0m] epoch 1 / 10, batch 3000 / 8689, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.568792, l2 = 0.036362, auc = 0.774063
elapsed : 0:03:06, ETA : 1:26:41
epoch 1 / 10, batch 3000 / 8689, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.568792, l2 = 0.036362, auc = 0.774063
[[34m2023-05-13 14:08:04[0m] elapsed : 0:04:07, ETA : 1:25:18
[[34m2023-05-13 14:08:04[0m] epoch 1 / 10, batch 4000 / 8689, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.566847, l2 = 0.022033, auc = 0.775901
elapsed : 0:04:07, ETA : 1:25:18
epoch 1 / 10, batch 4000 / 8689, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.566847, l2 = 0.022033, auc = 0.775901
[[34m2023-05-13 14:09:06[0m] elapsed : 0:05:09, ETA : 1:24:20
[[34m2023-05-13 14:09:06[0m] epoch 1 / 10, batch 5000 / 8689, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.563791, l2 = 0.017674, auc = 0.779022
elapsed : 0:05:09, ETA : 1:24:20
epoch 1 / 10, batch 5000 / 8689, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.563791, l2 = 0.017674, auc = 0.779022
[[34m2023-05-13 14:10:06[0m] elapsed : 0:06:09, ETA : 1:22:54
[[34m2023-05-13 14:10:06[0m] epoch 1 / 10, batch 6000 / 8689, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.562220, l2 = 0.016039, auc = 0.780404
elapsed : 0:06:09, ETA : 1:22:54
epoch 1 / 10, batch 6000 / 8689, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.562220, l2 = 0.016039, auc = 0.780404
[[34m2023-05-13 14:11:09[0m] elapsed : 0:07:12, ETA : 1:22:10
[[34m2023-05-13 14:11:09[0m] epoch 1 / 10, batch 7000 / 8689, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.560823, l2 = 0.015258, auc = 0.781910
elapsed : 0:07:12, ETA : 1:22:10
epoch 1 / 10, batch 7000 / 8689, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.560823, l2 = 0.015258, auc = 0.781910
[[34m2023-05-13 14:12:11[0m] elapsed : 0:08:14, ETA : 1:21:11
[[34m2023-05-13 14:12:11[0m] epoch 1 / 10, batch 8000 / 8689, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.560545, l2 = 0.014825, auc = 0.782217
elapsed : 0:08:14, ETA : 1:21:11
epoch 1 / 10, batch 8000 / 8689, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.560545, l2 = 0.014825, auc = 0.782217
[[34m2023-05-13 14:12:53[0m] running test...
on disk...
[[34m2023-05-13 14:12:58[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-13 14:13:03[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:13:09[0m] evaluated batches: 3000, 0:00:05
[[34m2023-05-13 14:13:14[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:13:20[0m] evaluated batches: 5000, 0:00:05
[[34m2023-05-13 14:13:25[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:13:30[0m] evaluated batches: 7000, 0:00:05
[[34m2023-05-13 14:13:35[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:13:41[0m] evaluated batches: 9000, 0:00:05
[[34m2023-05-13 14:13:46[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:13:51[0m] evaluated batches: 11000, 0:00:05
[[34m2023-05-13 14:13:56[0m] evaluated batches: 12000, 0:00:05
[[34m2023-05-13 14:14:07[0m] test loss = 0.560637, test auc = 0.782888
[[34m2023-05-13 14:14:07[0m] evaluated time: 0:01:14
[[34m2023-05-13 14:14:07[0m] analyse_structure
[[34m2023-05-13 14:15:09[0m] elapsed : 0:11:12, ETA : 1:29:14
[[34m2023-05-13 14:15:09[0m] epoch 2 / 10, batch 1000 / 8689, global_step = 9689, learning_rate = 1.000000e-02, loss = 0.944590, l2 = 0.024202, auc = 0.783475
elapsed : 0:11:12, ETA : 1:29:14
epoch 2 / 10, batch 1000 / 8689, global_step = 9689, learning_rate = 1.000000e-02, loss = 0.944590, l2 = 0.024202, auc = 0.783475
[[34m2023-05-13 14:16:10[0m] elapsed : 0:12:13, ETA : 1:27:05
[[34m2023-05-13 14:16:10[0m] epoch 2 / 10, batch 2000 / 8689, global_step = 10689, learning_rate = 1.000000e-02, loss = 0.557647, l2 = 0.013972, auc = 0.784909
elapsed : 0:12:13, ETA : 1:27:05
epoch 2 / 10, batch 2000 / 8689, global_step = 10689, learning_rate = 1.000000e-02, loss = 0.557647, l2 = 0.013972, auc = 0.784909
[[34m2023-05-13 14:17:12[0m] elapsed : 0:13:15, ETA : 1:25:14
[[34m2023-05-13 14:17:12[0m] epoch 2 / 10, batch 3000 / 8689, global_step = 11689, learning_rate = 1.000000e-02, loss = 0.557707, l2 = 0.013744, auc = 0.784870
elapsed : 0:13:15, ETA : 1:25:14
epoch 2 / 10, batch 3000 / 8689, global_step = 11689, learning_rate = 1.000000e-02, loss = 0.557707, l2 = 0.013744, auc = 0.784870
[[34m2023-05-13 14:18:14[0m] elapsed : 0:14:17, ETA : 1:23:31
[[34m2023-05-13 14:18:14[0m] epoch 2 / 10, batch 4000 / 8689, global_step = 12689, learning_rate = 1.000000e-02, loss = 0.556527, l2 = 0.013563, auc = 0.786125
elapsed : 0:14:17, ETA : 1:23:31
epoch 2 / 10, batch 4000 / 8689, global_step = 12689, learning_rate = 1.000000e-02, loss = 0.556527, l2 = 0.013563, auc = 0.786125
[[34m2023-05-13 14:19:16[0m] elapsed : 0:15:19, ETA : 1:21:54
[[34m2023-05-13 14:19:16[0m] epoch 2 / 10, batch 5000 / 8689, global_step = 13689, learning_rate = 1.000000e-02, loss = 0.555680, l2 = 0.013381, auc = 0.787009
elapsed : 0:15:19, ETA : 1:21:54
epoch 2 / 10, batch 5000 / 8689, global_step = 13689, learning_rate = 1.000000e-02, loss = 0.555680, l2 = 0.013381, auc = 0.787009
[[34m2023-05-13 14:20:19[0m] elapsed : 0:16:22, ETA : 1:20:26
[[34m2023-05-13 14:20:19[0m] epoch 2 / 10, batch 6000 / 8689, global_step = 14689, learning_rate = 1.000000e-02, loss = 0.555927, l2 = 0.013240, auc = 0.786677
elapsed : 0:16:22, ETA : 1:20:26
epoch 2 / 10, batch 6000 / 8689, global_step = 14689, learning_rate = 1.000000e-02, loss = 0.555927, l2 = 0.013240, auc = 0.786677
[[34m2023-05-13 14:21:19[0m] elapsed : 0:17:22, ETA : 1:18:48
[[34m2023-05-13 14:21:19[0m] epoch 2 / 10, batch 7000 / 8689, global_step = 15689, learning_rate = 1.000000e-02, loss = 0.556081, l2 = 0.013049, auc = 0.786523
elapsed : 0:17:22, ETA : 1:18:48
epoch 2 / 10, batch 7000 / 8689, global_step = 15689, learning_rate = 1.000000e-02, loss = 0.556081, l2 = 0.013049, auc = 0.786523
[[34m2023-05-13 14:22:20[0m] elapsed : 0:18:23, ETA : 1:17:19
[[34m2023-05-13 14:22:20[0m] epoch 2 / 10, batch 8000 / 8689, global_step = 16689, learning_rate = 1.000000e-02, loss = 0.554598, l2 = 0.012900, auc = 0.788069
elapsed : 0:18:23, ETA : 1:17:19
epoch 2 / 10, batch 8000 / 8689, global_step = 16689, learning_rate = 1.000000e-02, loss = 0.554598, l2 = 0.012900, auc = 0.788069
[[34m2023-05-13 14:23:04[0m] running test...
on disk...
[[34m2023-05-13 14:23:10[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-13 14:23:15[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:23:20[0m] evaluated batches: 3000, 0:00:05
[[34m2023-05-13 14:23:25[0m] evaluated batches: 4000, 0:00:05
[[34m2023-05-13 14:23:31[0m] evaluated batches: 5000, 0:00:05
[[34m2023-05-13 14:23:36[0m] evaluated batches: 6000, 0:00:05
[[34m2023-05-13 14:23:41[0m] evaluated batches: 7000, 0:00:05
[[34m2023-05-13 14:23:45[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:23:51[0m] evaluated batches: 9000, 0:00:05
[[34m2023-05-13 14:23:56[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:24:01[0m] evaluated batches: 11000, 0:00:05
[[34m2023-05-13 14:24:06[0m] evaluated batches: 12000, 0:00:05
[[34m2023-05-13 14:24:17[0m] test loss = 0.555020, test auc = 0.787363
[[34m2023-05-13 14:24:17[0m] evaluated time: 0:01:13
[[34m2023-05-13 14:24:17[0m] analyse_structure
[[34m2023-05-13 14:25:19[0m] elapsed : 0:21:21, ETA : 1:19:35
[[34m2023-05-13 14:25:19[0m] epoch 3 / 10, batch 1000 / 8689, global_step = 18378, learning_rate = 1.000000e-02, loss = 0.936480, l2 = 0.021453, auc = 0.788042
elapsed : 0:21:21, ETA : 1:19:35
epoch 3 / 10, batch 1000 / 8689, global_step = 18378, learning_rate = 1.000000e-02, loss = 0.936480, l2 = 0.021453, auc = 0.788042
[[34m2023-05-13 14:26:20[0m] elapsed : 0:22:23, ETA : 1:17:58
[[34m2023-05-13 14:26:20[0m] epoch 3 / 10, batch 2000 / 8689, global_step = 19378, learning_rate = 1.000000e-02, loss = 0.553327, l2 = 0.012554, auc = 0.789162
elapsed : 0:22:23, ETA : 1:17:58
epoch 3 / 10, batch 2000 / 8689, global_step = 19378, learning_rate = 1.000000e-02, loss = 0.553327, l2 = 0.012554, auc = 0.789162
[[34m2023-05-13 14:27:22[0m] elapsed : 0:23:24, ETA : 1:16:22
[[34m2023-05-13 14:27:22[0m] epoch 3 / 10, batch 3000 / 8689, global_step = 20378, learning_rate = 1.000000e-02, loss = 0.553524, l2 = 0.012475, auc = 0.788846
elapsed : 0:23:24, ETA : 1:16:22
epoch 3 / 10, batch 3000 / 8689, global_step = 20378, learning_rate = 1.000000e-02, loss = 0.553524, l2 = 0.012475, auc = 0.788846
[[34m2023-05-13 14:28:23[0m] elapsed : 0:24:26, ETA : 1:14:52
[[34m2023-05-13 14:28:23[0m] epoch 3 / 10, batch 4000 / 8689, global_step = 21378, learning_rate = 1.000000e-02, loss = 0.552794, l2 = 0.012352, auc = 0.789600
elapsed : 0:24:26, ETA : 1:14:52
epoch 3 / 10, batch 4000 / 8689, global_step = 21378, learning_rate = 1.000000e-02, loss = 0.552794, l2 = 0.012352, auc = 0.789600
[[34m2023-05-13 14:29:25[0m] elapsed : 0:25:28, ETA : 1:13:24
[[34m2023-05-13 14:29:25[0m] epoch 3 / 10, batch 5000 / 8689, global_step = 22378, learning_rate = 1.000000e-02, loss = 0.552919, l2 = 0.012220, auc = 0.789645
elapsed : 0:25:28, ETA : 1:13:24
epoch 3 / 10, batch 5000 / 8689, global_step = 22378, learning_rate = 1.000000e-02, loss = 0.552919, l2 = 0.012220, auc = 0.789645
[[34m2023-05-13 14:30:28[0m] elapsed : 0:26:31, ETA : 1:12:02
[[34m2023-05-13 14:30:28[0m] epoch 3 / 10, batch 6000 / 8689, global_step = 23378, learning_rate = 1.000000e-02, loss = 0.553328, l2 = 0.012150, auc = 0.789175
elapsed : 0:26:31, ETA : 1:12:02
epoch 3 / 10, batch 6000 / 8689, global_step = 23378, learning_rate = 1.000000e-02, loss = 0.553328, l2 = 0.012150, auc = 0.789175
[[34m2023-05-13 14:31:29[0m] elapsed : 0:27:31, ETA : 1:10:33
[[34m2023-05-13 14:31:29[0m] epoch 3 / 10, batch 7000 / 8689, global_step = 24378, learning_rate = 1.000000e-02, loss = 0.551937, l2 = 0.012041, auc = 0.790524
elapsed : 0:27:31, ETA : 1:10:33
epoch 3 / 10, batch 7000 / 8689, global_step = 24378, learning_rate = 1.000000e-02, loss = 0.551937, l2 = 0.012041, auc = 0.790524
[[34m2023-05-13 14:32:30[0m] elapsed : 0:28:33, ETA : 1:09:12
[[34m2023-05-13 14:32:30[0m] epoch 3 / 10, batch 8000 / 8689, global_step = 25378, learning_rate = 1.000000e-02, loss = 0.551613, l2 = 0.011974, auc = 0.790791
elapsed : 0:28:33, ETA : 1:09:12
epoch 3 / 10, batch 8000 / 8689, global_step = 25378, learning_rate = 1.000000e-02, loss = 0.551613, l2 = 0.011974, auc = 0.790791
[[34m2023-05-13 14:33:12[0m] running test...
on disk...
[[34m2023-05-13 14:33:18[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-13 14:33:22[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:33:28[0m] evaluated batches: 3000, 0:00:05
[[34m2023-05-13 14:33:33[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:33:38[0m] evaluated batches: 5000, 0:00:05
[[34m2023-05-13 14:33:43[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:33:49[0m] evaluated batches: 7000, 0:00:05
[[34m2023-05-13 14:33:54[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:33:59[0m] evaluated batches: 9000, 0:00:05
[[34m2023-05-13 14:34:04[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:34:10[0m] evaluated batches: 11000, 0:00:05
[[34m2023-05-13 14:34:15[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 14:34:26[0m] test loss = 0.553738, test auc = 0.789411
[[34m2023-05-13 14:34:26[0m] evaluated time: 0:01:13
[[34m2023-05-13 14:34:26[0m] analyse_structure
[[34m2023-05-13 14:35:27[0m] elapsed : 0:31:30, ETA : 1:09:37
[[34m2023-05-13 14:35:27[0m] epoch 4 / 10, batch 1000 / 8689, global_step = 27067, learning_rate = 1.000000e-02, loss = 0.931121, l2 = 0.019979, auc = 0.791009
elapsed : 0:31:30, ETA : 1:09:37
epoch 4 / 10, batch 1000 / 8689, global_step = 27067, learning_rate = 1.000000e-02, loss = 0.931121, l2 = 0.019979, auc = 0.791009
[[34m2023-05-13 14:36:30[0m] elapsed : 0:32:33, ETA : 1:08:13
[[34m2023-05-13 14:36:30[0m] epoch 4 / 10, batch 2000 / 8689, global_step = 28067, learning_rate = 1.000000e-02, loss = 0.550766, l2 = 0.011692, auc = 0.791420
elapsed : 0:32:33, ETA : 1:08:13
epoch 4 / 10, batch 2000 / 8689, global_step = 28067, learning_rate = 1.000000e-02, loss = 0.550766, l2 = 0.011692, auc = 0.791420
[[34m2023-05-13 14:37:32[0m] elapsed : 0:33:35, ETA : 1:06:48
[[34m2023-05-13 14:37:32[0m] epoch 4 / 10, batch 3000 / 8689, global_step = 29067, learning_rate = 1.000000e-02, loss = 0.551005, l2 = 0.011652, auc = 0.791353
elapsed : 0:33:35, ETA : 1:06:48
epoch 4 / 10, batch 3000 / 8689, global_step = 29067, learning_rate = 1.000000e-02, loss = 0.551005, l2 = 0.011652, auc = 0.791353
[[34m2023-05-13 14:38:34[0m] elapsed : 0:34:37, ETA : 1:05:25
[[34m2023-05-13 14:38:34[0m] epoch 4 / 10, batch 4000 / 8689, global_step = 30067, learning_rate = 1.000000e-02, loss = 0.551172, l2 = 0.011561, auc = 0.791095
elapsed : 0:34:37, ETA : 1:05:25
epoch 4 / 10, batch 4000 / 8689, global_step = 30067, learning_rate = 1.000000e-02, loss = 0.551172, l2 = 0.011561, auc = 0.791095
[[34m2023-05-13 14:39:37[0m] elapsed : 0:35:39, ETA : 1:04:03
[[34m2023-05-13 14:39:37[0m] epoch 4 / 10, batch 5000 / 8689, global_step = 31067, learning_rate = 1.000000e-02, loss = 0.551289, l2 = 0.011484, auc = 0.790991
elapsed : 0:35:39, ETA : 1:04:03
epoch 4 / 10, batch 5000 / 8689, global_step = 31067, learning_rate = 1.000000e-02, loss = 0.551289, l2 = 0.011484, auc = 0.790991
[[34m2023-05-13 14:40:37[0m] elapsed : 0:36:40, ETA : 1:02:41
[[34m2023-05-13 14:40:37[0m] epoch 4 / 10, batch 6000 / 8689, global_step = 32067, learning_rate = 1.000000e-02, loss = 0.550989, l2 = 0.011438, auc = 0.791280
elapsed : 0:36:40, ETA : 1:02:41
epoch 4 / 10, batch 6000 / 8689, global_step = 32067, learning_rate = 1.000000e-02, loss = 0.550989, l2 = 0.011438, auc = 0.791280
[[34m2023-05-13 14:41:38[0m] elapsed : 0:37:40, ETA : 1:01:18
[[34m2023-05-13 14:41:38[0m] epoch 4 / 10, batch 7000 / 8689, global_step = 33067, learning_rate = 1.000000e-02, loss = 0.550404, l2 = 0.011345, auc = 0.791944
elapsed : 0:37:40, ETA : 1:01:18
epoch 4 / 10, batch 7000 / 8689, global_step = 33067, learning_rate = 1.000000e-02, loss = 0.550404, l2 = 0.011345, auc = 0.791944
[[34m2023-05-13 14:42:42[0m] elapsed : 0:38:45, ETA : 1:00:05
[[34m2023-05-13 14:42:42[0m] epoch 4 / 10, batch 8000 / 8689, global_step = 34067, learning_rate = 1.000000e-02, loss = 0.549609, l2 = 0.011301, auc = 0.792631
elapsed : 0:38:45, ETA : 1:00:05
epoch 4 / 10, batch 8000 / 8689, global_step = 34067, learning_rate = 1.000000e-02, loss = 0.549609, l2 = 0.011301, auc = 0.792631
[[34m2023-05-13 14:43:26[0m] running test...
on disk...
[[34m2023-05-13 14:43:31[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-13 14:43:36[0m] evaluated batches: 2000, 0:00:05
[[34m2023-05-13 14:43:41[0m] evaluated batches: 3000, 0:00:04
[[34m2023-05-13 14:43:46[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:43:51[0m] evaluated batches: 5000, 0:00:05
[[34m2023-05-13 14:43:56[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:44:02[0m] evaluated batches: 7000, 0:00:05
[[34m2023-05-13 14:44:07[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:44:12[0m] evaluated batches: 9000, 0:00:05
[[34m2023-05-13 14:44:17[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:44:22[0m] evaluated batches: 11000, 0:00:05
[[34m2023-05-13 14:44:27[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 14:44:38[0m] test loss = 0.551779, test auc = 0.791219
[[34m2023-05-13 14:44:38[0m] evaluated time: 0:01:12
[[34m2023-05-13 14:44:38[0m] analyse_structure
[[34m2023-05-13 14:45:40[0m] elapsed : 0:41:42, ETA : 0:59:38
[[34m2023-05-13 14:45:40[0m] epoch 5 / 10, batch 1000 / 8689, global_step = 35756, learning_rate = 1.000000e-02, loss = 0.928708, l2 = 0.018924, auc = 0.792301
elapsed : 0:41:42, ETA : 0:59:38
epoch 5 / 10, batch 1000 / 8689, global_step = 35756, learning_rate = 1.000000e-02, loss = 0.928708, l2 = 0.018924, auc = 0.792301
[[34m2023-05-13 14:46:41[0m] elapsed : 0:42:44, ETA : 0:58:17
[[34m2023-05-13 14:46:41[0m] epoch 5 / 10, batch 2000 / 8689, global_step = 36756, learning_rate = 1.000000e-02, loss = 0.549067, l2 = 0.011124, auc = 0.793037
elapsed : 0:42:44, ETA : 0:58:17
epoch 5 / 10, batch 2000 / 8689, global_step = 36756, learning_rate = 1.000000e-02, loss = 0.549067, l2 = 0.011124, auc = 0.793037
[[34m2023-05-13 14:47:43[0m] elapsed : 0:43:46, ETA : 0:56:57
[[34m2023-05-13 14:47:43[0m] epoch 5 / 10, batch 3000 / 8689, global_step = 37756, learning_rate = 1.000000e-02, loss = 0.549537, l2 = 0.011088, auc = 0.792684
elapsed : 0:43:46, ETA : 0:56:57
epoch 5 / 10, batch 3000 / 8689, global_step = 37756, learning_rate = 1.000000e-02, loss = 0.549537, l2 = 0.011088, auc = 0.792684
[[34m2023-05-13 14:48:45[0m] elapsed : 0:44:48, ETA : 0:55:38
[[34m2023-05-13 14:48:45[0m] epoch 5 / 10, batch 4000 / 8689, global_step = 38756, learning_rate = 1.000000e-02, loss = 0.548337, l2 = 0.011026, auc = 0.793827
elapsed : 0:44:48, ETA : 0:55:38
epoch 5 / 10, batch 4000 / 8689, global_step = 38756, learning_rate = 1.000000e-02, loss = 0.548337, l2 = 0.011026, auc = 0.793827
[[34m2023-05-13 14:49:47[0m] elapsed : 0:45:50, ETA : 0:54:20
[[34m2023-05-13 14:49:47[0m] epoch 5 / 10, batch 5000 / 8689, global_step = 39756, learning_rate = 1.000000e-02, loss = 0.549042, l2 = 0.010991, auc = 0.793225
elapsed : 0:45:50, ETA : 0:54:20
epoch 5 / 10, batch 5000 / 8689, global_step = 39756, learning_rate = 1.000000e-02, loss = 0.549042, l2 = 0.010991, auc = 0.793225
[[34m2023-05-13 14:50:48[0m] elapsed : 0:46:51, ETA : 0:53:01
[[34m2023-05-13 14:50:48[0m] epoch 5 / 10, batch 6000 / 8689, global_step = 40756, learning_rate = 1.000000e-02, loss = 0.548738, l2 = 0.010928, auc = 0.793334
elapsed : 0:46:51, ETA : 0:53:01
epoch 5 / 10, batch 6000 / 8689, global_step = 40756, learning_rate = 1.000000e-02, loss = 0.548738, l2 = 0.010928, auc = 0.793334
[[34m2023-05-13 14:51:49[0m] elapsed : 0:47:52, ETA : 0:51:44
[[34m2023-05-13 14:51:49[0m] epoch 5 / 10, batch 7000 / 8689, global_step = 41756, learning_rate = 1.000000e-02, loss = 0.549430, l2 = 0.010876, auc = 0.792729
elapsed : 0:47:52, ETA : 0:51:44
epoch 5 / 10, batch 7000 / 8689, global_step = 41756, learning_rate = 1.000000e-02, loss = 0.549430, l2 = 0.010876, auc = 0.792729
[[34m2023-05-13 14:52:51[0m] elapsed : 0:48:54, ETA : 0:50:28
[[34m2023-05-13 14:52:51[0m] epoch 5 / 10, batch 8000 / 8689, global_step = 42756, learning_rate = 1.000000e-02, loss = 0.548719, l2 = 0.010848, auc = 0.793395
elapsed : 0:48:54, ETA : 0:50:28
epoch 5 / 10, batch 8000 / 8689, global_step = 42756, learning_rate = 1.000000e-02, loss = 0.548719, l2 = 0.010848, auc = 0.793395
[[34m2023-05-13 14:53:33[0m] running test...
on disk...
[[34m2023-05-13 14:53:39[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-13 14:53:44[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 14:53:49[0m] evaluated batches: 3000, 0:00:05
[[34m2023-05-13 14:53:54[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 14:54:00[0m] evaluated batches: 5000, 0:00:05
[[34m2023-05-13 14:54:04[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 14:54:10[0m] evaluated batches: 7000, 0:00:05
[[34m2023-05-13 14:54:15[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 14:54:20[0m] evaluated batches: 9000, 0:00:05
[[34m2023-05-13 14:54:25[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 14:54:30[0m] evaluated batches: 11000, 0:00:05
[[34m2023-05-13 14:54:35[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 14:54:46[0m] test loss = 0.549856, test auc = 0.792445
[[34m2023-05-13 14:54:46[0m] evaluated time: 0:01:12
[[34m2023-05-13 14:54:46[0m] analyse_structure
[[34m2023-05-13 14:55:49[0m] elapsed : 0:51:52, ETA : 0:49:31
[[34m2023-05-13 14:55:49[0m] epoch 6 / 10, batch 1000 / 8689, global_step = 44445, learning_rate = 1.000000e-02, loss = 0.926769, l2 = 0.018192, auc = 0.793401
elapsed : 0:51:52, ETA : 0:49:31
epoch 6 / 10, batch 1000 / 8689, global_step = 44445, learning_rate = 1.000000e-02, loss = 0.926769, l2 = 0.018192, auc = 0.793401
[[34m2023-05-13 14:56:52[0m] elapsed : 0:52:55, ETA : 0:48:15
[[34m2023-05-13 14:56:52[0m] epoch 6 / 10, batch 2000 / 8689, global_step = 45445, learning_rate = 1.000000e-02, loss = 0.548696, l2 = 0.010684, auc = 0.793325
elapsed : 0:52:55, ETA : 0:48:15
epoch 6 / 10, batch 2000 / 8689, global_step = 45445, learning_rate = 1.000000e-02, loss = 0.548696, l2 = 0.010684, auc = 0.793325
[[34m2023-05-13 14:57:55[0m] elapsed : 0:53:58, ETA : 0:46:59
[[34m2023-05-13 14:57:55[0m] epoch 6 / 10, batch 3000 / 8689, global_step = 46445, learning_rate = 1.000000e-02, loss = 0.547872, l2 = 0.010649, auc = 0.794159
elapsed : 0:53:58, ETA : 0:46:59
epoch 6 / 10, batch 3000 / 8689, global_step = 46445, learning_rate = 1.000000e-02, loss = 0.547872, l2 = 0.010649, auc = 0.794159
[[34m2023-05-13 14:58:55[0m] elapsed : 0:54:58, ETA : 0:45:41
[[34m2023-05-13 14:58:55[0m] epoch 6 / 10, batch 4000 / 8689, global_step = 47445, learning_rate = 1.000000e-02, loss = 0.548261, l2 = 0.010590, auc = 0.793759
elapsed : 0:54:58, ETA : 0:45:41
epoch 6 / 10, batch 4000 / 8689, global_step = 47445, learning_rate = 1.000000e-02, loss = 0.548261, l2 = 0.010590, auc = 0.793759
[[34m2023-05-13 14:59:57[0m] elapsed : 0:56:00, ETA : 0:44:26
[[34m2023-05-13 14:59:57[0m] epoch 6 / 10, batch 5000 / 8689, global_step = 48445, learning_rate = 1.000000e-02, loss = 0.548472, l2 = 0.010572, auc = 0.793642
elapsed : 0:56:00, ETA : 0:44:26
epoch 6 / 10, batch 5000 / 8689, global_step = 48445, learning_rate = 1.000000e-02, loss = 0.548472, l2 = 0.010572, auc = 0.793642
[[34m2023-05-13 15:00:58[0m] elapsed : 0:57:00, ETA : 0:43:09
[[34m2023-05-13 15:00:58[0m] epoch 6 / 10, batch 6000 / 8689, global_step = 49445, learning_rate = 1.000000e-02, loss = 0.548043, l2 = 0.010481, auc = 0.794016
elapsed : 0:57:00, ETA : 0:43:09
epoch 6 / 10, batch 6000 / 8689, global_step = 49445, learning_rate = 1.000000e-02, loss = 0.548043, l2 = 0.010481, auc = 0.794016
[[34m2023-05-13 15:02:00[0m] elapsed : 0:58:03, ETA : 0:41:56
[[34m2023-05-13 15:02:00[0m] epoch 6 / 10, batch 7000 / 8689, global_step = 50445, learning_rate = 1.000000e-02, loss = 0.547155, l2 = 0.010457, auc = 0.794815
elapsed : 0:58:03, ETA : 0:41:56
epoch 6 / 10, batch 7000 / 8689, global_step = 50445, learning_rate = 1.000000e-02, loss = 0.547155, l2 = 0.010457, auc = 0.794815
[[34m2023-05-13 15:03:04[0m] elapsed : 0:59:06, ETA : 0:40:43
[[34m2023-05-13 15:03:04[0m] epoch 6 / 10, batch 8000 / 8689, global_step = 51445, learning_rate = 1.000000e-02, loss = 0.547880, l2 = 0.010429, auc = 0.794146
elapsed : 0:59:06, ETA : 0:40:43
epoch 6 / 10, batch 8000 / 8689, global_step = 51445, learning_rate = 1.000000e-02, loss = 0.547880, l2 = 0.010429, auc = 0.794146
[[34m2023-05-13 15:03:45[0m] running test...
on disk...
[[34m2023-05-13 15:03:50[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-13 15:03:55[0m] evaluated batches: 2000, 0:00:05
[[34m2023-05-13 15:04:01[0m] evaluated batches: 3000, 0:00:05
[[34m2023-05-13 15:04:06[0m] evaluated batches: 4000, 0:00:05
[[34m2023-05-13 15:04:11[0m] evaluated batches: 5000, 0:00:05
[[34m2023-05-13 15:04:16[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 15:04:22[0m] evaluated batches: 7000, 0:00:05
[[34m2023-05-13 15:04:27[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 15:04:32[0m] evaluated batches: 9000, 0:00:05
[[34m2023-05-13 15:04:37[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 15:04:43[0m] evaluated batches: 11000, 0:00:05
[[34m2023-05-13 15:04:48[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 15:04:59[0m] test loss = 0.548495, test auc = 0.793362
[[34m2023-05-13 15:04:59[0m] evaluated time: 0:01:13
[[34m2023-05-13 15:04:59[0m] analyse_structure
[[34m2023-05-13 15:06:00[0m] elapsed : 1:02:03, ETA : 0:39:25
[[34m2023-05-13 15:06:00[0m] epoch 7 / 10, batch 1000 / 8689, global_step = 53134, learning_rate = 1.000000e-02, loss = 0.924472, l2 = 0.017491, auc = 0.794572
elapsed : 1:02:03, ETA : 0:39:25
epoch 7 / 10, batch 1000 / 8689, global_step = 53134, learning_rate = 1.000000e-02, loss = 0.924472, l2 = 0.017491, auc = 0.794572
[[34m2023-05-13 15:07:02[0m] elapsed : 1:03:05, ETA : 0:38:10
[[34m2023-05-13 15:07:02[0m] epoch 7 / 10, batch 2000 / 8689, global_step = 54134, learning_rate = 1.000000e-02, loss = 0.547432, l2 = 0.010306, auc = 0.794481
elapsed : 1:03:05, ETA : 0:38:10
epoch 7 / 10, batch 2000 / 8689, global_step = 54134, learning_rate = 1.000000e-02, loss = 0.547432, l2 = 0.010306, auc = 0.794481
[[34m2023-05-13 15:08:04[0m] elapsed : 1:04:07, ETA : 0:36:55
[[34m2023-05-13 15:08:04[0m] epoch 7 / 10, batch 3000 / 8689, global_step = 55134, learning_rate = 1.000000e-02, loss = 0.546110, l2 = 0.010268, auc = 0.795895
elapsed : 1:04:07, ETA : 0:36:55
epoch 7 / 10, batch 3000 / 8689, global_step = 55134, learning_rate = 1.000000e-02, loss = 0.546110, l2 = 0.010268, auc = 0.795895
[[34m2023-05-13 15:09:08[0m] elapsed : 1:05:11, ETA : 0:35:42
[[34m2023-05-13 15:09:08[0m] epoch 7 / 10, batch 4000 / 8689, global_step = 56134, learning_rate = 1.000000e-02, loss = 0.547550, l2 = 0.010239, auc = 0.794483
elapsed : 1:05:11, ETA : 0:35:42
epoch 7 / 10, batch 4000 / 8689, global_step = 56134, learning_rate = 1.000000e-02, loss = 0.547550, l2 = 0.010239, auc = 0.794483
[[34m2023-05-13 15:10:08[0m] elapsed : 1:06:11, ETA : 0:34:28
[[34m2023-05-13 15:10:08[0m] epoch 7 / 10, batch 5000 / 8689, global_step = 57134, learning_rate = 1.000000e-02, loss = 0.545949, l2 = 0.010226, auc = 0.795893
elapsed : 1:06:11, ETA : 0:34:28
epoch 7 / 10, batch 5000 / 8689, global_step = 57134, learning_rate = 1.000000e-02, loss = 0.545949, l2 = 0.010226, auc = 0.795893
[[34m2023-05-13 15:11:10[0m] elapsed : 1:07:13, ETA : 0:33:14
[[34m2023-05-13 15:11:10[0m] epoch 7 / 10, batch 6000 / 8689, global_step = 58134, learning_rate = 1.000000e-02, loss = 0.547198, l2 = 0.010165, auc = 0.794708
elapsed : 1:07:13, ETA : 0:33:14
epoch 7 / 10, batch 6000 / 8689, global_step = 58134, learning_rate = 1.000000e-02, loss = 0.547198, l2 = 0.010165, auc = 0.794708
[[34m2023-05-13 15:12:11[0m] elapsed : 1:08:14, ETA : 0:32:01
[[34m2023-05-13 15:12:11[0m] epoch 7 / 10, batch 7000 / 8689, global_step = 59134, learning_rate = 1.000000e-02, loss = 0.546841, l2 = 0.010121, auc = 0.795096
elapsed : 1:08:14, ETA : 0:32:01
epoch 7 / 10, batch 7000 / 8689, global_step = 59134, learning_rate = 1.000000e-02, loss = 0.546841, l2 = 0.010121, auc = 0.795096
[[34m2023-05-13 15:13:13[0m] elapsed : 1:09:15, ETA : 0:30:48
[[34m2023-05-13 15:13:13[0m] epoch 7 / 10, batch 8000 / 8689, global_step = 60134, learning_rate = 1.000000e-02, loss = 0.547219, l2 = 0.010108, auc = 0.794698
elapsed : 1:09:15, ETA : 0:30:48
epoch 7 / 10, batch 8000 / 8689, global_step = 60134, learning_rate = 1.000000e-02, loss = 0.547219, l2 = 0.010108, auc = 0.794698
[[34m2023-05-13 15:13:55[0m] running test...
on disk...
[[34m2023-05-13 15:14:01[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-13 15:14:06[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 15:14:11[0m] evaluated batches: 3000, 0:00:05
[[34m2023-05-13 15:14:16[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 15:14:21[0m] evaluated batches: 5000, 0:00:05
[[34m2023-05-13 15:14:26[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 15:14:32[0m] evaluated batches: 7000, 0:00:05
[[34m2023-05-13 15:14:37[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 15:14:42[0m] evaluated batches: 9000, 0:00:05
[[34m2023-05-13 15:14:47[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 15:14:53[0m] evaluated batches: 11000, 0:00:05
[[34m2023-05-13 15:14:57[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 15:15:09[0m] test loss = 0.547852, test auc = 0.794096
[[34m2023-05-13 15:15:09[0m] evaluated time: 0:01:13
[[34m2023-05-13 15:15:09[0m] analyse_structure
[[34m2023-05-13 15:16:11[0m] elapsed : 1:12:14, ETA : 0:29:17
[[34m2023-05-13 15:16:11[0m] epoch 8 / 10, batch 1000 / 8689, global_step = 61823, learning_rate = 1.000000e-02, loss = 0.922708, l2 = 0.017006, auc = 0.795539
elapsed : 1:12:14, ETA : 0:29:17
epoch 8 / 10, batch 1000 / 8689, global_step = 61823, learning_rate = 1.000000e-02, loss = 0.922708, l2 = 0.017006, auc = 0.795539
[[34m2023-05-13 15:17:13[0m] elapsed : 1:13:16, ETA : 0:28:04
[[34m2023-05-13 15:17:13[0m] epoch 8 / 10, batch 2000 / 8689, global_step = 62823, learning_rate = 1.000000e-02, loss = 0.546424, l2 = 0.010019, auc = 0.795485
elapsed : 1:13:16, ETA : 0:28:04
epoch 8 / 10, batch 2000 / 8689, global_step = 62823, learning_rate = 1.000000e-02, loss = 0.546424, l2 = 0.010019, auc = 0.795485
[[34m2023-05-13 15:18:14[0m] elapsed : 1:14:17, ETA : 0:26:50
[[34m2023-05-13 15:18:14[0m] epoch 8 / 10, batch 3000 / 8689, global_step = 63823, learning_rate = 1.000000e-02, loss = 0.546667, l2 = 0.010003, auc = 0.795300
elapsed : 1:14:17, ETA : 0:26:50
epoch 8 / 10, batch 3000 / 8689, global_step = 63823, learning_rate = 1.000000e-02, loss = 0.546667, l2 = 0.010003, auc = 0.795300
[[34m2023-05-13 15:19:14[0m] elapsed : 1:15:17, ETA : 0:25:37
[[34m2023-05-13 15:19:14[0m] epoch 8 / 10, batch 4000 / 8689, global_step = 64823, learning_rate = 1.000000e-02, loss = 0.546356, l2 = 0.009951, auc = 0.795405
elapsed : 1:15:17, ETA : 0:25:37
epoch 8 / 10, batch 4000 / 8689, global_step = 64823, learning_rate = 1.000000e-02, loss = 0.546356, l2 = 0.009951, auc = 0.795405
[[34m2023-05-13 15:20:15[0m] elapsed : 1:16:18, ETA : 0:24:25
[[34m2023-05-13 15:20:15[0m] epoch 8 / 10, batch 5000 / 8689, global_step = 65823, learning_rate = 1.000000e-02, loss = 0.546150, l2 = 0.009902, auc = 0.795700
elapsed : 1:16:18, ETA : 0:24:25
epoch 8 / 10, batch 5000 / 8689, global_step = 65823, learning_rate = 1.000000e-02, loss = 0.546150, l2 = 0.009902, auc = 0.795700
[[34m2023-05-13 15:21:17[0m] elapsed : 1:17:20, ETA : 0:23:13
[[34m2023-05-13 15:21:17[0m] epoch 8 / 10, batch 6000 / 8689, global_step = 66823, learning_rate = 1.000000e-02, loss = 0.545972, l2 = 0.009875, auc = 0.795902
elapsed : 1:17:20, ETA : 0:23:13
epoch 8 / 10, batch 6000 / 8689, global_step = 66823, learning_rate = 1.000000e-02, loss = 0.545972, l2 = 0.009875, auc = 0.795902
[[34m2023-05-13 15:22:20[0m] elapsed : 1:18:23, ETA : 0:22:02
[[34m2023-05-13 15:22:20[0m] epoch 8 / 10, batch 7000 / 8689, global_step = 67823, learning_rate = 1.000000e-02, loss = 0.546219, l2 = 0.009857, auc = 0.795674
elapsed : 1:18:23, ETA : 0:22:02
epoch 8 / 10, batch 7000 / 8689, global_step = 67823, learning_rate = 1.000000e-02, loss = 0.546219, l2 = 0.009857, auc = 0.795674
[[34m2023-05-13 15:23:24[0m] elapsed : 1:19:27, ETA : 0:20:51
[[34m2023-05-13 15:23:24[0m] epoch 8 / 10, batch 8000 / 8689, global_step = 68823, learning_rate = 1.000000e-02, loss = 0.546741, l2 = 0.009813, auc = 0.795064
elapsed : 1:19:27, ETA : 0:20:51
epoch 8 / 10, batch 8000 / 8689, global_step = 68823, learning_rate = 1.000000e-02, loss = 0.546741, l2 = 0.009813, auc = 0.795064
[[34m2023-05-13 15:24:03[0m] running test...
on disk...
[[34m2023-05-13 15:24:08[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-13 15:24:13[0m] evaluated batches: 2000, 0:00:04
[[34m2023-05-13 15:24:19[0m] evaluated batches: 3000, 0:00:05
[[34m2023-05-13 15:24:24[0m] evaluated batches: 4000, 0:00:04
[[34m2023-05-13 15:24:29[0m] evaluated batches: 5000, 0:00:05
[[34m2023-05-13 15:24:34[0m] evaluated batches: 6000, 0:00:04
[[34m2023-05-13 15:24:40[0m] evaluated batches: 7000, 0:00:05
[[34m2023-05-13 15:24:45[0m] evaluated batches: 8000, 0:00:04
[[34m2023-05-13 15:24:50[0m] evaluated batches: 9000, 0:00:05
[[34m2023-05-13 15:24:55[0m] evaluated batches: 10000, 0:00:04
[[34m2023-05-13 15:25:00[0m] evaluated batches: 11000, 0:00:05
[[34m2023-05-13 15:25:05[0m] evaluated batches: 12000, 0:00:04
[[34m2023-05-13 15:25:17[0m] test loss = 0.547635, test auc = 0.794738
[[34m2023-05-13 15:25:17[0m] evaluated time: 0:01:13
[[34m2023-05-13 15:25:17[0m] analyse_structure
[[34m2023-05-13 15:26:19[0m] elapsed : 1:22:22, ETA : 0:19:07
[[34m2023-05-13 15:26:19[0m] epoch 9 / 10, batch 1000 / 8689, global_step = 70512, learning_rate = 1.000000e-02, loss = 0.922683, l2 = 0.016519, auc = 0.795559
elapsed : 1:22:22, ETA : 0:19:07
epoch 9 / 10, batch 1000 / 8689, global_step = 70512, learning_rate = 1.000000e-02, loss = 0.922683, l2 = 0.016519, auc = 0.795559
[[34m2023-05-13 15:27:21[0m] elapsed : 1:23:24, ETA : 0:17:56
[[34m2023-05-13 15:27:21[0m] epoch 9 / 10, batch 2000 / 8689, global_step = 71512, learning_rate = 1.000000e-02, loss = 0.545705, l2 = 0.009763, auc = 0.796245
elapsed : 1:23:24, ETA : 0:17:56
epoch 9 / 10, batch 2000 / 8689, global_step = 71512, learning_rate = 1.000000e-02, loss = 0.545705, l2 = 0.009763, auc = 0.796245
[[34m2023-05-13 15:28:24[0m] elapsed : 1:24:27, ETA : 0:16:44
[[34m2023-05-13 15:28:24[0m] epoch 9 / 10, batch 3000 / 8689, global_step = 72512, learning_rate = 1.000000e-02, loss = 0.545792, l2 = 0.009716, auc = 0.796186
elapsed : 1:24:27, ETA : 0:16:44
epoch 9 / 10, batch 3000 / 8689, global_step = 72512, learning_rate = 1.000000e-02, loss = 0.545792, l2 = 0.009716, auc = 0.796186
[[34m2023-05-13 15:29:24[0m] elapsed : 1:25:27, ETA : 0:15:33
[[34m2023-05-13 15:29:24[0m] epoch 9 / 10, batch 4000 / 8689, global_step = 73512, learning_rate = 1.000000e-02, loss = 0.545609, l2 = 0.009694, auc = 0.796244
elapsed : 1:25:27, ETA : 0:15:33
epoch 9 / 10, batch 4000 / 8689, global_step = 73512, learning_rate = 1.000000e-02, loss = 0.545609, l2 = 0.009694, auc = 0.796244
[[34m2023-05-13 15:30:26[0m] elapsed : 1:26:29, ETA : 0:14:22
[[34m2023-05-13 15:30:26[0m] epoch 9 / 10, batch 5000 / 8689, global_step = 74512, learning_rate = 1.000000e-02, loss = 0.545350, l2 = 0.009658, auc = 0.796526
elapsed : 1:26:29, ETA : 0:14:22
epoch 9 / 10, batch 5000 / 8689, global_step = 74512, learning_rate = 1.000000e-02, loss = 0.545350, l2 = 0.009658, auc = 0.796526
[[34m2023-05-13 15:31:45[0m] elapsed : 1:27:48, ETA : 0:13:13
[[34m2023-05-13 15:31:45[0m] epoch 9 / 10, batch 6000 / 8689, global_step = 75512, learning_rate = 1.000000e-02, loss = 0.545196, l2 = 0.009635, auc = 0.796679
elapsed : 1:27:48, ETA : 0:13:13
epoch 9 / 10, batch 6000 / 8689, global_step = 75512, learning_rate = 1.000000e-02, loss = 0.545196, l2 = 0.009635, auc = 0.796679
[[34m2023-05-13 15:33:07[0m] elapsed : 1:29:09, ETA : 0:12:05
[[34m2023-05-13 15:33:07[0m] epoch 9 / 10, batch 7000 / 8689, global_step = 76512, learning_rate = 1.000000e-02, loss = 0.545337, l2 = 0.009630, auc = 0.796473
elapsed : 1:29:09, ETA : 0:12:05
epoch 9 / 10, batch 7000 / 8689, global_step = 76512, learning_rate = 1.000000e-02, loss = 0.545337, l2 = 0.009630, auc = 0.796473
[[34m2023-05-13 15:34:29[0m] elapsed : 1:30:32, ETA : 0:10:57
[[34m2023-05-13 15:34:29[0m] epoch 9 / 10, batch 8000 / 8689, global_step = 77512, learning_rate = 1.000000e-02, loss = 0.545716, l2 = 0.009608, auc = 0.796087
elapsed : 1:30:32, ETA : 0:10:57
epoch 9 / 10, batch 8000 / 8689, global_step = 77512, learning_rate = 1.000000e-02, loss = 0.545716, l2 = 0.009608, auc = 0.796087
[[34m2023-05-13 15:35:26[0m] running test...
on disk...
[[34m2023-05-13 15:35:32[0m] evaluated batches: 1000, 0:00:06
[[34m2023-05-13 15:35:38[0m] evaluated batches: 2000, 0:00:05
[[34m2023-05-13 15:35:44[0m] evaluated batches: 3000, 0:00:06
[[34m2023-05-13 15:35:50[0m] evaluated batches: 4000, 0:00:05
[[34m2023-05-13 15:35:56[0m] evaluated batches: 5000, 0:00:06
[[34m2023-05-13 15:36:02[0m] evaluated batches: 6000, 0:00:05
[[34m2023-05-13 15:36:08[0m] evaluated batches: 7000, 0:00:06
[[34m2023-05-13 15:36:14[0m] evaluated batches: 8000, 0:00:05
[[34m2023-05-13 15:36:20[0m] evaluated batches: 9000, 0:00:06
[[34m2023-05-13 15:36:26[0m] evaluated batches: 10000, 0:00:05
[[34m2023-05-13 15:36:32[0m] evaluated batches: 11000, 0:00:06
[[34m2023-05-13 15:36:38[0m] evaluated batches: 12000, 0:00:05
[[34m2023-05-13 15:36:50[0m] test loss = 0.546457, test auc = 0.795259
[[34m2023-05-13 15:36:50[0m] evaluated time: 0:01:23
[[34m2023-05-13 15:36:50[0m] analyse_structure
[[34m2023-05-13 15:38:11[0m] elapsed : 1:34:14, ETA : 0:09:08
[[34m2023-05-13 15:38:11[0m] epoch 10 / 10, batch 1000 / 8689, global_step = 79201, learning_rate = 1.000000e-02, loss = 0.920590, l2 = 0.016150, auc = 0.796778
elapsed : 1:34:14, ETA : 0:09:08
epoch 10 / 10, batch 1000 / 8689, global_step = 79201, learning_rate = 1.000000e-02, loss = 0.920590, l2 = 0.016150, auc = 0.796778
[[34m2023-05-13 15:39:34[0m] elapsed : 1:35:37, ETA : 0:07:58
[[34m2023-05-13 15:39:34[0m] epoch 10 / 10, batch 2000 / 8689, global_step = 80201, learning_rate = 1.000000e-02, loss = 0.544524, l2 = 0.009519, auc = 0.797136
elapsed : 1:35:37, ETA : 0:07:58
epoch 10 / 10, batch 2000 / 8689, global_step = 80201, learning_rate = 1.000000e-02, loss = 0.544524, l2 = 0.009519, auc = 0.797136
[[34m2023-05-13 15:40:43[0m] elapsed : 1:36:46, ETA : 0:06:46
[[34m2023-05-13 15:40:43[0m] epoch 10 / 10, batch 3000 / 8689, global_step = 81201, learning_rate = 1.000000e-02, loss = 0.544951, l2 = 0.009504, auc = 0.796830
elapsed : 1:36:46, ETA : 0:06:46
epoch 10 / 10, batch 3000 / 8689, global_step = 81201, learning_rate = 1.000000e-02, loss = 0.544951, l2 = 0.009504, auc = 0.796830
[[34m2023-05-13 15:41:46[0m] elapsed : 1:37:49, ETA : 0:05:34
[[34m2023-05-13 15:41:46[0m] epoch 10 / 10, batch 4000 / 8689, global_step = 82201, learning_rate = 1.000000e-02, loss = 0.544627, l2 = 0.009475, auc = 0.797032
elapsed : 1:37:49, ETA : 0:05:34
epoch 10 / 10, batch 4000 / 8689, global_step = 82201, learning_rate = 1.000000e-02, loss = 0.544627, l2 = 0.009475, auc = 0.797032
[[34m2023-05-13 15:42:49[0m] elapsed : 1:38:51, ETA : 0:04:22
[[34m2023-05-13 15:42:49[0m] epoch 10 / 10, batch 5000 / 8689, global_step = 83201, learning_rate = 1.000000e-02, loss = 0.544272, l2 = 0.009443, auc = 0.797502
elapsed : 1:38:51, ETA : 0:04:22
epoch 10 / 10, batch 5000 / 8689, global_step = 83201, learning_rate = 1.000000e-02, loss = 0.544272, l2 = 0.009443, auc = 0.797502
[[34m2023-05-13 15:43:51[0m] elapsed : 1:39:54, ETA : 0:03:11
[[34m2023-05-13 15:43:51[0m] epoch 10 / 10, batch 6000 / 8689, global_step = 84201, learning_rate = 1.000000e-02, loss = 0.545450, l2 = 0.009445, auc = 0.796356
elapsed : 1:39:54, ETA : 0:03:11
epoch 10 / 10, batch 6000 / 8689, global_step = 84201, learning_rate = 1.000000e-02, loss = 0.545450, l2 = 0.009445, auc = 0.796356
[[34m2023-05-13 15:44:57[0m] elapsed : 1:40:59, ETA : 0:02:00
[[34m2023-05-13 15:44:57[0m] epoch 10 / 10, batch 7000 / 8689, global_step = 85201, learning_rate = 1.000000e-02, loss = 0.544282, l2 = 0.009402, auc = 0.797494
elapsed : 1:40:59, ETA : 0:02:00
epoch 10 / 10, batch 7000 / 8689, global_step = 85201, learning_rate = 1.000000e-02, loss = 0.544282, l2 = 0.009402, auc = 0.797494
[[34m2023-05-13 15:46:00[0m] elapsed : 1:42:03, ETA : 0:00:48
[[34m2023-05-13 15:46:00[0m] epoch 10 / 10, batch 8000 / 8689, global_step = 86201, learning_rate = 1.000000e-02, loss = 0.543847, l2 = 0.009383, auc = 0.797733
elapsed : 1:42:03, ETA : 0:00:48
epoch 10 / 10, batch 8000 / 8689, global_step = 86201, learning_rate = 1.000000e-02, loss = 0.543847, l2 = 0.009383, auc = 0.797733
[[34m2023-05-13 15:46:41[0m] new iteration
new iteration
on disk...
[[34m2023-05-13 15:46:42[0m] running test...
on disk...
[[34m2023-05-13 15:46:47[0m] evaluated batches: 1000, 0:00:05
[[34m2023-05-13 15:46:52[0m] evaluated batches: 2000, 0:00:05
[[34m2023-05-13 15:46:58[0m] evaluated batches: 3000, 0:00:05
[[34m2023-05-13 15:47:03[0m] evaluated batches: 4000, 0:00:05
[[34m2023-05-13 15:47:08[0m] evaluated batches: 5000, 0:00:05
[[34m2023-05-13 15:47:13[0m] evaluated batches: 6000, 0:00:05
[[34m2023-05-13 15:47:19[0m] evaluated batches: 7000, 0:00:05
[[34m2023-05-13 15:47:24[0m] evaluated batches: 8000, 0:00:05
[[34m2023-05-13 15:47:29[0m] evaluated batches: 9000, 0:00:05
[[34m2023-05-13 15:47:34[0m] evaluated batches: 10000, 0:00:05
[[34m2023-05-13 15:47:40[0m] evaluated batches: 11000, 0:00:05
[[34m2023-05-13 15:47:45[0m] evaluated batches: 12000, 0:00:05
[[34m2023-05-13 15:47:56[0m] test loss = 0.546967, test auc = 0.794933
[[34m2023-05-13 15:47:56[0m] evaluated time: 0:01:14
[[34m2023-05-13 15:47:56[0m] analyse_structure
[[34m2023-05-13 15:47:56[0m] Done!
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:       batch_size ‚ñÅ
wandb:               lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         test_auc ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:    test_log_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:    train_l2_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train_loss ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb: train_moving_auc ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb: batch_size 1000
wandb:         lr 0.01
wandb: 
wandb: üöÄ View run criteo-BS-1000-001-retrain_irazor-2023-05-13 14:03:46 at: https://wandb.ai/yao-yao/irazor/runs/b3ul517s
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20230513_140347-b3ul517s/logs
