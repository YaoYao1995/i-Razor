[[34m2023-05-13 13:54:45[0m] Experiment directory created at ~/results/retrain_irazor/criteo/001-retrain_irazor
[[34m2023-05-13 13:54:45[0m] Batchsize: 1000
[[34m2023-05-13 13:54:53[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
[[34m2023-05-13 13:54:54[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
[[34m2023-05-13 13:54:55[0m] total batches: 86890	batch per epoch: 8689
[[34m2023-05-13 13:54:55[0m] new iteration
[[34m2023-05-13 13:56:00[0m] elapsed : 0:01:04, ETA : 1:31:36
[[34m2023-05-13 13:56:00[0m] epoch 1 / 10, batch 1000 / 8689, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.621013, l2 = 0.774113, auc = 0.748400
[[34m2023-05-13 13:57:02[0m] elapsed : 0:02:06, ETA : 1:29:08
[[34m2023-05-13 13:57:02[0m] epoch 1 / 10, batch 2000 / 8689, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.574838, l2 = 0.097151, auc = 0.767821
[[34m2023-05-13 13:58:04[0m] elapsed : 0:03:09, ETA : 1:28:05
[[34m2023-05-13 13:58:04[0m] epoch 1 / 10, batch 3000 / 8689, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.568821, l2 = 0.036223, auc = 0.774022
[[34m2023-05-13 13:59:07[0m] elapsed : 0:04:11, ETA : 1:26:41
[[34m2023-05-13 13:59:07[0m] epoch 1 / 10, batch 4000 / 8689, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.566904, l2 = 0.021938, auc = 0.775841
[[34m2023-05-13 14:00:09[0m] elapsed : 0:05:13, ETA : 1:25:26
[[34m2023-05-13 14:00:09[0m] epoch 1 / 10, batch 5000 / 8689, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.563885, l2 = 0.017572, auc = 0.778910
