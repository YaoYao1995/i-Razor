[[34m2023-05-13 13:52:33[0m] Experiment directory created at ~/results/retrain_irazor/avazu/000-retrain_irazor
[[34m2023-05-13 13:52:33[0m] Batchsize: 128
[[34m2023-05-13 13:52:40[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.
Instructions for updating:
targets is deprecated, use labels instead
[[34m2023-05-13 13:52:41[0m] From /home/ubuntu/miniconda3/envs/irazor/lib/python3.11/site-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
[[34m2023-05-13 13:52:42[0m] total batches: 252690	batch per epoch: 25269
[[34m2023-05-13 13:52:42[0m] new iteration
[[34m2023-05-13 13:52:51[0m] elapsed : 0:00:09, ETA : 0:37:45
[[34m2023-05-13 13:52:51[0m] epoch 1 / 10, batch 1000 / 25269, global_step = 1000, learning_rate = 1.000000e-02, loss = 0.444659, l2 = 0.114786, auc = 0.662531
[[34m2023-05-13 13:53:04[0m] elapsed : 0:00:22, ETA : 0:45:57
[[34m2023-05-13 13:53:04[0m] epoch 1 / 10, batch 2000 / 25269, global_step = 2000, learning_rate = 1.000000e-02, loss = 0.404715, l2 = 0.045487, auc = 0.728780
[[34m2023-05-13 13:53:20[0m] elapsed : 0:00:38, ETA : 0:52:42
[[34m2023-05-13 13:53:20[0m] epoch 1 / 10, batch 3000 / 25269, global_step = 3000, learning_rate = 1.000000e-02, loss = 0.403034, l2 = 0.026249, auc = 0.738258
[[34m2023-05-13 13:53:35[0m] elapsed : 0:00:53, ETA : 0:54:55
[[34m2023-05-13 13:53:35[0m] epoch 1 / 10, batch 4000 / 25269, global_step = 4000, learning_rate = 1.000000e-02, loss = 0.400796, l2 = 0.017445, auc = 0.741341
[[34m2023-05-13 13:53:51[0m] elapsed : 0:01:09, ETA : 0:56:58
[[34m2023-05-13 13:53:51[0m] epoch 1 / 10, batch 5000 / 25269, global_step = 5000, learning_rate = 1.000000e-02, loss = 0.399472, l2 = 0.012756, auc = 0.744170
[[34m2023-05-13 13:54:06[0m] elapsed : 0:01:24, ETA : 0:57:33
[[34m2023-05-13 13:54:06[0m] epoch 1 / 10, batch 6000 / 25269, global_step = 6000, learning_rate = 1.000000e-02, loss = 0.396704, l2 = 0.009902, auc = 0.747877
[[34m2023-05-13 13:54:22[0m] elapsed : 0:01:39, ETA : 0:57:54
[[34m2023-05-13 13:54:22[0m] epoch 1 / 10, batch 7000 / 25269, global_step = 7000, learning_rate = 1.000000e-02, loss = 0.394821, l2 = 0.008060, auc = 0.750493
[[34m2023-05-13 13:54:37[0m] elapsed : 0:01:55, ETA : 0:58:37
[[34m2023-05-13 13:54:37[0m] epoch 1 / 10, batch 8000 / 25269, global_step = 8000, learning_rate = 1.000000e-02, loss = 0.395940, l2 = 0.006853, auc = 0.750478
[[34m2023-05-13 13:54:57[0m] elapsed : 0:02:15, ETA : 1:00:55
[[34m2023-05-13 13:54:57[0m] epoch 1 / 10, batch 9000 / 25269, global_step = 9000, learning_rate = 1.000000e-02, loss = 0.395216, l2 = 0.005988, auc = 0.749281
[[34m2023-05-13 13:55:32[0m] elapsed : 0:02:49, ETA : 1:08:21
[[34m2023-05-13 13:55:32[0m] epoch 1 / 10, batch 10000 / 25269, global_step = 10000, learning_rate = 1.000000e-02, loss = 0.394563, l2 = 0.005371, auc = 0.751686
[[34m2023-05-13 13:56:06[0m] elapsed : 0:03:24, ETA : 1:14:42
[[34m2023-05-13 13:56:06[0m] epoch 1 / 10, batch 11000 / 25269, global_step = 11000, learning_rate = 1.000000e-02, loss = 0.393619, l2 = 0.004913, auc = 0.752226
[[34m2023-05-13 13:56:41[0m] elapsed : 0:03:59, ETA : 1:19:53
[[34m2023-05-13 13:56:41[0m] epoch 1 / 10, batch 12000 / 25269, global_step = 12000, learning_rate = 1.000000e-02, loss = 0.395282, l2 = 0.004542, auc = 0.753829
[[34m2023-05-13 13:57:15[0m] elapsed : 0:04:33, ETA : 1:23:53
[[34m2023-05-13 13:57:15[0m] epoch 1 / 10, batch 13000 / 25269, global_step = 13000, learning_rate = 1.000000e-02, loss = 0.395215, l2 = 0.004204, auc = 0.750308
[[34m2023-05-13 13:57:49[0m] elapsed : 0:05:07, ETA : 1:27:14
[[34m2023-05-13 13:57:49[0m] epoch 1 / 10, batch 14000 / 25269, global_step = 14000, learning_rate = 1.000000e-02, loss = 0.392322, l2 = 0.003983, auc = 0.755685
[[34m2023-05-13 13:58:24[0m] elapsed : 0:05:42, ETA : 1:30:19
[[34m2023-05-13 13:58:24[0m] epoch 1 / 10, batch 15000 / 25269, global_step = 15000, learning_rate = 1.000000e-02, loss = 0.392701, l2 = 0.003792, auc = 0.757019
[[34m2023-05-13 13:58:59[0m] elapsed : 0:06:17, ETA : 1:32:57
[[34m2023-05-13 13:58:59[0m] epoch 1 / 10, batch 16000 / 25269, global_step = 16000, learning_rate = 1.000000e-02, loss = 0.393566, l2 = 0.003649, auc = 0.755389
[[34m2023-05-13 13:59:33[0m] elapsed : 0:06:51, ETA : 1:34:58
[[34m2023-05-13 13:59:33[0m] epoch 1 / 10, batch 17000 / 25269, global_step = 17000, learning_rate = 1.000000e-02, loss = 0.393015, l2 = 0.003519, auc = 0.756716
[[34m2023-05-13 14:00:06[0m] elapsed : 0:07:24, ETA : 1:36:29
[[34m2023-05-13 14:00:06[0m] epoch 1 / 10, batch 18000 / 25269, global_step = 18000, learning_rate = 1.000000e-02, loss = 0.390796, l2 = 0.003404, auc = 0.757488
